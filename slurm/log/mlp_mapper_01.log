WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
| distributed init (rank 0): env://, gpu 0
| distributed init (rank 1): env://, gpu 1
[00:09:59.999552] Input args:
 {
    "accum_iter": 2,
    "ae": "kl_d512_m512_l8",
    "ae_pth": "output/graph_edit/ae/ae_m512.pth",
    "batch_size": 32,
    "blr": 0.0001,
    "clip_grad": 3.0,
    "data_path": "/ibex/user/slimhy/ShapeWalk/",
    "data_type": "basic_edit",
    "dataset": "graphedits",
    "debug_mode": true,
    "debug_with_forward": true,
    "device": "cuda",
    "dist_backend": "nccl",
    "dist_eval": true,
    "dist_on_itp": false,
    "dist_url": "env://",
    "distributed": true,
    "epochs": 800,
    "eval": false,
    "exp_name": "mlp_mapper_01",
    "fetch_keys": false,
    "gpu": 0,
    "is_mlp": true,
    "layer_decay": 0.75,
    "local_rank": -1,
    "log_dir": "output/graph_edit/mlp/mlp_mapper_01",
    "lr": 0.0001,
    "max_edge_level": 1,
    "min_lr": 1e-06,
    "model": "mlp_mapper_bert",
    "num_workers": 8,
    "output_dir": "output/graph_edit/mlp/mlp_mapper_01",
    "pin_mem": true,
    "rank": 0,
    "resume": "",
    "resume_full_weights": false,
    "resume_weights": false,
    "seed": 0,
    "start_epoch": 0,
    "text_model_name": "bert-base-uncased",
    "use_clip": false,
    "use_embeds": true,
    "wandb_id": null,
    "warmup_epochs": 40,
    "weight_decay": 0.05,
    "world_size": 2
}
[00:10:00.137026] Job dir: /ibex/user/slimhy/Shape2VecSet/code
[00:10:00.143454] |Train| size = [256]
[00:10:00.150118] |Valid| size = [32]
[00:10:01.024165] Loading autoencoder output/graph_edit/ae/ae_m512.pth
[00:10:04.680692] Model = MLPLatentMapper(
  (dir_mlp): MLP(
    (net): Sequential(
      (0): Linear(in_features=4864, out_features=4096, bias=True)
      (1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Linear(in_features=4096, out_features=4096, bias=True)
      (4): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Linear(in_features=4096, out_features=4096, bias=True)
      (7): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): ReLU(inplace=True)
      (9): Linear(in_features=4096, out_features=4096, bias=True)
      (10): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): ReLU(inplace=True)
      (12): Linear(in_features=4096, out_features=4096, bias=False)
    )
  )
  (mag_mlp): MLP(
    (net): Sequential(
      (0): Linear(in_features=4864, out_features=256, bias=True)
      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Linear(in_features=256, out_features=128, bias=True)
      (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Linear(in_features=128, out_features=64, bias=True)
      (7): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): ReLU(inplace=True)
      (9): Linear(in_features=64, out_features=1, bias=False)
    )
  )
)
[00:10:04.686262] Params (M): 88.37
wandb: Currently logged in as: habib-slim (shapewalk). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /ibex/user/slimhy/Shape2VecSet/wandb/run-20231106_001007-4zdgc5om
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mlp_mapper_01__aep9eojr
wandb: ‚≠êÔ∏è View project at https://wandb.ai/shapewalk/shape2vecset
wandb: üöÄ View run at https://wandb.ai/shapewalk/shape2vecset/runs/4zdgc5om
[00:10:18.354541] Model params:
 {
    "accum_iter": 2,
    "base_blr": 0.0001,
    "batch_size": 32,
    "clip_grad": 3.0,
    "dist_eval": true,
    "eff_batch_size": 128,
    "epochs": 800,
    "eval": false,
    "layer_decay": 0.75,
    "lr": 0.0001,
    "max_edge_level": 1,
    "min_lr": 1e-06,
    "model": "mlp_mapper_bert",
    "resume": "",
    "resume_full_weights": false,
    "start_epoch": 0,
    "weight_decay": 0.05
}
[00:10:18.486634] Start training for 800 epochs
[00:10:19.453568] Epoch: [0]  [0/8]  eta: 0:00:07  lr: 0.000000  loss: 1.4949 (1.4949)  time: 0.9351  data: 0.5726  max mem: 1853
[00:10:20.077315] Epoch: [0]  [7/8]  eta: 0:00:00  lr: 0.000002  loss: 1.4706 (1.4731)  time: 0.1923  data: 0.0716  max mem: 3201
[00:10:20.146876] Epoch: [0] Total time: 0:00:01 (0.2037 s / it)
[00:10:20.153963] Averaged stats: lr: 0.000002  loss: 1.4706 (1.4733)
[00:10:21.910529] Test:  [ 0/16]  eta: 0:00:03  loss: 1.5971 (1.5971)  time: 0.2079  data: 0.1963  max mem: 3201
[00:10:21.978794] Test:  [15/16]  eta: 0:00:00  loss: 1.5595 (1.5019)  time: 0.0146  data: 0.0123  max mem: 3201
[00:10:22.039137] Test: Total time: 0:00:00 (0.0211 s / it)
[00:10:22.055236] * loss 1.476
[00:10:22.075540] loss on 32 validation shape pairs: 1.476
[00:10:22.210781] Training time 0:00:03
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:            epoch ‚ñÅ‚ñÅ
wandb:      epoch_1000x ‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:           max_lr ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñà
wandb:           min_lr ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñà
wandb: train_batch_loss ‚ñÑ‚ñÇ‚ñà‚ñÅ
wandb:       train_loss ‚ñÅ
wandb:       valid_loss ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            epoch 0
wandb:      epoch_1000x 875
wandb:           max_lr 0.0
wandb:           min_lr 0.0
wandb: train_batch_loss 1.45799
wandb:       train_loss 1.47326
wandb:       valid_loss 1.47579
wandb: 
wandb: üöÄ View run mlp_mapper_01__aep9eojr at: https://wandb.ai/shapewalk/shape2vecset/runs/4zdgc5om
wandb: Ô∏è‚ö° View job at https://wandb.ai/shapewalk/shape2vecset/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExMTIyMDEzMQ==/version_details/v14
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231106_001007-4zdgc5om/logs
