master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
| distributed init (rank 1): env://, gpu 1
| distributed init (rank 0): env://, gpu 0
[07:55:42.726905] Input args:
 {
    "accum_iter": 1,
    "ae": "kl_d512_m512_l8",
    "ae_pth": null,
    "alt_ae_embeds": "pc_ae",
    "batch_size": 64,
    "blr": 0.0001,
    "clip_grad": 3.0,
    "data_path": "/ibex/user/slimhy/ShapeWalk/",
    "data_type": "release",
    "dataset": "graphedits",
    "debug_mode": false,
    "debug_with_forward": false,
    "device": "cuda",
    "dist_backend": "nccl",
    "dist_eval": true,
    "dist_on_itp": false,
    "dist_url": "env://",
    "distributed": true,
    "epochs": 800,
    "eval": false,
    "exp_name": "mlp_mapper_bert_direct_latent_512",
    "fetch_keys": false,
    "ft_bert": false,
    "gpu": 0,
    "intensity_loss": false,
    "is_diff": false,
    "is_mlp": true,
    "is_nrl_listener": false,
    "layer_decay": 0.75,
    "local_rank": -1,
    "log_dir": "output/graph_edit/dm/mlp_mapper_bert_direct_latent_512",
    "lr": 0.0001,
    "max_edge_level": 1,
    "min_lr": 1e-06,
    "model": "mlp_mapper_bert_direct_latent_512",
    "num_workers": 4,
    "output_dir": "output/graph_edit/dm/mlp_mapper_bert_direct_latent_512",
    "pin_mem": true,
    "plateau_scheduler": false,
    "point_cloud_size": 2048,
    "rank": 0,
    "resume": "",
    "resume_full_weights": false,
    "resume_weights": false,
    "seed": 0,
    "start_epoch": 0,
    "text_model_name": "bert-base-uncased",
    "use_adam": false,
    "use_clip": false,
    "use_embeds": true,
    "valid_step": 5,
    "wandb_id": null,
    "warmup_epochs": 8,
    "weight_decay": 0.05,
    "world_size": 2
}
[07:55:42.727391] Job dir: /ibex/user/slimhy/Shape2VecSet/code
[07:55:42.728155] |Train| size = [632472]
[07:55:42.728180] |Valid| size = [4160]
[07:55:42.747837] Model = MLPLatentMapperDirect(
  (dir_mlp): MLP(
    (net): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Dropout(p=0.2, inplace=False)
      (4): Linear(in_features=512, out_features=512, bias=True)
    )
  )
)
[07:55:42.747869] Params (M): 0.92
wandb: Currently logged in as: habib-slim (shapewalk). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /ibex/user/slimhy/Shape2VecSet/wandb/run-20231118_075545-zphd3d00
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mlp_mapper_bert_direct_latent_512__g0zbrb7w
wandb: ‚≠êÔ∏è View project at https://wandb.ai/shapewalk/shape2vecset
wandb: üöÄ View run at https://wandb.ai/shapewalk/shape2vecset/runs/zphd3d00
[07:55:52.349140] Model params:
 {
    "accum_iter": 1,
    "base_blr": 0.0001,
    "batch_size": 64,
    "clip_grad": 3.0,
    "dist_eval": true,
    "eff_batch_size": 128,
    "epochs": 800,
    "eval": false,
    "layer_decay": 0.75,
    "lr": 0.0001,
    "max_edge_level": 1,
    "min_lr": 1e-06,
    "model": "mlp_mapper_bert_direct_latent_512",
    "resume": "",
    "resume_full_weights": false,
    "start_epoch": 0,
    "weight_decay": 0.05
}
[07:55:52.381432] Start training for 800 epochs
Traceback (most recent call last):
  File "/ibex/user/slimhy/Shape2VecSet/code/main_node2node.py", line 586, in <module>
    main(args)
  File "/ibex/user/slimhy/Shape2VecSet/code/main_node2node.py", line 517, in main
    train_stats = train_func(
  File "/ibex/user/slimhy/Shape2VecSet/code/engine_node2node.py", line 133, in train_one_epoch
    loss = forward_pass(
  File "/ibex/user/slimhy/Shape2VecSet/code/engine_node2node.py", line 91, in forward_pass
    loss = criterion(model, embeds_node_a, embeds_node_b, embeds_text_ab)
  File "/ibex/user/slimhy/Shape2VecSet/code/losses/mapper_loss.py", line 29, in __call__
    x_bp = net(x_a, embed_ab)
  File "/home/slimhy/conda/envs/shape2vecset/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/slimhy/conda/envs/shape2vecset/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1156, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/home/slimhy/conda/envs/shape2vecset/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1110, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]
  File "/home/slimhy/conda/envs/shape2vecset/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ibex/user/slimhy/Shape2VecSet/code/models/mlp_mapper.py", line 382, in forward
    pred_latent = self.dir_mlp(x)
  File "/ibex/user/slimhy/Shape2VecSet/code/models/mlp_mapper.py", line 91, in __call__
    return self.net(x)
  File "/home/slimhy/conda/envs/shape2vecset/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/slimhy/conda/envs/shape2vecset/lib/python3.10/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/home/slimhy/conda/envs/shape2vecset/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/slimhy/conda/envs/shape2vecset/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (64x1024 and 1280x512)
[07:55:53.740474] Traceback (most recent call last):
[07:55:53.740915]   File "/ibex/user/slimhy/Shape2VecSet/code/main_node2node.py", line 586, in <module>
    main(args)
[07:55:53.741066]   File "/ibex/user/slimhy/Shape2VecSet/code/main_node2node.py", line 517, in main
    train_stats = train_func(
[07:55:53.741199]   File "/ibex/user/slimhy/Shape2VecSet/code/engine_node2node.py", line 133, in train_one_epoch
    loss = forward_pass(
[07:55:53.741326]   File "/ibex/user/slimhy/Shape2VecSet/code/engine_node2node.py", line 91, in forward_pass
    loss = criterion(model, embeds_node_a, embeds_node_b, embeds_text_ab)
[07:55:53.741438]   File "/ibex/user/slimhy/Shape2VecSet/code/losses/mapper_loss.py", line 29, in __call__
    x_bp = net(x_a, embed_ab)
[07:55:53.741540]   File "/home/slimhy/conda/envs/shape2vecset/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
[07:55:53.741646]   File "/home/slimhy/conda/envs/shape2vecset/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1156, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
[07:55:53.741750]   File "/home/slimhy/conda/envs/shape2vecset/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1110, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]
[07:55:53.741869]   File "/home/slimhy/conda/envs/shape2vecset/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
[07:55:53.741973]   File "/ibex/user/slimhy/Shape2VecSet/code/models/mlp_mapper.py", line 382, in forward
    pred_latent = self.dir_mlp(x)
[07:55:53.742074]   File "/ibex/user/slimhy/Shape2VecSet/code/models/mlp_mapper.py", line 91, in __call__
    return self.net(x)
[07:55:53.742175]   File "/home/slimhy/conda/envs/shape2vecset/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
[07:55:53.742277]   File "/home/slimhy/conda/envs/shape2vecset/lib/python3.10/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
[07:55:53.742381]   File "/home/slimhy/conda/envs/shape2vecset/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
[07:55:53.742480]   File "/home/slimhy/conda/envs/shape2vecset/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
[07:55:53.742588] RuntimeError: mat1 and mat2 shapes cannot be multiplied (64x1024 and 1280x512)
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 55603 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 55604) of binary: /home/slimhy/conda/envs/shape2vecset/bin/python3.10
Traceback (most recent call last):
  File "/home/slimhy/conda/envs/shape2vecset/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/slimhy/conda/envs/shape2vecset/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/slimhy/conda/envs/shape2vecset/lib/python3.10/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/home/slimhy/conda/envs/shape2vecset/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/slimhy/conda/envs/shape2vecset/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/slimhy/conda/envs/shape2vecset/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
code/main_node2node.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-11-18_07:55:56
  host      : gpu214-06.ibex.kaust.edu.sa
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 55604)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
