WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
| distributed init (rank 0): env://, gpu 0
| distributed init (rank 1): env://, gpu 1
/home/slimhy/conda/envs/shape2vecset/lib/python3.10/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[01:28:07.323413] Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[01:28:07.324456] Input args:
 {
    "accum_iter": 8,
    "ae": "kl_d512_m512_l8",
    "ae_pth": "output/graph_edit/ae/ae_m512.pth",
    "batch_size": 8,
    "blr": 0.0001,
    "clip_grad": 3.0,
    "data_path": "/ibex/user/slimhy/ShapeWalk/",
    "dataset": "graphedits",
    "debug_mode": true,
    "debug_with_forward": true,
    "device": "cuda",
    "dist_backend": "nccl",
    "dist_eval": true,
    "dist_on_itp": false,
    "dist_url": "env://",
    "distributed": true,
    "epochs": 800,
    "eval": false,
    "exp_name": "ldm_finetune_debug_01",
    "gpu": 0,
    "layer_decay": 0.75,
    "local_rank": -1,
    "log_dir": "output/graph_edit/dm/ldm_finetune_debug_01",
    "lr": 0.0001,
    "max_edge_level": 1,
    "min_lr": 1e-06,
    "model": "kl_d512_m512_l8_d24_edm",
    "num_workers": 32,
    "output_dir": "output/graph_edit/dm/ldm_finetune_debug_01",
    "pin_mem": true,
    "point_cloud_size": 2048,
    "rank": 0,
    "resume": "output/graph_edit/dm/ldm_m512.pth",
    "resume_weights": false,
    "seed": 0,
    "start_epoch": 0,
    "text_model_name": "bert-base-uncased",
    "use_clip": false,
    "warmup_epochs": 40,
    "weight_decay": 0.05,
    "world_size": 2
}
[01:28:07.325888] Job dir: /ibex/user/slimhy/Shape2VecSet/code
/home/slimhy/conda/envs/shape2vecset/lib/python3.10/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[01:28:07.328562] |Train| size = [65848]
[01:28:07.329163] |Valid| size = [421]
[01:28:08.103316] Loading autoencoder output/graph_edit/ae/ae_m512.pth
[01:28:12.720898] Model = EDMTextCond(
  (edm_model): EDMPrecond(
    (model): LatentArrayTransformer(
      (proj_in): Linear(in_features=8, out_features=512, bias=False)
      (transformer_blocks): ModuleList(
        (0-23): 24 x BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=512, out_features=512, bias=False)
            (to_k): Linear(in_features=512, out_features=512, bias=False)
            (to_v): Linear(in_features=512, out_features=512, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=512, out_features=512, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=512, out_features=4096, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=2048, out_features=512, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=512, out_features=512, bias=False)
            (to_k): Linear(in_features=512, out_features=512, bias=False)
            (to_v): Linear(in_features=512, out_features=512, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=512, out_features=512, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm1): AdaLayerNorm(
            (silu): SiLU()
            (linear): Linear(in_features=512, out_features=1024, bias=True)
            (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=False)
          )
          (norm2): AdaLayerNorm(
            (silu): SiLU()
            (linear): Linear(in_features=512, out_features=1024, bias=True)
            (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=False)
          )
          (norm3): AdaLayerNorm(
            (silu): SiLU()
            (linear): Linear(in_features=512, out_features=1024, bias=True)
            (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (ls2): Identity()
          (drop_path2): Identity()
          (ls3): Identity()
          (drop_path3): Identity()
        )
      )
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (proj_out): Linear(in_features=512, out_features=8, bias=False)
      (map_noise): PositionalEmbedding()
      (map_layer0): Linear(in_features=256, out_features=512, bias=True)
      (map_layer1): Linear(in_features=512, out_features=512, bias=True)
    )
    (category_emb): Embedding(55, 512)
  )
  (linear_proj): Linear(in_features=768, out_features=512, bias=True)
)
[01:28:12.721654] Params (M): 164.61
wandb: Currently logged in as: habib-slim (shapewalk). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /ibex/user/slimhy/Shape2VecSet/wandb/run-20231101_012814-kwmdb900
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ldm_finetune_debug_01__w4fkz3yz
wandb: ‚≠êÔ∏è View project at https://wandb.ai/shapewalk/shape2vecset
wandb: üöÄ View run at https://wandb.ai/shapewalk/shape2vecset/runs/kwmdb900
[01:28:18.904916] Model params:
 {
    "accum_iter": 8,
    "base_blr": 0.0001,
    "batch_size": 8,
    "clip_grad": 3.0,
    "dist_eval": true,
    "eff_batch_size": 128,
    "epochs": 800,
    "eval": false,
    "layer_decay": 0.75,
    "lr": 0.0001,
    "max_edge_level": 1,
    "min_lr": 1e-06,
    "model": "kl_d512_m512_l8_d24_edm",
    "point_cloud_size": 2048,
    "resume": "output/graph_edit/dm/ldm_m512.pth",
    "start_epoch": 0,
    "weight_decay": 0.05
}
[01:28:18.969029] Start training for 800 epochs
/home/slimhy/conda/envs/shape2vecset/lib/python3.10/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[01:28:23.979179] Linear projection unchanged!
[01:28:23.981334] Transformer weight unchanged!
[01:28:23.983873] Epoch: [0]  [   0/8231]  eta: 11:26:44  lr: 0.000000  loss: 31.3100 (31.3100)  time: 5.0060  data: 3.8784  max mem: 9671
[01:28:50.368082] Linear projection changed by 2.980585236400657e-07
[01:28:50.370579] Transformer weight changed by 1.0372519199108865e-07
[01:28:50.372821] Epoch: [0]  [  50/8231]  eta: 1:23:55  lr: 0.000000  loss: 91.0888 (235.3001)  time: 0.5238  data: 0.0003  max mem: 11551
[01:29:16.631923] Linear projection changed by 8.829855687508825e-06
[01:29:16.634500] Transformer weight changed by 2.9387695121840807e-06
[01:29:16.636989] Epoch: [0]  [ 100/8231]  eta: 1:17:21  lr: 0.000000  loss: 100.4859 (352.2042)  time: 0.5240  data: 0.0004  max mem: 11551
[01:29:43.245542] Linear projection changed by 4.3167430703761056e-05
[01:29:43.247879] Transformer weight changed by 1.1752666068787221e-05
[01:29:43.250567] Epoch: [0]  [ 150/8231]  eta: 1:15:09  lr: 0.000000  loss: 102.4875 (342.6166)  time: 0.5328  data: 0.0003  max mem: 11551
[01:30:09.414918] Linear projection changed by 0.00014324267976917326
[01:30:09.416166] Transformer weight changed by 3.101494803559035e-05
[01:30:09.417407] Epoch: [0]  [ 200/8231]  eta: 1:13:31  lr: 0.000000  loss: 66.3485 (333.1140)  time: 0.5206  data: 0.0002  max mem: 11551
[01:30:35.462059] Linear projection changed by 0.00029648139025084674
[01:30:35.464344] Transformer weight changed by 5.633710679830983e-05
[01:30:35.467044] Epoch: [0]  [ 250/8231]  eta: 1:12:19  lr: 0.000000  loss: 107.9516 (321.1878)  time: 0.5224  data: 0.0003  max mem: 11551
[01:31:01.679241] Linear projection changed by 0.000524397473782301
[01:31:01.681481] Transformer weight changed by 9.036775009008124e-05
[01:31:01.684146] Epoch: [0]  [ 300/8231]  eta: 1:11:26  lr: 0.000000  loss: 81.0459 (325.2212)  time: 0.5251  data: 0.0003  max mem: 11551
[01:31:27.695556] Linear projection changed by 0.0008319165208376944
[01:31:27.698025] Transformer weight changed by 0.0001328606449533254
[01:31:27.700555] Epoch: [0]  [ 350/8231]  eta: 1:10:36  lr: 0.000000  loss: 84.0845 (305.4418)  time: 0.5191  data: 0.0003  max mem: 11551
[01:31:53.699053] Linear projection changed by 0.001297165174037218
[01:31:53.700164] Transformer weight changed by 0.00019360829901415855
[01:31:53.701396] Epoch: [0]  [ 400/8231]  eta: 1:09:52  lr: 0.000000  loss: 103.6689 (296.6966)  time: 0.5191  data: 0.0002  max mem: 11551
[01:32:19.677460] Linear projection changed by 0.001793058356270194
[01:32:19.679861] Transformer weight changed by 0.0002552379737608135
[01:32:19.682327] Epoch: [0]  [ 450/8231]  eta: 1:09:12  lr: 0.000000  loss: 76.0707 (292.0479)  time: 0.5215  data: 0.0003  max mem: 11551
[01:32:45.619457] Linear projection changed by 0.0023802760988473892
[01:32:45.620631] Transformer weight changed by 0.00032671200460754335
[01:32:45.621858] Epoch: [0]  [ 500/8231]  eta: 1:08:34  lr: 0.000000  loss: 148.9223 (310.2116)  time: 0.5175  data: 0.0002  max mem: 11551
[01:33:11.553056] Linear projection changed by 0.0030553170945495367
[01:33:11.555145] Transformer weight changed by 0.0004078846250195056
[01:33:11.557508] Epoch: [0]  [ 550/8231]  eta: 1:07:58  lr: 0.000000  loss: 104.4169 (318.9667)  time: 0.5189  data: 0.0002  max mem: 11551
[01:33:37.562740] Linear projection changed by 0.003960076719522476
[01:33:37.564708] Transformer weight changed by 0.0005153959500603378
[01:33:37.567088] Epoch: [0]  [ 600/8231]  eta: 1:07:24  lr: 0.000000  loss: 97.6138 (308.0707)  time: 0.5207  data: 0.0002  max mem: 11551
[01:34:03.473466] Linear projection changed by 0.004821499343961477
[01:34:03.475956] Transformer weight changed by 0.0006181499338708818
[01:34:03.478541] Epoch: [0]  [ 650/8231]  eta: 1:06:51  lr: 0.000000  loss: 123.8236 (310.0487)  time: 0.5206  data: 0.0002  max mem: 11551
[01:34:29.473124] Linear projection changed by 0.0057695722207427025
[01:34:29.475127] Transformer weight changed by 0.0007310807704925537
[01:34:29.477660] Epoch: [0]  [ 700/8231]  eta: 1:06:19  lr: 0.000000  loss: 114.2025 (302.5672)  time: 0.5197  data: 0.0002  max mem: 11551
[01:34:55.392157] Linear projection changed by 0.006778964772820473
[01:34:55.394047] Transformer weight changed by 0.0008538911934010684
[01:34:55.396716] Epoch: [0]  [ 750/8231]  eta: 1:05:48  lr: 0.000000  loss: 106.6608 (297.6921)  time: 0.5175  data: 0.0003  max mem: 11551
[01:35:21.394233] Linear projection changed by 0.008021880872547626
[01:35:21.396803] Transformer weight changed by 0.0010078696068376303
[01:35:21.399334] Epoch: [0]  [ 800/8231]  eta: 1:05:18  lr: 0.000000  loss: 176.3065 (304.3078)  time: 0.5211  data: 0.0003  max mem: 11551
[01:35:47.299359] Linear projection changed by 0.009140750393271446
[01:35:47.301622] Transformer weight changed by 0.0011488833697512746
[01:35:47.304212] Epoch: [0]  [ 850/8231]  eta: 1:04:47  lr: 0.000000  loss: 192.1595 (300.4698)  time: 0.5204  data: 0.0002  max mem: 11551
[01:36:13.234564] Linear projection changed by 0.010318880900740623
[01:36:13.237024] Transformer weight changed by 0.0012983960332348943
[01:36:13.239453] Epoch: [0]  [ 900/8231]  eta: 1:04:18  lr: 0.000000  loss: 125.5523 (305.7888)  time: 0.5180  data: 0.0002  max mem: 11551
[01:36:39.178917] Linear projection changed by 0.011516992002725601
[01:36:39.180818] Transformer weight changed by 0.0014552543871104717
[01:36:39.182557] Epoch: [0]  [ 950/8231]  eta: 1:03:49  lr: 0.000000  loss: 105.8460 (301.5473)  time: 0.5192  data: 0.0003  max mem: 11551
[01:37:05.142547] Linear projection changed by 0.012911451980471611
[01:37:05.144709] Transformer weight changed by 0.0016463816864416003
[01:37:05.147295] Epoch: [0]  [1000/8231]  eta: 1:03:20  lr: 0.000000  loss: 75.7132 (298.1045)  time: 0.5184  data: 0.0002  max mem: 11551
[01:37:30.931546] Linear projection changed by 0.014111367054283619
[01:37:30.934012] Transformer weight changed by 0.0018169850809499621
[01:37:30.936732] Epoch: [0]  [1050/8231]  eta: 1:02:50  lr: 0.000000  loss: 109.1654 (303.6091)  time: 0.5192  data: 0.0002  max mem: 11551
[01:37:56.821945] Linear projection changed by 0.015319704078137875
[01:37:56.824459] Transformer weight changed by 0.001993414480239153
[01:37:56.827162] Epoch: [0]  [1100/8231]  eta: 1:02:22  lr: 0.000000  loss: 99.3770 (309.5055)  time: 0.5182  data: 0.0002  max mem: 11551
[01:38:22.893154] Linear projection changed by 0.0165657140314579
[01:38:22.895418] Transformer weight changed by 0.0021774391643702984
[01:38:22.897915] Epoch: [0]  [1150/8231]  eta: 1:01:54  lr: 0.000000  loss: 94.7040 (306.6193)  time: 0.5261  data: 0.0002  max mem: 11551
[01:38:49.046611] Linear projection changed by 0.017962709069252014
[01:38:49.048022] Transformer weight changed by 0.0024003242142498493
[01:38:49.049450] Epoch: [0]  [1200/8231]  eta: 1:01:28  lr: 0.000000  loss: 135.4175 (306.2085)  time: 0.5270  data: 0.0002  max mem: 11551
[01:39:15.013785] Linear projection changed by 0.0192179623991251
[01:39:15.016245] Transformer weight changed by 0.002599031198769808
[01:39:15.018874] Epoch: [0]  [1250/8231]  eta: 1:01:00  lr: 0.000000  loss: 97.3348 (301.3909)  time: 0.5215  data: 0.0002  max mem: 11551
[01:39:40.930244] Linear projection changed by 0.020507659763097763
[01:39:40.931934] Transformer weight changed by 0.002801758237183094
[01:39:40.933528] Epoch: [0]  [1300/8231]  eta: 1:00:32  lr: 0.000000  loss: 128.3529 (300.3430)  time: 0.5191  data: 0.0002  max mem: 11551
[01:40:06.911794] Linear projection changed by 0.021800484508275986
[01:40:06.913927] Transformer weight changed by 0.003009075066074729
[01:40:06.916496] Epoch: [0]  [1350/8231]  eta: 1:00:05  lr: 0.000000  loss: 94.3284 (300.4896)  time: 0.5191  data: 0.0002  max mem: 11551
[01:40:32.982407] Linear projection changed by 0.023349110037088394
[01:40:32.985092] Transformer weight changed by 0.0032571163028478622
[01:40:32.987588] Epoch: [0]  [1400/8231]  eta: 0:59:38  lr: 0.000000  loss: 90.4623 (302.7302)  time: 0.5212  data: 0.0002  max mem: 11551
[01:40:58.923692] Linear projection changed by 0.02470913715660572
[01:40:58.925855] Transformer weight changed by 0.0034737740643322468
[01:40:58.928660] Epoch: [0]  [1450/8231]  eta: 0:59:11  lr: 0.000000  loss: 96.9481 (301.1999)  time: 0.5206  data: 0.0002  max mem: 11551
[01:41:24.877256] Linear projection changed by 0.026060787960886955
[01:41:24.879383] Transformer weight changed by 0.003694569692015648
[01:41:24.881938] Epoch: [0]  [1500/8231]  eta: 0:58:43  lr: 0.000000  loss: 117.9337 (297.5496)  time: 0.5192  data: 0.0002  max mem: 11551
[01:41:50.837128] Linear projection changed by 0.0274786539375782
[01:41:50.838526] Transformer weight changed by 0.003919836599379778
[01:41:50.839931] Epoch: [0]  [1550/8231]  eta: 0:58:16  lr: 0.000000  loss: 99.7851 (296.1130)  time: 0.5187  data: 0.0002  max mem: 11551
[01:42:16.852825] Linear projection changed by 0.02910558693110943
[01:42:16.855128] Transformer weight changed by 0.0041843862272799015
[01:42:16.857734] Epoch: [0]  [1600/8231]  eta: 0:57:49  lr: 0.000000  loss: 206.3451 (296.6270)  time: 0.5209  data: 0.0002  max mem: 11551
[01:42:42.815869] Linear projection changed by 0.0305488221347332
[01:42:42.818057] Transformer weight changed by 0.0044123949483036995
[01:42:42.820615] Epoch: [0]  [1650/8231]  eta: 0:57:22  lr: 0.000001  loss: 174.2262 (299.9697)  time: 0.5205  data: 0.0003  max mem: 11551
[01:43:08.801745] Linear projection changed by 0.03198090195655823
[01:43:08.803906] Transformer weight changed by 0.004642881918698549
[01:43:08.806220] Epoch: [0]  [1700/8231]  eta: 0:56:56  lr: 0.000001  loss: 146.7181 (299.2435)  time: 0.5186  data: 0.0002  max mem: 11551
[01:43:34.750321] Linear projection changed by 0.033423714339733124
[01:43:34.752100] Transformer weight changed by 0.004877611994743347
[01:43:34.753472] Epoch: [0]  [1750/8231]  eta: 0:56:29  lr: 0.000001  loss: 83.3650 (295.6687)  time: 0.5178  data: 0.0003  max mem: 11551
[01:44:00.777814] Linear projection changed by 0.03526534140110016
[01:44:00.779966] Transformer weight changed by 0.00515081686899066
[01:44:00.782549] Epoch: [0]  [1800/8231]  eta: 0:56:02  lr: 0.000001  loss: 80.5616 (310.9597)  time: 0.5207  data: 0.0002  max mem: 11551
[01:44:26.696405] Linear projection changed by 0.036850471049547195
[01:44:26.698500] Transformer weight changed by 0.0053840521723032
[01:44:26.701061] Epoch: [0]  [1850/8231]  eta: 0:55:35  lr: 0.000001  loss: 125.9639 (309.5430)  time: 0.5207  data: 0.0002  max mem: 11551
[01:44:52.643967] Linear projection changed by 0.03836199268698692
[01:44:52.645965] Transformer weight changed by 0.005618224386125803
[01:44:52.648547] Epoch: [0]  [1900/8231]  eta: 0:55:08  lr: 0.000001  loss: 73.9119 (307.7304)  time: 0.5164  data: 0.0002  max mem: 11551
[01:45:18.505655] Linear projection changed by 0.039894361048936844
[01:45:18.508081] Transformer weight changed by 0.0058512301184237
[01:45:18.510757] Epoch: [0]  [1950/8231]  eta: 0:54:41  lr: 0.000001  loss: 138.2494 (304.5657)  time: 0.5193  data: 0.0003  max mem: 11551
[01:45:44.522050] Linear projection changed by 0.04174633324146271
[01:45:44.524200] Transformer weight changed by 0.00612392695620656
[01:45:44.526658] Epoch: [0]  [2000/8231]  eta: 0:54:15  lr: 0.000001  loss: 89.9115 (302.2041)  time: 0.5210  data: 0.0002  max mem: 11551
[01:46:10.491747] Linear projection changed by 0.04342806711792946
[01:46:10.493855] Transformer weight changed by 0.006359018851071596
[01:46:10.496715] Epoch: [0]  [2050/8231]  eta: 0:53:48  lr: 0.000001  loss: 118.2660 (300.6980)  time: 0.5208  data: 0.0002  max mem: 11551
[01:46:36.452258] Linear projection changed by 0.04511148855090141
[01:46:36.453396] Transformer weight changed by 0.006593936588615179
[01:46:36.454524] Epoch: [0]  [2100/8231]  eta: 0:53:22  lr: 0.000001  loss: 103.1183 (301.4582)  time: 0.5179  data: 0.0002  max mem: 11551
[01:47:02.593281] Linear projection changed by 0.04684632271528244
[01:47:02.595388] Transformer weight changed by 0.00682632764801383
[01:47:02.597973] Epoch: [0]  [2150/8231]  eta: 0:52:56  lr: 0.000001  loss: 81.1813 (298.8664)  time: 0.5256  data: 0.0002  max mem: 11551
[01:47:28.632537] Linear projection changed by 0.04883028566837311
[01:47:28.634908] Transformer weight changed by 0.007092300336807966
[01:47:28.637511] Epoch: [0]  [2200/8231]  eta: 0:52:29  lr: 0.000001  loss: 140.9195 (302.8129)  time: 0.5211  data: 0.0003  max mem: 11551
[01:47:54.765534] Linear projection changed by 0.05059696361422539
[01:47:54.767586] Transformer weight changed by 0.0073204850777983665
[01:47:54.770177] Epoch: [0]  [2250/8231]  eta: 0:52:03  lr: 0.000001  loss: 93.6572 (306.1717)  time: 0.5195  data: 0.0002  max mem: 11551
[01:48:20.695170] Linear projection changed by 0.05229073390364647
[01:48:20.697020] Transformer weight changed by 0.0075426227413117886
[01:48:20.699564] Epoch: [0]  [2300/8231]  eta: 0:51:37  lr: 0.000001  loss: 63.8450 (306.6065)  time: 0.5177  data: 0.0002  max mem: 11551
[01:48:46.686947] Linear projection changed by 0.05397309735417366
[01:48:46.688874] Transformer weight changed by 0.007765295449644327
[01:48:46.690999] Epoch: [0]  [2350/8231]  eta: 0:51:10  lr: 0.000001  loss: 138.8939 (306.6752)  time: 0.5191  data: 0.0002  max mem: 11551
[01:49:12.679555] Linear projection changed by 0.055919576436281204
[01:49:12.680894] Transformer weight changed by 0.00802323967218399
[01:49:12.682084] Epoch: [0]  [2400/8231]  eta: 0:50:44  lr: 0.000001  loss: 70.7196 (303.7118)  time: 0.5201  data: 0.0002  max mem: 11551
[01:49:38.567434] Linear projection changed by 0.057638850063085556
[01:49:38.569650] Transformer weight changed by 0.008240681141614914
[01:49:38.572015] Epoch: [0]  [2450/8231]  eta: 0:50:17  lr: 0.000001  loss: 41.7834 (302.9710)  time: 0.5179  data: 0.0002  max mem: 11551
[01:50:04.524381] Linear projection changed by 0.05924908444285393
[01:50:04.526462] Transformer weight changed by 0.008454948663711548
[01:50:04.528782] Epoch: [0]  [2500/8231]  eta: 0:49:51  lr: 0.000001  loss: 109.7034 (302.6670)  time: 0.5188  data: 0.0002  max mem: 11551
[01:50:30.499086] Linear projection changed by 0.06087208911776543
[01:50:30.501121] Transformer weight changed by 0.008670750074088573
[01:50:30.503666] Epoch: [0]  [2550/8231]  eta: 0:49:24  lr: 0.000001  loss: 71.8435 (301.5595)  time: 0.5192  data: 0.0002  max mem: 11551
[01:50:56.474994] Linear projection changed by 0.06293454021215439
[01:50:56.476975] Transformer weight changed by 0.008921531960368156
[01:50:56.479491] Epoch: [0]  [2600/8231]  eta: 0:48:58  lr: 0.000001  loss: 103.6782 (299.7642)  time: 0.5203  data: 0.0002  max mem: 11551
[01:51:22.417726] Linear projection changed by 0.06467655301094055
[01:51:22.420081] Transformer weight changed by 0.009131208062171936
[01:51:22.422922] Epoch: [0]  [2650/8231]  eta: 0:48:32  lr: 0.000001  loss: 178.9456 (299.4204)  time: 0.5209  data: 0.0002  max mem: 11551
[01:51:48.365485] Linear projection changed by 0.06631223112344742
[01:51:48.367691] Transformer weight changed by 0.009335940703749657
[01:51:48.370063] Epoch: [0]  [2700/8231]  eta: 0:48:05  lr: 0.000001  loss: 146.3184 (300.9388)  time: 0.5176  data: 0.0002  max mem: 11551
[01:52:14.322750] Linear projection changed by 0.06798532605171204
[01:52:14.325032] Transformer weight changed by 0.009540826082229614
[01:52:14.327398] Epoch: [0]  [2750/8231]  eta: 0:47:39  lr: 0.000001  loss: 148.0649 (303.4485)  time: 0.5193  data: 0.0002  max mem: 11551
[01:52:40.327422] Linear projection changed by 0.06982111930847168
[01:52:40.329483] Transformer weight changed by 0.009772720746695995
[01:52:40.331853] Epoch: [0]  [2800/8231]  eta: 0:47:13  lr: 0.000001  loss: 72.4147 (304.7622)  time: 0.5217  data: 0.0002  max mem: 11551
[01:53:06.319663] Linear projection changed by 0.07148087024688721
[01:53:06.323000] Transformer weight changed by 0.009976201690733433
[01:53:06.325838] Epoch: [0]  [2850/8231]  eta: 0:46:46  lr: 0.000001  loss: 80.9535 (304.7718)  time: 0.5216  data: 0.0003  max mem: 11551
[01:53:32.261680] Linear projection changed by 0.07311495393514633
[01:53:32.263914] Transformer weight changed by 0.010171799920499325
[01:53:32.266226] Epoch: [0]  [2900/8231]  eta: 0:46:20  lr: 0.000001  loss: 158.9452 (307.9994)  time: 0.5184  data: 0.0002  max mem: 11551
[01:53:58.245457] Linear projection changed by 0.0747082382440567
[01:53:58.247509] Transformer weight changed by 0.010363584384322166
[01:53:58.250370] Epoch: [0]  [2950/8231]  eta: 0:45:54  lr: 0.000001  loss: 71.3756 (307.3097)  time: 0.5188  data: 0.0002  max mem: 11551
[01:54:24.255192] Linear projection changed by 0.07662735134363174
[01:54:24.257294] Transformer weight changed by 0.010582475923001766
[01:54:24.259998] Epoch: [0]  [3000/8231]  eta: 0:45:28  lr: 0.000001  loss: 164.1881 (307.9751)  time: 0.5207  data: 0.0002  max mem: 11551
[01:54:50.214353] Linear projection changed by 0.07813560217618942
[01:54:50.216200] Transformer weight changed by 0.01076643168926239
[01:54:50.218421] Epoch: [0]  [3050/8231]  eta: 0:45:01  lr: 0.000001  loss: 110.7086 (310.7535)  time: 0.5212  data: 0.0002  max mem: 11551
[01:55:16.278913] Linear projection changed by 0.07964745163917542
[01:55:16.281486] Transformer weight changed by 0.010950029827654362
[01:55:16.284055] Epoch: [0]  [3100/8231]  eta: 0:44:35  lr: 0.000001  loss: 101.1385 (310.2595)  time: 0.5262  data: 0.0002  max mem: 11551
[01:55:42.239861] Linear projection changed by 0.08113038539886475
[01:55:42.241907] Transformer weight changed by 0.01112799346446991
[01:55:42.244188] Epoch: [0]  [3150/8231]  eta: 0:44:09  lr: 0.000001  loss: 132.8252 (308.6594)  time: 0.5189  data: 0.0002  max mem: 11551
[01:56:08.248044] Linear projection changed by 0.0829014703631401
[01:56:08.250233] Transformer weight changed by 0.011335284449160099
[01:56:08.252488] Epoch: [0]  [3200/8231]  eta: 0:43:43  lr: 0.000001  loss: 106.7313 (309.2572)  time: 0.5217  data: 0.0002  max mem: 11551
[01:56:34.240480] Linear projection changed by 0.08434834331274033
[01:56:34.241804] Transformer weight changed by 0.011507651768624783
[01:56:34.243879] Epoch: [0]  [3250/8231]  eta: 0:43:17  lr: 0.000001  loss: 202.9785 (309.2668)  time: 0.5206  data: 0.0002  max mem: 11551
[01:57:00.354222] Linear projection changed by 0.08578674495220184
[01:57:00.356337] Transformer weight changed by 0.011679571121931076
[01:57:00.359072] Epoch: [0]  [3300/8231]  eta: 0:42:51  lr: 0.000001  loss: 79.5589 (306.7381)  time: 0.5173  data: 0.0002  max mem: 11551
[01:57:26.294295] Linear projection changed by 0.0871492326259613
[01:57:26.296626] Transformer weight changed by 0.011845050379633904
[01:57:26.299118] Epoch: [0]  [3350/8231]  eta: 0:42:24  lr: 0.000001  loss: 126.4940 (306.1772)  time: 0.5182  data: 0.0003  max mem: 11551
[01:57:52.297886] Linear projection changed by 0.08870001137256622
[01:57:52.300075] Transformer weight changed by 0.012031425721943378
[01:57:52.302522] Epoch: [0]  [3400/8231]  eta: 0:41:58  lr: 0.000001  loss: 113.4814 (306.9264)  time: 0.5202  data: 0.0002  max mem: 11551
[01:58:18.112762] Linear projection changed by 0.09016972780227661
[01:58:18.114028] Transformer weight changed by 0.012200133875012398
[01:58:18.115447] Epoch: [0]  [3450/8231]  eta: 0:41:32  lr: 0.000001  loss: 82.3665 (306.8273)  time: 0.5186  data: 0.0002  max mem: 11551
[01:58:44.074134] Linear projection changed by 0.09167151153087616
[01:58:44.076224] Transformer weight changed by 0.012365488335490227
[01:58:44.078722] Epoch: [0]  [3500/8231]  eta: 0:41:05  lr: 0.000001  loss: 188.0106 (306.5200)  time: 0.5181  data: 0.0002  max mem: 11551
[01:59:09.996473] Linear projection changed by 0.09306536614894867
[01:59:09.998923] Transformer weight changed by 0.012522414326667786
[01:59:10.001354] Epoch: [0]  [3550/8231]  eta: 0:40:39  lr: 0.000001  loss: 101.3953 (312.2582)  time: 0.5180  data: 0.0002  max mem: 11551
[01:59:35.948797] Linear projection changed by 0.09466995298862457
[01:59:35.951137] Transformer weight changed by 0.012703891843557358
[01:59:35.953577] Epoch: [0]  [3600/8231]  eta: 0:40:13  lr: 0.000001  loss: 101.0951 (311.5470)  time: 0.5196  data: 0.0002  max mem: 11551
[02:00:01.906416] Linear projection changed by 0.09598644077777863
[02:00:01.909872] Transformer weight changed by 0.012852123007178307
[02:00:01.912194] Epoch: [0]  [3650/8231]  eta: 0:39:47  lr: 0.000001  loss: 89.4090 (310.3850)  time: 0.5207  data: 0.0002  max mem: 11551
[02:00:27.855644] Linear projection changed by 0.09726962447166443
[02:00:27.857788] Transformer weight changed by 0.01300300844013691
[02:00:27.860168] Epoch: [0]  [3700/8231]  eta: 0:39:21  lr: 0.000001  loss: 117.2714 (310.5423)  time: 0.5178  data: 0.0002  max mem: 11551
[02:00:53.785491] Linear projection changed by 0.09850496798753738
[02:00:53.788079] Transformer weight changed by 0.013148386962711811
[02:00:53.790862] Epoch: [0]  [3750/8231]  eta: 0:38:54  lr: 0.000001  loss: 70.7409 (315.8907)  time: 0.5176  data: 0.0002  max mem: 11551
[02:01:19.752100] Linear projection changed by 0.09992212802171707
[02:01:19.754427] Transformer weight changed by 0.01331549696624279
[02:01:19.756889] Epoch: [0]  [3800/8231]  eta: 0:38:28  lr: 0.000001  loss: 114.2540 (316.1451)  time: 0.5196  data: 0.0002  max mem: 11551
[02:01:45.609838] Linear projection changed by 0.10117519646883011
[02:01:45.612038] Transformer weight changed by 0.013456529937684536
[02:01:45.614425] Epoch: [0]  [3850/8231]  eta: 0:38:02  lr: 0.000001  loss: 149.3774 (314.6675)  time: 0.5167  data: 0.0002  max mem: 11551
[02:02:11.607924] Linear projection changed by 0.10237628221511841
[02:02:11.610148] Transformer weight changed by 0.013596759177744389
[02:02:11.612514] Epoch: [0]  [3900/8231]  eta: 0:37:36  lr: 0.000001  loss: 79.5444 (313.1076)  time: 0.5196  data: 0.0002  max mem: 11551
[02:02:37.516866] Linear projection changed by 0.10376095771789551
[02:02:37.518079] Transformer weight changed by 0.013746622949838638
[02:02:37.519154] Epoch: [0]  [3950/8231]  eta: 0:37:10  lr: 0.000001  loss: 96.3236 (313.4746)  time: 0.5176  data: 0.0002  max mem: 11551
[02:03:03.451701] Linear projection changed by 0.1052379459142685
[02:03:03.454024] Transformer weight changed by 0.013908904045820236
[02:03:03.456713] Epoch: [0]  [4000/8231]  eta: 0:36:44  lr: 0.000001  loss: 90.5597 (314.0487)  time: 0.5213  data: 0.0002  max mem: 11551
[02:03:29.439869] Linear projection changed by 0.10641047358512878
[02:03:29.442086] Transformer weight changed by 0.014042261056602001
[02:03:29.444527] Epoch: [0]  [4050/8231]  eta: 0:36:17  lr: 0.000001  loss: 248.6379 (316.1942)  time: 0.5214  data: 0.0002  max mem: 11551
[02:03:55.374665] Linear projection changed by 0.10761348158121109
[02:03:55.376828] Transformer weight changed by 0.014170435257256031
[02:03:55.379200] Epoch: [0]  [4100/8231]  eta: 0:35:51  lr: 0.000001  loss: 97.2220 (317.2097)  time: 0.5178  data: 0.0002  max mem: 11551
[02:04:21.340165] Linear projection changed by 0.10876094549894333
[02:04:21.342248] Transformer weight changed by 0.014298447407782078
[02:04:21.344492] Epoch: [0]  [4150/8231]  eta: 0:35:25  lr: 0.000001  loss: 131.9793 (317.1129)  time: 0.5188  data: 0.0003  max mem: 11551
[02:04:47.445986] Linear projection changed by 0.11024778336286545
[02:04:47.448159] Transformer weight changed by 0.014455202966928482
[02:04:47.450515] Epoch: [0]  [4200/8231]  eta: 0:34:59  lr: 0.000001  loss: 87.4488 (317.3948)  time: 0.5204  data: 0.0003  max mem: 11551
[02:05:13.366753] Linear projection changed by 0.1113799512386322
[02:05:13.369238] Transformer weight changed by 0.014583326876163483
[02:05:13.371732] Epoch: [0]  [4250/8231]  eta: 0:34:33  lr: 0.000001  loss: 113.1701 (317.1744)  time: 0.5197  data: 0.0002  max mem: 11551
[02:05:39.493985] Linear projection changed by 0.11249388754367828
[02:05:39.496178] Transformer weight changed by 0.01471364963799715
[02:05:39.498700] Epoch: [0]  [4300/8231]  eta: 0:34:07  lr: 0.000001  loss: 91.8720 (316.5006)  time: 0.5269  data: 0.0002  max mem: 11551
[02:06:05.415476] Linear projection changed by 0.1135595366358757
[02:06:05.417557] Transformer weight changed by 0.014836949296295643
[02:06:05.419651] Epoch: [0]  [4350/8231]  eta: 0:33:41  lr: 0.000001  loss: 159.8143 (317.4833)  time: 0.5186  data: 0.0002  max mem: 11551
[02:06:31.399521] Linear projection changed by 0.11476469039916992
[02:06:31.401638] Transformer weight changed by 0.014971097931265831
[02:06:31.403982] Epoch: [0]  [4400/8231]  eta: 0:33:15  lr: 0.000001  loss: 77.9265 (317.6338)  time: 0.5196  data: 0.0002  max mem: 11551
[02:06:57.355042] Linear projection changed by 0.11575347930192947
[02:06:57.357388] Transformer weight changed by 0.015087079256772995
[02:06:57.359863] Epoch: [0]  [4450/8231]  eta: 0:32:49  lr: 0.000001  loss: 148.5597 (317.0435)  time: 0.5202  data: 0.0003  max mem: 11551
[02:07:23.288497] Linear projection changed by 0.11673735827207565
[02:07:23.290602] Transformer weight changed by 0.015201044268906116
[02:07:23.292986] Epoch: [0]  [4500/8231]  eta: 0:32:23  lr: 0.000001  loss: 73.1059 (316.4096)  time: 0.5178  data: 0.0002  max mem: 11551
[02:07:49.174471] Linear projection changed by 0.11770287156105042
[02:07:49.176565] Transformer weight changed by 0.015313507057726383
[02:07:49.178766] Epoch: [0]  [4550/8231]  eta: 0:31:56  lr: 0.000001  loss: 203.0564 (316.9161)  time: 0.5189  data: 0.0003  max mem: 11551
[02:08:15.190319] Linear projection changed by 0.1188618391752243
[02:08:15.192638] Transformer weight changed by 0.015436231158673763
[02:08:15.195007] Epoch: [0]  [4600/8231]  eta: 0:31:30  lr: 0.000001  loss: 103.6928 (316.6141)  time: 0.5205  data: 0.0002  max mem: 11551
[02:08:41.132624] Linear projection changed by 0.11981061846017838
[02:08:41.134751] Transformer weight changed by 0.015546226873993874
[02:08:41.137924] Epoch: [0]  [4650/8231]  eta: 0:31:04  lr: 0.000001  loss: 248.3692 (321.6941)  time: 0.5205  data: 0.0002  max mem: 11551
[02:09:07.066266] Linear projection changed by 0.12072589248418808
[02:09:07.068742] Transformer weight changed by 0.015659525990486145
[02:09:07.071244] Epoch: [0]  [4700/8231]  eta: 0:30:38  lr: 0.000001  loss: 119.2026 (322.2306)  time: 0.5174  data: 0.0002  max mem: 11551
[02:09:33.030584] Linear projection changed by 0.1216561570763588
[02:09:33.032746] Transformer weight changed by 0.01576654054224491
[02:09:33.035143] Epoch: [0]  [4750/8231]  eta: 0:30:12  lr: 0.000001  loss: 100.9942 (321.2690)  time: 0.5174  data: 0.0002  max mem: 11551
[02:09:59.029841] Linear projection changed by 0.12284353375434875
[02:09:59.032010] Transformer weight changed by 0.01588629186153412
[02:09:59.034384] Epoch: [0]  [4800/8231]  eta: 0:29:46  lr: 0.000001  loss: 55.3923 (324.8596)  time: 0.5201  data: 0.0002  max mem: 11551
[02:10:24.953157] Linear projection changed by 0.12377001345157623
[02:10:24.955331] Transformer weight changed by 0.015992408618330956
[02:10:24.957656] Epoch: [0]  [4850/8231]  eta: 0:29:20  lr: 0.000001  loss: 99.0837 (323.8527)  time: 0.5203  data: 0.0002  max mem: 11551
[02:10:50.839691] Linear projection changed by 0.12466342747211456
[02:10:50.841942] Transformer weight changed by 0.016095099970698357
[02:10:50.844272] Epoch: [0]  [4900/8231]  eta: 0:28:54  lr: 0.000001  loss: 94.2053 (323.4531)  time: 0.5184  data: 0.0003  max mem: 11551
[02:11:16.773081] Linear projection changed by 0.12543965876102448
[02:11:16.775424] Transformer weight changed by 0.016201555728912354
[02:11:16.777781] Epoch: [0]  [4950/8231]  eta: 0:28:28  lr: 0.000002  loss: 83.0654 (322.6894)  time: 0.5190  data: 0.0003  max mem: 11551
[02:11:42.739814] Linear projection changed by 0.12631168961524963
[02:11:42.741286] Transformer weight changed by 0.016321765258908272
[02:11:42.742595] Epoch: [0]  [5000/8231]  eta: 0:28:01  lr: 0.000002  loss: 145.7967 (321.5186)  time: 0.5203  data: 0.0002  max mem: 11551
[02:12:08.658427] Linear projection changed by 0.12706753611564636
[02:12:08.660521] Transformer weight changed by 0.016420884057879448
[02:12:08.662717] Epoch: [0]  [5050/8231]  eta: 0:27:35  lr: 0.000002  loss: 86.8723 (321.1165)  time: 0.5197  data: 0.0002  max mem: 11551
[02:12:34.581506] Linear projection changed by 0.12792202830314636
[02:12:34.583812] Transformer weight changed by 0.016522614285349846
[02:12:34.585982] Epoch: [0]  [5100/8231]  eta: 0:27:09  lr: 0.000002  loss: 60.0040 (319.4254)  time: 0.5180  data: 0.0003  max mem: 11551
[02:13:00.481248] Linear projection changed by 0.12873706221580505
[02:13:00.483598] Transformer weight changed by 0.01662122830748558
[02:13:00.485767] Epoch: [0]  [5150/8231]  eta: 0:26:43  lr: 0.000002  loss: 162.2423 (319.2523)  time: 0.5177  data: 0.0002  max mem: 11551
[02:13:26.568600] Linear projection changed by 0.1296771764755249
[02:13:26.570706] Transformer weight changed by 0.016734648495912552
[02:13:26.572856] Epoch: [0]  [5200/8231]  eta: 0:26:17  lr: 0.000002  loss: 143.8396 (318.7010)  time: 0.5202  data: 0.0002  max mem: 11551
[02:13:52.525581] Linear projection changed by 0.1304894983768463
[02:13:52.527615] Transformer weight changed by 0.0168312918394804
[02:13:52.529771] Epoch: [0]  [5250/8231]  eta: 0:25:51  lr: 0.000002  loss: 135.3474 (317.8595)  time: 0.5207  data: 0.0002  max mem: 11551
[02:14:18.402173] Linear projection changed by 0.13128137588500977
[02:14:18.403384] Transformer weight changed by 0.01692263036966324
[02:14:18.404657] Epoch: [0]  [5300/8231]  eta: 0:25:25  lr: 0.000002  loss: 105.5660 (316.3760)  time: 0.5155  data: 0.0002  max mem: 11551
[02:14:44.356126] Linear projection changed by 0.1320168524980545
[02:14:44.358659] Transformer weight changed by 0.017008477821946144
[02:14:44.361804] Epoch: [0]  [5350/8231]  eta: 0:24:59  lr: 0.000002  loss: 87.3540 (321.1574)  time: 0.5182  data: 0.0003  max mem: 11551
[02:15:10.500651] Linear projection changed by 0.13300268352031708
[02:15:10.502790] Transformer weight changed by 0.01712629571557045
[02:15:10.505098] Epoch: [0]  [5400/8231]  eta: 0:24:33  lr: 0.000002  loss: 125.7094 (324.8252)  time: 0.5195  data: 0.0002  max mem: 11551
[02:15:36.415617] Linear projection changed by 0.13368386030197144
[02:15:36.417724] Transformer weight changed by 0.017216337844729424
[02:15:36.420134] Epoch: [0]  [5450/8231]  eta: 0:24:07  lr: 0.000002  loss: 105.3899 (323.6969)  time: 0.5193  data: 0.0002  max mem: 11551
[02:16:02.334686] Linear projection changed by 0.13444675505161285
[02:16:02.336950] Transformer weight changed by 0.01730283908545971
[02:16:02.339339] Epoch: [0]  [5500/8231]  eta: 0:23:41  lr: 0.000002  loss: 128.1624 (322.6237)  time: 0.5178  data: 0.0002  max mem: 11551
[02:16:28.265203] Linear projection changed by 0.13514654338359833
[02:16:28.267582] Transformer weight changed by 0.017390776425600052
[02:16:28.269817] Epoch: [0]  [5550/8231]  eta: 0:23:15  lr: 0.000002  loss: 156.0055 (322.8981)  time: 0.5182  data: 0.0002  max mem: 11551
[02:16:54.167077] Linear projection changed by 0.135942280292511
[02:16:54.169441] Transformer weight changed by 0.017490986734628677
[02:16:54.171835] Epoch: [0]  [5600/8231]  eta: 0:22:49  lr: 0.000002  loss: 87.5978 (322.4518)  time: 0.5167  data: 0.0002  max mem: 11551
[02:17:19.968266] Linear projection changed by 0.1366230696439743
[02:17:19.970387] Transformer weight changed by 0.017578164115548134
[02:17:19.972503] Epoch: [0]  [5650/8231]  eta: 0:22:23  lr: 0.000002  loss: 48.7260 (322.1988)  time: 0.5180  data: 0.0002  max mem: 11551
[02:17:45.902618] Linear projection changed by 0.13735485076904297
[02:17:45.904934] Transformer weight changed by 0.017657682299613953
[02:17:45.907247] Epoch: [0]  [5700/8231]  eta: 0:21:57  lr: 0.000002  loss: 109.5958 (321.8870)  time: 0.5181  data: 0.0002  max mem: 11551
[02:18:11.858564] Linear projection changed by 0.1380806416273117
[02:18:11.860594] Transformer weight changed by 0.01774485595524311
[02:18:11.862735] Epoch: [0]  [5750/8231]  eta: 0:21:30  lr: 0.000002  loss: 83.6991 (322.1173)  time: 0.5171  data: 0.0002  max mem: 11551
[02:18:37.803208] Linear projection changed by 0.13878440856933594
[02:18:37.805677] Transformer weight changed by 0.01785288006067276
[02:18:37.808084] Epoch: [0]  [5800/8231]  eta: 0:21:04  lr: 0.000002  loss: 71.9298 (322.3798)  time: 0.5198  data: 0.0003  max mem: 11551
[02:19:03.725991] Linear projection changed by 0.13934661448001862
[02:19:03.728068] Transformer weight changed by 0.017937669530510902
[02:19:03.730186] Epoch: [0]  [5850/8231]  eta: 0:20:38  lr: 0.000002  loss: 77.0761 (321.5453)  time: 0.5195  data: 0.0002  max mem: 11551
[02:19:29.700464] Linear projection changed by 0.13986685872077942
[02:19:29.702518] Transformer weight changed by 0.01801900379359722
[02:19:29.704713] Epoch: [0]  [5900/8231]  eta: 0:20:12  lr: 0.000002  loss: 84.5391 (321.0359)  time: 0.5186  data: 0.0002  max mem: 11551
[02:19:55.625954] Linear projection changed by 0.1404389590024948
[02:19:55.628049] Transformer weight changed by 0.01809661090373993
[02:19:55.630429] Epoch: [0]  [5950/8231]  eta: 0:19:46  lr: 0.000002  loss: 76.7198 (319.9073)  time: 0.5177  data: 0.0002  max mem: 11551
[02:20:21.597257] Linear projection changed by 0.14123795926570892
[02:20:21.599611] Transformer weight changed by 0.018194420263171196
[02:20:21.601989] Epoch: [0]  [6000/8231]  eta: 0:19:20  lr: 0.000002  loss: 54.9712 (318.5018)  time: 0.5205  data: 0.0002  max mem: 11551
[02:20:47.522559] Linear projection changed by 0.1417740285396576
[02:20:47.524826] Transformer weight changed by 0.01827916130423546
[02:20:47.527246] Epoch: [0]  [6050/8231]  eta: 0:18:54  lr: 0.000002  loss: 93.2187 (319.6927)  time: 0.5198  data: 0.0002  max mem: 11551
[02:21:13.342866] Linear projection changed by 0.14233078062534332
[02:21:13.344089] Transformer weight changed by 0.018363019451498985
[02:21:13.345244] Epoch: [0]  [6100/8231]  eta: 0:18:28  lr: 0.000002  loss: 76.1788 (320.3506)  time: 0.5163  data: 0.0002  max mem: 11551
[02:21:39.192135] Linear projection changed by 0.14288729429244995
[02:21:39.193369] Transformer weight changed by 0.018444042652845383
[02:21:39.194451] Epoch: [0]  [6150/8231]  eta: 0:18:02  lr: 0.000002  loss: 107.7698 (319.4975)  time: 0.5160  data: 0.0001  max mem: 11551
[02:22:05.134779] Linear projection changed by 0.14349305629730225
[02:22:05.136828] Transformer weight changed by 0.01853262074291706
[02:22:05.139157] Epoch: [0]  [6200/8231]  eta: 0:17:36  lr: 0.000002  loss: 104.8101 (320.1990)  time: 0.5202  data: 0.0002  max mem: 11551
[02:22:31.241802] Linear projection changed by 0.14412133395671844
[02:22:31.244551] Transformer weight changed by 0.018617084249854088
[02:22:31.247012] Epoch: [0]  [6250/8231]  eta: 0:17:10  lr: 0.000002  loss: 100.0525 (320.5678)  time: 0.5200  data: 0.0003  max mem: 11551
[02:22:57.167710] Linear projection changed by 0.1446940004825592
[02:22:57.169894] Transformer weight changed by 0.0186869315803051
[02:22:57.172160] Epoch: [0]  [6300/8231]  eta: 0:16:44  lr: 0.000002  loss: 86.7023 (319.5621)  time: 0.5180  data: 0.0002  max mem: 11551
[02:23:23.010801] Linear projection changed by 0.14529696106910706
[02:23:23.012891] Transformer weight changed by 0.018760817125439644
[02:23:23.015129] Epoch: [0]  [6350/8231]  eta: 0:16:18  lr: 0.000002  loss: 96.0339 (320.3551)  time: 0.5166  data: 0.0002  max mem: 11551
[02:23:49.186030] Linear projection changed by 0.14609365165233612
[02:23:49.188146] Transformer weight changed by 0.018843043595552444
[02:23:49.190474] Epoch: [0]  [6400/8231]  eta: 0:15:52  lr: 0.000002  loss: 91.8747 (319.7621)  time: 0.5201  data: 0.0002  max mem: 11551
[02:24:15.008545] Linear projection changed by 0.14661912620067596
[02:24:15.009843] Transformer weight changed by 0.01890847273170948
[02:24:15.011023] Epoch: [0]  [6450/8231]  eta: 0:15:26  lr: 0.000002  loss: 123.9318 (324.9440)  time: 0.5160  data: 0.0003  max mem: 11551
[02:24:40.959549] Linear projection changed by 0.1470882147550583
[02:24:40.961948] Transformer weight changed by 0.018976083025336266
[02:24:40.964359] Epoch: [0]  [6500/8231]  eta: 0:15:00  lr: 0.000002  loss: 59.7253 (323.8286)  time: 0.5179  data: 0.0002  max mem: 11551
[02:25:06.926412] Linear projection changed by 0.14762000739574432
[02:25:06.928683] Transformer weight changed by 0.01904672384262085
[02:25:06.930916] Epoch: [0]  [6550/8231]  eta: 0:14:34  lr: 0.000002  loss: 150.0287 (322.9605)  time: 0.5186  data: 0.0003  max mem: 11551
[02:25:32.920360] Linear projection changed by 0.1482386440038681
[02:25:32.922458] Transformer weight changed by 0.019130781292915344
[02:25:32.925053] Epoch: [0]  [6600/8231]  eta: 0:14:08  lr: 0.000002  loss: 70.4817 (321.7429)  time: 0.5201  data: 0.0002  max mem: 11551
[02:25:58.815337] Linear projection changed by 0.14867940545082092
[02:25:58.817833] Transformer weight changed by 0.01920579932630062
[02:25:58.820183] Epoch: [0]  [6650/8231]  eta: 0:13:42  lr: 0.000002  loss: 112.3098 (320.7530)  time: 0.5198  data: 0.0002  max mem: 11551
[02:26:24.748817] Linear projection changed by 0.14917530119419098
[02:26:24.751193] Transformer weight changed by 0.019273191690444946
[02:26:24.753571] Epoch: [0]  [6700/8231]  eta: 0:13:16  lr: 0.000002  loss: 184.7127 (320.9281)  time: 0.5183  data: 0.0003  max mem: 11551
[02:26:50.665563] Linear projection changed by 0.14973996579647064
[02:26:50.667819] Transformer weight changed by 0.019353553652763367
[02:26:50.670193] Epoch: [0]  [6750/8231]  eta: 0:12:50  lr: 0.000002  loss: 210.4359 (321.7025)  time: 0.5176  data: 0.0002  max mem: 11551
[02:27:16.640794] Linear projection changed by 0.15044763684272766
[02:27:16.642952] Transformer weight changed by 0.019437022507190704
[02:27:16.645395] Epoch: [0]  [6800/8231]  eta: 0:12:24  lr: 0.000002  loss: 37.0927 (326.7566)  time: 0.5204  data: 0.0002  max mem: 11551
[02:27:42.533042] Linear projection changed by 0.15084807574748993
[02:27:42.535325] Transformer weight changed by 0.01950688101351261
[02:27:42.537611] Epoch: [0]  [6850/8231]  eta: 0:11:58  lr: 0.000002  loss: 173.0922 (326.8360)  time: 0.5192  data: 0.0002  max mem: 11551
[02:28:08.435112] Linear projection changed by 0.15140019357204437
[02:28:08.437408] Transformer weight changed by 0.01957634463906288
[02:28:08.439802] Epoch: [0]  [6900/8231]  eta: 0:11:32  lr: 0.000002  loss: 129.3221 (326.4589)  time: 0.5169  data: 0.0002  max mem: 11551
[02:28:34.354453] Linear projection changed by 0.1519162952899933
[02:28:34.356550] Transformer weight changed by 0.019641324877738953
[02:28:34.358997] Epoch: [0]  [6950/8231]  eta: 0:11:06  lr: 0.000002  loss: 125.4480 (325.9314)  time: 0.5174  data: 0.0002  max mem: 11551
[02:29:00.322476] Linear projection changed by 0.15235494077205658
[02:29:00.324567] Transformer weight changed by 0.019724437966942787
[02:29:00.326785] Epoch: [0]  [7000/8231]  eta: 0:10:40  lr: 0.000002  loss: 123.2771 (325.3790)  time: 0.5199  data: 0.0002  max mem: 11551
[02:29:26.252053] Linear projection changed by 0.15278096497058868
[02:29:26.254182] Transformer weight changed by 0.019805504009127617
[02:29:26.256338] Epoch: [0]  [7050/8231]  eta: 0:10:14  lr: 0.000002  loss: 68.8804 (325.4010)  time: 0.5200  data: 0.0002  max mem: 11551
[02:29:52.172107] Linear projection changed by 0.15316292643547058
[02:29:52.174087] Transformer weight changed by 0.01987934671342373
[02:29:52.176187] Epoch: [0]  [7100/8231]  eta: 0:09:48  lr: 0.000002  loss: 81.6481 (324.5228)  time: 0.5177  data: 0.0002  max mem: 11551
[02:30:18.014358] Linear projection changed by 0.15360280871391296
[02:30:18.015862] Transformer weight changed by 0.019957564771175385
[02:30:18.017311] Epoch: [0]  [7150/8231]  eta: 0:09:22  lr: 0.000002  loss: 68.2029 (324.3150)  time: 0.5154  data: 0.0003  max mem: 11551
[02:30:44.106990] Linear projection changed by 0.15407660603523254
[02:30:44.109203] Transformer weight changed by 0.020043473690748215
[02:30:44.111508] Epoch: [0]  [7200/8231]  eta: 0:08:56  lr: 0.000002  loss: 103.8869 (324.8671)  time: 0.5192  data: 0.0003  max mem: 11551
[02:31:10.001883] Linear projection changed by 0.15451732277870178
[02:31:10.003877] Transformer weight changed by 0.0201109666377306
[02:31:10.005863] Epoch: [0]  [7250/8231]  eta: 0:08:30  lr: 0.000002  loss: 83.7196 (323.9181)  time: 0.5186  data: 0.0002  max mem: 11551
[02:31:35.939837] Linear projection changed by 0.15499630570411682
[02:31:35.941949] Transformer weight changed by 0.020180990919470787
[02:31:35.944081] Epoch: [0]  [7300/8231]  eta: 0:08:04  lr: 0.000002  loss: 89.2693 (323.0549)  time: 0.5175  data: 0.0002  max mem: 11551
[02:32:01.761628] Linear projection changed by 0.15543626248836517
[02:32:01.764322] Transformer weight changed by 0.020249266177415848
[02:32:01.766890] Epoch: [0]  [7350/8231]  eta: 0:07:38  lr: 0.000002  loss: 83.7361 (322.1113)  time: 0.5130  data: 0.0002  max mem: 11551
[02:32:27.879053] Linear projection changed by 0.1558137983083725
[02:32:27.881510] Transformer weight changed by 0.020309578627347946
[02:32:27.884221] Epoch: [0]  [7400/8231]  eta: 0:07:12  lr: 0.000002  loss: 148.5059 (322.4792)  time: 0.5188  data: 0.0002  max mem: 11551
[02:32:53.808343] Linear projection changed by 0.15618905425071716
[02:32:53.810909] Transformer weight changed by 0.020369431003928185
[02:32:53.813338] Epoch: [0]  [7450/8231]  eta: 0:06:46  lr: 0.000002  loss: 141.2085 (322.0693)  time: 0.5203  data: 0.0002  max mem: 11551
[02:33:19.762409] Linear projection changed by 0.15666700899600983
[02:33:19.765061] Transformer weight changed by 0.020443471148610115
[02:33:19.767859] Epoch: [0]  [7500/8231]  eta: 0:06:20  lr: 0.000002  loss: 132.7089 (323.4890)  time: 0.5182  data: 0.0002  max mem: 11551
[02:33:45.746508] Linear projection changed by 0.1569632887840271
[02:33:45.749136] Transformer weight changed by 0.020502829924225807
[02:33:45.751922] Epoch: [0]  [7550/8231]  eta: 0:05:54  lr: 0.000002  loss: 145.5944 (326.3929)  time: 0.5197  data: 0.0002  max mem: 11551
[02:34:11.728954] Linear projection changed by 0.1573798656463623
[02:34:11.730563] Transformer weight changed by 0.02056668885052204
[02:34:11.731883] Epoch: [0]  [7600/8231]  eta: 0:05:28  lr: 0.000002  loss: 128.6072 (326.1423)  time: 0.5192  data: 0.0003  max mem: 11551
[02:34:37.612614] Linear projection changed by 0.15774114429950714
[02:34:37.614313] Transformer weight changed by 0.0206303633749485
[02:34:37.615978] Epoch: [0]  [7650/8231]  eta: 0:05:02  lr: 0.000002  loss: 64.4010 (325.4994)  time: 0.5180  data: 0.0001  max mem: 11551
[02:35:03.540872] Linear projection changed by 0.15796181559562683
[02:35:03.543041] Transformer weight changed by 0.02069557085633278
[02:35:03.545473] Epoch: [0]  [7700/8231]  eta: 0:04:36  lr: 0.000002  loss: 106.7132 (326.2255)  time: 0.5183  data: 0.0002  max mem: 11551
[02:35:29.373926] Linear projection changed by 0.15841642022132874
[02:35:29.376295] Transformer weight changed by 0.02076610177755356
[02:35:29.378667] Epoch: [0]  [7750/8231]  eta: 0:04:10  lr: 0.000002  loss: 101.2945 (326.6325)  time: 0.5181  data: 0.0002  max mem: 11551
[02:35:55.307463] Linear projection changed by 0.15893857181072235
[02:35:55.309823] Transformer weight changed by 0.02083481289446354
[02:35:55.312326] Epoch: [0]  [7800/8231]  eta: 0:03:44  lr: 0.000002  loss: 101.2068 (325.8848)  time: 0.5207  data: 0.0002  max mem: 11551
[02:36:21.287222] Linear projection changed by 0.1593998223543167
[02:36:21.289507] Transformer weight changed by 0.02089175581932068
[02:36:21.291804] Epoch: [0]  [7850/8231]  eta: 0:03:18  lr: 0.000002  loss: 219.6516 (325.7376)  time: 0.5202  data: 0.0002  max mem: 11551
[02:36:47.125979] Linear projection changed by 0.1598060131072998
[02:36:47.127326] Transformer weight changed by 0.020954396575689316
[02:36:47.128496] Epoch: [0]  [7900/8231]  eta: 0:02:52  lr: 0.000002  loss: 90.0683 (327.4907)  time: 0.5152  data: 0.0001  max mem: 11551
[02:37:12.998958] Linear projection changed by 0.16027307510375977
[02:37:13.001056] Transformer weight changed by 0.021014967933297157
[02:37:13.003218] Epoch: [0]  [7950/8231]  eta: 0:02:26  lr: 0.000002  loss: 82.4456 (327.3217)  time: 0.5176  data: 0.0002  max mem: 11551
[02:37:38.991731] Linear projection changed by 0.16068822145462036
[02:37:38.993917] Transformer weight changed by 0.021072935312986374
[02:37:38.996286] Epoch: [0]  [8000/8231]  eta: 0:02:00  lr: 0.000002  loss: 98.7540 (326.5745)  time: 0.5200  data: 0.0002  max mem: 11551
[02:38:04.863814] Linear projection changed by 0.16097308695316315
[02:38:04.866344] Transformer weight changed by 0.02113170176744461
[02:38:04.868606] Epoch: [0]  [8050/8231]  eta: 0:01:34  lr: 0.000002  loss: 109.5671 (326.0521)  time: 0.5187  data: 0.0002  max mem: 11551
[02:38:30.741852] Linear projection changed by 0.1613590121269226
[02:38:30.744093] Transformer weight changed by 0.021191183477640152
[02:38:30.746846] Epoch: [0]  [8100/8231]  eta: 0:01:08  lr: 0.000002  loss: 90.5653 (325.4476)  time: 0.5180  data: 0.0002  max mem: 11551
[02:38:56.753829] Linear projection changed by 0.16164423525333405
[02:38:56.756116] Transformer weight changed by 0.021255476400256157
[02:38:56.758494] Epoch: [0]  [8150/8231]  eta: 0:00:42  lr: 0.000002  loss: 79.5498 (327.6443)  time: 0.5243  data: 0.0002  max mem: 11551
[02:39:22.701948] Linear projection changed by 0.16190551221370697
[02:39:22.704155] Transformer weight changed by 0.021338462829589844
[02:39:22.706507] Epoch: [0]  [8200/8231]  eta: 0:00:16  lr: 0.000002  loss: 61.6004 (326.5890)  time: 0.5188  data: 0.0002  max mem: 11551
[02:39:38.206415] Epoch: [0]  [8230/8231]  eta: 0:00:00  lr: 0.000002  loss: 56.3495 (327.0803)  time: 0.5164  data: 0.0002  max mem: 11551
[02:39:38.588233] Epoch: [0] Total time: 1:11:19 (0.5199 s / it)
[02:39:38.591912] Averaged stats: lr: 0.000002  loss: 56.3495 (327.3269)
/home/slimhy/conda/envs/shape2vecset/lib/python3.10/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/slimhy/conda/envs/shape2vecset/lib/python3.10/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[02:39:41.513763] Test:  [  0/211]  eta: 0:03:22  loss: 93.5055 (93.5055)  time: 0.9574  data: 0.8531  max mem: 11551
[02:39:44.004912] Test:  [ 50/211]  eta: 0:00:10  loss: 48.7857 (96.1196)  time: 0.0452  data: 0.0001  max mem: 11551
[02:39:46.256966] Test:  [100/211]  eta: 0:00:06  loss: 15.2445 (111.1213)  time: 0.0447  data: 0.0001  max mem: 11551
[02:39:48.427257] Test:  [150/211]  eta: 0:00:03  loss: 31.2578 (151.7742)  time: 0.0418  data: 0.0001  max mem: 11551
[02:39:50.482976] Test:  [200/211]  eta: 0:00:00  loss: 9.2758 (145.5466)  time: 0.0418  data: 0.0001  max mem: 11551
[02:39:50.879707] Test:  [210/211]  eta: 0:00:00  loss: 11.6783 (144.7752)  time: 0.0397  data: 0.0001  max mem: 11551
[02:39:51.026042] Test: Total time: 0:00:10 (0.0496 s / it)
[02:39:51.027768] * loss 204.354
[02:39:51.028929] loss on 421 validation shape pairs: 204.354
[02:39:51.031913] Training time 1:11:32
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:       epoch ‚ñÅ
wandb: epoch_1000x ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:      max_lr ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:      min_lr ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train_loss ‚ñÅ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÜ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÖ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñà‚ñÅ
wandb:  valid_loss ‚ñÅ
wandb: 
wandb: Run summary:
wandb:       epoch 0
wandb: epoch_1000x 999
wandb:      max_lr 0.0
wandb:      min_lr 0.0
wandb:  train_loss 1534.604
wandb:  valid_loss 204.35407
wandb: 
wandb: üöÄ View run ldm_finetune_debug_01__w4fkz3yz at: https://wandb.ai/shapewalk/shape2vecset/runs/kwmdb900
wandb: Ô∏è‚ö° View job at https://wandb.ai/shapewalk/shape2vecset/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExMTIyMDEzMQ==/version_details/v10
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231101_012814-kwmdb900/logs
