master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
| distributed init (rank 4): env://, gpu 4
| distributed init (rank 0): env://, gpu 0
| distributed init (rank 7): env://, gpu 7
| distributed init (rank 3): env://, gpu 3
| distributed init (rank 2): env://, gpu 2
| distributed init (rank 6): env://, gpu 6
| distributed init (rank 5): env://, gpu 5
| distributed init (rank 1): env://, gpu 1
[23:07:53.742111] Input args:
 {
    "accum_iter": 2,
    "ae": "kl_d512_m512_l8",
    "ae_pth": "output/graph_edit/ae/ae_m512.pth",
    "alt_ae_embeds": null,
    "batch_size": 16,
    "blr": 0.0001,
    "clip_grad": 3.0,
    "data_path": "/ibex/user/slimhy/ShapeWalk/",
    "data_type": "release",
    "dataset": "graphedits",
    "debug_mode": false,
    "debug_with_forward": false,
    "device": "cuda",
    "dist_backend": "nccl",
    "dist_eval": true,
    "dist_on_itp": false,
    "dist_url": "env://",
    "distributed": true,
    "epochs": 800,
    "eval": false,
    "exp_name": "ldm_scratch__ft",
    "fetch_keys": false,
    "ft_bert": false,
    "gpu": 0,
    "intensity_loss": false,
    "is_diff": true,
    "is_direct": false,
    "is_mlp": false,
    "is_nrl_listener": false,
    "layer_decay": 0.75,
    "local_rank": -1,
    "log_dir": "output/graph_edit/dm/ldm_scratch__ft",
    "lr": null,
    "max_edge_level": 1,
    "min_lr": 1e-08,
    "model": "kl_d512_m512_l8_d24_edm",
    "n_replicas": 1,
    "num_workers": 8,
    "output_dir": "output/graph_edit/dm/ldm_scratch__ft",
    "pin_mem": true,
    "plateau_scheduler": false,
    "point_cloud_size": 2048,
    "rank": 0,
    "resume": "output/graph_edit/dm/ldm_scratch__ft/checkpoint-42.pth",
    "resume_full_weights": true,
    "resume_mismatch": false,
    "resume_weights": false,
    "save_every_n": 5,
    "seed": 0,
    "start_epoch": 0,
    "text_model_name": "bert-base-uncased",
    "use_adam": false,
    "use_clip": false,
    "use_embeds": true,
    "valid_step": 5,
    "wandb_id": null,
    "warmup_epochs": 40,
    "weight_decay": 0.05,
    "world_size": 8
}
[23:07:53.744023] Job dir: /ibex/user/slimhy/Shape2VecSet/code
[23:07:53.745545] |Train| size = [9882]
[23:07:53.746015] |Valid| size = [4160]
[23:07:55.039757] Model = EDMTextCond(
  (edm_model): EDMConcatPrecond(
    (model): LatentArrayTransformer(
      (proj_in): Linear(in_features=16, out_features=512, bias=False)
      (transformer_blocks): ModuleList(
        (0-23): 24 x BasicTransformerBlock(
          (attn1): CrossAttention(
            (to_q): Linear(in_features=512, out_features=512, bias=False)
            (to_k): Linear(in_features=512, out_features=512, bias=False)
            (to_v): Linear(in_features=512, out_features=512, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=512, out_features=512, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ff): FeedForward(
            (net): Sequential(
              (0): GEGLU(
                (proj): Linear(in_features=512, out_features=4096, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=2048, out_features=512, bias=True)
            )
          )
          (attn2): CrossAttention(
            (to_q): Linear(in_features=512, out_features=512, bias=False)
            (to_k): Linear(in_features=512, out_features=512, bias=False)
            (to_v): Linear(in_features=512, out_features=512, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=512, out_features=512, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm1): AdaLayerNorm(
            (silu): SiLU()
            (linear): Linear(in_features=512, out_features=1024, bias=True)
            (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=False)
          )
          (norm2): AdaLayerNorm(
            (silu): SiLU()
            (linear): Linear(in_features=512, out_features=1024, bias=True)
            (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=False)
          )
          (norm3): AdaLayerNorm(
            (silu): SiLU()
            (linear): Linear(in_features=512, out_features=1024, bias=True)
            (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (ls2): Identity()
          (drop_path2): Identity()
          (ls3): Identity()
          (drop_path3): Identity()
        )
      )
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (proj_out): Linear(in_features=512, out_features=8, bias=False)
      (map_noise): PositionalEmbedding()
      (map_layer0): Linear(in_features=256, out_features=512, bias=True)
      (map_layer1): Linear(in_features=512, out_features=512, bias=True)
    )
  )
  (linear_proj): Linear(in_features=768, out_features=512, bias=True)
)
[23:07:55.040472] Params (M): 164.59
wandb: Currently logged in as: habib-slim (shapewalk). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /ibex/user/slimhy/Shape2VecSet/wandb/run-20240127_230756-kh4mr3ik
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ldm_scratch__ft__2cg5rfgy
wandb: ‚≠êÔ∏è View project at https://wandb.ai/shapewalk/shape2vecset
wandb: üöÄ View run at https://wandb.ai/shapewalk/shape2vecset/runs/kh4mr3ik
[23:08:01.409955] Model params:
 {
    "accum_iter": 2,
    "base_blr": 0.0001,
    "batch_size": 16,
    "clip_grad": 3.0,
    "dist_eval": true,
    "eff_batch_size": 256,
    "epochs": 800,
    "eval": false,
    "layer_decay": 0.75,
    "lr": 0.0001,
    "max_edge_level": 1,
    "min_lr": 1e-08,
    "model": "kl_d512_m512_l8_d24_edm",
    "resume": "output/graph_edit/dm/ldm_scratch__ft/checkpoint-42.pth",
    "resume_full_weights": true,
    "start_epoch": 0,
    "weight_decay": 0.05
}
[23:08:03.223474] Resume checkpoint output/graph_edit/dm/ldm_scratch__ft/checkpoint-42.pth
[23:08:04.507927] With optim & sched!
[23:08:04.522724] Loaded full EDM wrapper + DM weights from checkpoint.
[23:08:04.523401] Start training for 800 epochs
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[23:08:07.455371] Epoch: [43]  [  0/617]  eta: 0:30:05  lr: 0.000100  loss: 0.6986 (0.6986)  time: 2.9266  data: 1.2947  max mem: 17669
[23:08:42.357410] Epoch: [43]  [ 50/617]  eta: 0:07:00  lr: 0.000100  loss: 0.6902 (0.6934)  time: 0.6858  data: 0.0002  max mem: 18293
[23:09:17.179306] Epoch: [43]  [100/617]  eta: 0:06:11  lr: 0.000100  loss: 0.6899 (0.6936)  time: 0.6893  data: 0.0001  max mem: 18293
[23:09:51.707927] Epoch: [43]  [150/617]  eta: 0:05:31  lr: 0.000100  loss: 0.6992 (0.6950)  time: 0.6908  data: 0.0001  max mem: 18293
[23:10:26.241259] Epoch: [43]  [200/617]  eta: 0:04:53  lr: 0.000100  loss: 0.6963 (0.6953)  time: 0.6909  data: 0.0002  max mem: 18293
[23:11:00.800855] Epoch: [43]  [250/617]  eta: 0:04:17  lr: 0.000100  loss: 0.7035 (0.6958)  time: 0.6914  data: 0.0002  max mem: 18293
[23:11:35.364768] Epoch: [43]  [300/617]  eta: 0:03:42  lr: 0.000100  loss: 0.6956 (0.6953)  time: 0.6915  data: 0.0002  max mem: 18293
[23:12:09.930567] Epoch: [43]  [350/617]  eta: 0:03:06  lr: 0.000100  loss: 0.6934 (0.6958)  time: 0.6916  data: 0.0002  max mem: 18293
[23:12:44.531767] Epoch: [43]  [400/617]  eta: 0:02:31  lr: 0.000100  loss: 0.6903 (0.6962)  time: 0.6924  data: 0.0002  max mem: 18293
[23:13:19.141859] Epoch: [43]  [450/617]  eta: 0:01:56  lr: 0.000100  loss: 0.6866 (0.6956)  time: 0.6925  data: 0.0002  max mem: 18293
[23:13:53.899149] Epoch: [43]  [500/617]  eta: 0:01:21  lr: 0.000100  loss: 0.6953 (0.6956)  time: 0.6961  data: 0.0001  max mem: 18293
[23:14:28.736646] Epoch: [43]  [550/617]  eta: 0:00:46  lr: 0.000100  loss: 0.6947 (0.6957)  time: 0.6945  data: 0.0001  max mem: 18293
[23:15:03.317682] Epoch: [43]  [600/617]  eta: 0:00:11  lr: 0.000100  loss: 0.7003 (0.6955)  time: 0.6918  data: 0.0002  max mem: 18293
[23:15:14.380192] Epoch: [43]  [616/617]  eta: 0:00:00  lr: 0.000100  loss: 0.6916 (0.6956)  time: 0.6915  data: 0.0002  max mem: 18293
[23:15:14.683030] Epoch: [43] Total time: 0:07:10 (0.6972 s / it)
[23:15:14.684759] Averaged stats: lr: 0.000100  loss: 0.6916 (0.6942)
[23:15:16.191903] Epoch: [44]  [  0/617]  eta: 0:15:24  lr: 0.000100  loss: 0.6984 (0.6984)  time: 1.4977  data: 0.8467  max mem: 18293
[23:15:50.811015] Epoch: [44]  [ 50/617]  eta: 0:06:41  lr: 0.000100  loss: 0.6848 (0.6942)  time: 0.6921  data: 0.0001  max mem: 18293
[23:16:25.416670] Epoch: [44]  [100/617]  eta: 0:06:01  lr: 0.000100  loss: 0.6908 (0.6931)  time: 0.6921  data: 0.0001  max mem: 18293
[23:17:00.038312] Epoch: [44]  [150/617]  eta: 0:05:25  lr: 0.000100  loss: 0.6893 (0.6942)  time: 0.6924  data: 0.0001  max mem: 18293
[23:17:34.627271] Epoch: [44]  [200/617]  eta: 0:04:50  lr: 0.000100  loss: 0.6901 (0.6928)  time: 0.6918  data: 0.0001  max mem: 18293
[23:18:09.258748] Epoch: [44]  [250/617]  eta: 0:04:15  lr: 0.000100  loss: 0.6798 (0.6921)  time: 0.6921  data: 0.0001  max mem: 18293
[23:18:44.194724] Epoch: [44]  [300/617]  eta: 0:03:40  lr: 0.000100  loss: 0.6928 (0.6926)  time: 0.7040  data: 0.0001  max mem: 18293
[23:19:18.922291] Epoch: [44]  [350/617]  eta: 0:03:05  lr: 0.000100  loss: 0.6857 (0.6920)  time: 0.6980  data: 0.0001  max mem: 18293
[23:19:53.591551] Epoch: [44]  [400/617]  eta: 0:02:30  lr: 0.000100  loss: 0.7067 (0.6928)  time: 0.6924  data: 0.0001  max mem: 18293
[23:20:28.283355] Epoch: [44]  [450/617]  eta: 0:01:56  lr: 0.000100  loss: 0.6927 (0.6930)  time: 0.6927  data: 0.0001  max mem: 18293
[23:21:02.924447] Epoch: [44]  [500/617]  eta: 0:01:21  lr: 0.000100  loss: 0.6896 (0.6930)  time: 0.6928  data: 0.0001  max mem: 18293
[23:21:37.579902] Epoch: [44]  [550/617]  eta: 0:00:46  lr: 0.000100  loss: 0.7017 (0.6932)  time: 0.6941  data: 0.0002  max mem: 18293
[23:22:12.214988] Epoch: [44]  [600/617]  eta: 0:00:11  lr: 0.000100  loss: 0.7083 (0.6940)  time: 0.6927  data: 0.0001  max mem: 18293
[23:22:23.284713] Epoch: [44]  [616/617]  eta: 0:00:00  lr: 0.000100  loss: 0.7044 (0.6941)  time: 0.6921  data: 0.0002  max mem: 18293
[23:22:23.604584] Epoch: [44] Total time: 0:07:08 (0.6952 s / it)
[23:22:23.608205] Averaged stats: lr: 0.000100  loss: 0.7044 (0.6945)
[23:22:24.971256] Epoch: [45]  [  0/617]  eta: 0:13:55  lr: 0.000100  loss: 0.7019 (0.7019)  time: 1.3536  data: 0.6132  max mem: 18293
[23:22:59.603882] Epoch: [45]  [ 50/617]  eta: 0:06:40  lr: 0.000100  loss: 0.6978 (0.6970)  time: 0.6922  data: 0.0002  max mem: 18293
[23:23:34.329109] Epoch: [45]  [100/617]  eta: 0:06:01  lr: 0.000100  loss: 0.6944 (0.6960)  time: 0.6919  data: 0.0001  max mem: 18293
[23:24:09.060221] Epoch: [45]  [150/617]  eta: 0:05:26  lr: 0.000100  loss: 0.6979 (0.6958)  time: 0.6926  data: 0.0001  max mem: 18293
[23:24:43.797751] Epoch: [45]  [200/617]  eta: 0:04:50  lr: 0.000100  loss: 0.6909 (0.6948)  time: 0.6977  data: 0.0001  max mem: 18293
[23:25:18.415456] Epoch: [45]  [250/617]  eta: 0:04:15  lr: 0.000100  loss: 0.7011 (0.6950)  time: 0.6926  data: 0.0001  max mem: 18293
[23:25:53.041252] Epoch: [45]  [300/617]  eta: 0:03:40  lr: 0.000100  loss: 0.7024 (0.6963)  time: 0.6927  data: 0.0001  max mem: 18293
[23:26:27.654882] Epoch: [45]  [350/617]  eta: 0:03:05  lr: 0.000100  loss: 0.6985 (0.6961)  time: 0.6923  data: 0.0002  max mem: 18293
[23:27:02.283156] Epoch: [45]  [400/617]  eta: 0:02:30  lr: 0.000100  loss: 0.6904 (0.6952)  time: 0.6922  data: 0.0001  max mem: 18293
[23:27:36.903474] Epoch: [45]  [450/617]  eta: 0:01:55  lr: 0.000100  loss: 0.6916 (0.6950)  time: 0.6919  data: 0.0001  max mem: 18293
[23:28:11.596743] Epoch: [45]  [500/617]  eta: 0:01:21  lr: 0.000100  loss: 0.7048 (0.6956)  time: 0.6959  data: 0.0001  max mem: 18293
[23:28:46.291102] Epoch: [45]  [550/617]  eta: 0:00:46  lr: 0.000100  loss: 0.6887 (0.6953)  time: 0.6922  data: 0.0001  max mem: 18293
[23:29:21.047325] Epoch: [45]  [600/617]  eta: 0:00:11  lr: 0.000100  loss: 0.6999 (0.6953)  time: 0.6985  data: 0.0001  max mem: 18293
[23:29:32.132342] Epoch: [45]  [616/617]  eta: 0:00:00  lr: 0.000100  loss: 0.6885 (0.6952)  time: 0.6929  data: 0.0001  max mem: 18293
[23:29:32.352876] Epoch: [45] Total time: 0:07:08 (0.6949 s / it)
[23:29:32.370170] Averaged stats: lr: 0.000100  loss: 0.6885 (0.6939)
[23:29:35.153794] Test:  [ 0/33]  eta: 0:00:26  loss: 0.7100 (0.7100)  time: 0.8160  data: 0.5840  max mem: 18293
[23:29:42.037267] Test:  [32/33]  eta: 0:00:00  loss: 0.6899 (0.6950)  time: 0.2131  data: 0.0001  max mem: 18293
[23:29:42.142488] Test: Total time: 0:00:07 (0.2365 s / it)
[23:29:42.146373] * loss 0.693
[23:29:42.147227] loss on 4160 validation shape pairs: 0.693
[23:29:43.503950] Epoch: [46]  [  0/617]  eta: 0:13:53  lr: 0.000100  loss: 0.6829 (0.6829)  time: 1.3510  data: 0.6102  max mem: 18293
[23:30:18.161234] Epoch: [46]  [ 50/617]  eta: 0:06:40  lr: 0.000100  loss: 0.7007 (0.6987)  time: 0.6925  data: 0.0001  max mem: 18293
[23:30:52.777244] Epoch: [46]  [100/617]  eta: 0:06:01  lr: 0.000100  loss: 0.6862 (0.6952)  time: 0.6926  data: 0.0001  max mem: 18293
[23:31:27.434594] Epoch: [46]  [150/617]  eta: 0:05:25  lr: 0.000100  loss: 0.6818 (0.6938)  time: 0.6927  data: 0.0001  max mem: 18293
[23:32:02.072684] Epoch: [46]  [200/617]  eta: 0:04:50  lr: 0.000100  loss: 0.6829 (0.6925)  time: 0.6927  data: 0.0001  max mem: 18293
[23:32:36.751965] Epoch: [46]  [250/617]  eta: 0:04:15  lr: 0.000100  loss: 0.6946 (0.6926)  time: 0.6933  data: 0.0001  max mem: 18293
[23:33:11.419759] Epoch: [46]  [300/617]  eta: 0:03:40  lr: 0.000100  loss: 0.6938 (0.6929)  time: 0.6935  data: 0.0001  max mem: 18293
[23:33:46.646701] Epoch: [46]  [350/617]  eta: 0:03:05  lr: 0.000100  loss: 0.6990 (0.6936)  time: 0.6940  data: 0.0001  max mem: 18293
[23:34:21.426143] Epoch: [46]  [400/617]  eta: 0:02:31  lr: 0.000100  loss: 0.7058 (0.6941)  time: 0.7006  data: 0.0001  max mem: 18293
[23:34:56.093681] Epoch: [46]  [450/617]  eta: 0:01:56  lr: 0.000100  loss: 0.6975 (0.6943)  time: 0.6924  data: 0.0001  max mem: 18293
[23:35:30.713620] Epoch: [46]  [500/617]  eta: 0:01:21  lr: 0.000100  loss: 0.7008 (0.6946)  time: 0.6924  data: 0.0001  max mem: 18293
[23:36:05.341123] Epoch: [46]  [550/617]  eta: 0:00:46  lr: 0.000100  loss: 0.6969 (0.6945)  time: 0.6922  data: 0.0001  max mem: 18293
[23:36:39.971746] Epoch: [46]  [600/617]  eta: 0:00:11  lr: 0.000100  loss: 0.6877 (0.6944)  time: 0.6930  data: 0.0001  max mem: 18293
[23:36:51.061371] Epoch: [46]  [616/617]  eta: 0:00:00  lr: 0.000100  loss: 0.6941 (0.6942)  time: 0.6931  data: 0.0001  max mem: 18293
[23:36:51.279996] Epoch: [46] Total time: 0:07:09 (0.6955 s / it)
[23:36:51.317510] Averaged stats: lr: 0.000100  loss: 0.6941 (0.6935)
[23:36:52.678281] Epoch: [47]  [  0/617]  eta: 0:13:52  lr: 0.000100  loss: 0.7146 (0.7146)  time: 1.3497  data: 0.5479  max mem: 18293
[23:37:27.294189] Epoch: [47]  [ 50/617]  eta: 0:06:39  lr: 0.000100  loss: 0.6955 (0.6972)  time: 0.6920  data: 0.0001  max mem: 18293
[23:38:01.963115] Epoch: [47]  [100/617]  eta: 0:06:01  lr: 0.000100  loss: 0.6951 (0.6973)  time: 0.6944  data: 0.0001  max mem: 18293
[23:38:36.811953] Epoch: [47]  [150/617]  eta: 0:05:26  lr: 0.000100  loss: 0.6984 (0.6973)  time: 0.6979  data: 0.0001  max mem: 18293
[23:39:11.554976] Epoch: [47]  [200/617]  eta: 0:04:50  lr: 0.000100  loss: 0.6860 (0.6961)  time: 0.6930  data: 0.0001  max mem: 18293
[23:39:46.184131] Epoch: [47]  [250/617]  eta: 0:04:15  lr: 0.000100  loss: 0.6901 (0.6965)  time: 0.6928  data: 0.0001  max mem: 18293
[23:40:20.936471] Epoch: [47]  [300/617]  eta: 0:03:40  lr: 0.000100  loss: 0.6827 (0.6958)  time: 0.6931  data: 0.0001  max mem: 18293
[23:40:55.571981] Epoch: [47]  [350/617]  eta: 0:03:05  lr: 0.000100  loss: 0.6969 (0.6956)  time: 0.6925  data: 0.0001  max mem: 18293
[23:41:30.220669] Epoch: [47]  [400/617]  eta: 0:02:30  lr: 0.000100  loss: 0.6830 (0.6953)  time: 0.6930  data: 0.0001  max mem: 18293
[23:42:04.851756] Epoch: [47]  [450/617]  eta: 0:01:56  lr: 0.000100  loss: 0.6862 (0.6952)  time: 0.6930  data: 0.0001  max mem: 18293
[23:42:39.514713] Epoch: [47]  [500/617]  eta: 0:01:21  lr: 0.000100  loss: 0.6931 (0.6945)  time: 0.6927  data: 0.0001  max mem: 18293
[23:43:14.237560] Epoch: [47]  [550/617]  eta: 0:00:46  lr: 0.000100  loss: 0.6867 (0.6947)  time: 0.6969  data: 0.0001  max mem: 18293
[23:43:49.067357] Epoch: [47]  [600/617]  eta: 0:00:11  lr: 0.000100  loss: 0.6947 (0.6948)  time: 0.6970  data: 0.0001  max mem: 18293
[23:44:00.146992] Epoch: [47]  [616/617]  eta: 0:00:00  lr: 0.000100  loss: 0.6852 (0.6945)  time: 0.6926  data: 0.0001  max mem: 18293
[23:44:00.376977] Epoch: [47] Total time: 0:07:09 (0.6954 s / it)
[23:44:00.385574] Averaged stats: lr: 0.000100  loss: 0.6852 (0.6932)
[23:44:01.749486] Epoch: [48]  [  0/617]  eta: 0:13:55  lr: 0.000100  loss: 0.6665 (0.6665)  time: 1.3547  data: 0.6101  max mem: 18293
[23:44:36.483773] Epoch: [48]  [ 50/617]  eta: 0:06:41  lr: 0.000100  loss: 0.7009 (0.6942)  time: 0.6924  data: 0.0001  max mem: 18293
[23:45:11.112555] Epoch: [48]  [100/617]  eta: 0:06:01  lr: 0.000100  loss: 0.6985 (0.6967)  time: 0.6927  data: 0.0001  max mem: 18293
[23:45:45.745722] Epoch: [48]  [150/617]  eta: 0:05:25  lr: 0.000100  loss: 0.6956 (0.6952)  time: 0.6930  data: 0.0001  max mem: 18293
[23:46:20.372682] Epoch: [48]  [200/617]  eta: 0:04:50  lr: 0.000100  loss: 0.6861 (0.6954)  time: 0.6924  data: 0.0002  max mem: 18293
[23:46:55.027814] Epoch: [48]  [250/617]  eta: 0:04:15  lr: 0.000100  loss: 0.6854 (0.6950)  time: 0.6930  data: 0.0001  max mem: 18293
[23:47:29.667818] Epoch: [48]  [300/617]  eta: 0:03:40  lr: 0.000100  loss: 0.6914 (0.6947)  time: 0.6928  data: 0.0002  max mem: 18293
[23:48:04.399388] Epoch: [48]  [350/617]  eta: 0:03:05  lr: 0.000100  loss: 0.6898 (0.6946)  time: 0.6980  data: 0.0001  max mem: 18293
[23:48:39.139089] Epoch: [48]  [400/617]  eta: 0:02:30  lr: 0.000100  loss: 0.6847 (0.6944)  time: 0.6965  data: 0.0002  max mem: 18293
[23:49:14.061859] Epoch: [48]  [450/617]  eta: 0:01:56  lr: 0.000100  loss: 0.6871 (0.6940)  time: 0.6974  data: 0.0001  max mem: 18293
[23:49:48.691571] Epoch: [48]  [500/617]  eta: 0:01:21  lr: 0.000100  loss: 0.6956 (0.6937)  time: 0.6922  data: 0.0001  max mem: 18293
[23:50:23.324060] Epoch: [48]  [550/617]  eta: 0:00:46  lr: 0.000100  loss: 0.6841 (0.6934)  time: 0.6929  data: 0.0002  max mem: 18293
slurmstepd: error: *** JOB 31875739 ON gpu213-06 CANCELLED AT 2024-01-27T23:50:28 ***
