WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
| distributed init (rank 3): env://, gpu 3
| distributed init (rank 0): env://, gpu 0
| distributed init (rank 1): env://, gpu 1
| distributed init (rank 2): env://, gpu 2
[01:37:45.960600] Input args:
 {
    "accum_iter": 1,
    "ae": "kl_d512_m512_l8",
    "ae_pth": null,
    "alt_ae_embeds": "pc_ae",
    "batch_size": 32,
    "blr": 0.0001,
    "clip_grad": 3.0,
    "data_path": "/ibex/user/slimhy/ShapeWalk_RND/",
    "data_type": "release",
    "dataset": "graphedits",
    "debug_mode": false,
    "debug_with_forward": false,
    "device": "cuda",
    "dist_backend": "nccl",
    "dist_eval": true,
    "dist_on_itp": false,
    "dist_url": "env://",
    "distributed": true,
    "epochs": 800,
    "eval": false,
    "exp_name": "mlp_mapper_bert_bneck_256_pcae_ft_bert",
    "fetch_keys": false,
    "ft_bert": true,
    "gpu": 0,
    "intensity_loss": false,
    "is_diff": false,
    "is_mlp": true,
    "is_nrl_listener": false,
    "layer_decay": 0.75,
    "local_rank": -1,
    "log_dir": "output/graph_edit/dm/mlp_mapper_bert_bneck_256_pcae_ft_bert",
    "lr": 0.0001,
    "max_edge_level": 1,
    "min_lr": 1e-06,
    "model": "mlp_mapper_bert_bneck_256_pcae",
    "num_workers": 4,
    "output_dir": "output/graph_edit/dm/mlp_mapper_bert_bneck_256_pcae_ft_bert",
    "pin_mem": true,
    "plateau_scheduler": false,
    "point_cloud_size": 2048,
    "rank": 0,
    "resume": "",
    "resume_full_weights": false,
    "resume_weights": false,
    "seed": 0,
    "start_epoch": 0,
    "text_model_name": "bert-base-uncased",
    "use_adam": false,
    "use_clip": false,
    "use_embeds": true,
    "valid_step": 5,
    "wandb_id": null,
    "warmup_epochs": 40,
    "weight_decay": 0.05,
    "world_size": 4
}
[01:37:45.961054] Job dir: /ibex/user/slimhy/Shape2VecSet/code
[01:37:45.961722] |Train| size = [228000]
[01:37:45.961746] |Valid| size = [2976]
[01:37:52.427537] Loaded BERT model in [fine-tuning] mode.
[01:37:52.461333] Model = MLPLatentMapperBottlenecked(
  (editor): MLP(
    (net): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=256, out_features=256, bias=True)
      (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): ReLU(inplace=True)
      (7): Dropout(p=0.1, inplace=False)
      (8): Linear(in_features=256, out_features=256, bias=True)
      (9): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (10): ReLU(inplace=True)
      (11): Dropout(p=0.1, inplace=False)
      (12): Linear(in_features=256, out_features=256, bias=False)
    )
  )
  (shape_encoder): MLP(
    (net): Sequential(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=256, out_features=256, bias=True)
    )
  )
  (mag_mlp): MLP(
    (net): Sequential(
      (0): Linear(in_features=1024, out_features=256, bias=True)
      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Linear(in_features=256, out_features=128, bias=True)
      (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Linear(in_features=128, out_features=64, bias=True)
      (7): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): ReLU(inplace=True)
      (9): Linear(in_features=64, out_features=1, bias=True)
      (10): ReLU()
    )
  )
)
[01:37:52.461387] Params (M): 0.90
wandb: Currently logged in as: habib-slim (shapewalk). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /ibex/user/slimhy/Shape2VecSet/wandb/run-20231114_013754-pzvo9doq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mlp_mapper_bert_bneck_256_pcae_ft_bert__92yysfxz
wandb: ‚≠êÔ∏è View project at https://wandb.ai/shapewalk/shape2vecset
wandb: üöÄ View run at https://wandb.ai/shapewalk/shape2vecset/runs/pzvo9doq
[01:38:01.085865] Model params:
 {
    "accum_iter": 1,
    "base_blr": 0.0001,
    "batch_size": 32,
    "clip_grad": 3.0,
    "dist_eval": true,
    "eff_batch_size": 128,
    "epochs": 800,
    "eval": false,
    "layer_decay": 0.75,
    "lr": 0.0001,
    "max_edge_level": 1,
    "min_lr": 1e-06,
    "model": "mlp_mapper_bert_bneck_256_pcae",
    "resume": "",
    "resume_full_weights": false,
    "start_epoch": 0,
    "weight_decay": 0.05
}
[01:38:01.093255] Start training for 800 epochs
[01:38:05.578475] Epoch: [0]  [   0/7125]  eta: 8:49:55  lr: 0.000000  loss: 0.0294 (0.0294)  time: 4.4626  data: 0.6819  max mem: 1301
[01:38:13.596917] Epoch: [0]  [  50/7125]  eta: 0:28:51  lr: 0.000000  loss: 0.0314 (0.0295)  time: 0.1616  data: 0.0002  max mem: 3357
[01:38:21.351695] Epoch: [0]  [ 100/7125]  eta: 0:23:27  lr: 0.000000  loss: 0.0275 (0.0296)  time: 0.1515  data: 0.0001  max mem: 3357
[01:38:29.154539] Epoch: [0]  [ 150/7125]  eta: 0:21:34  lr: 0.000000  loss: 0.0261 (0.0295)  time: 0.1607  data: 0.0002  max mem: 3357
[01:38:37.106753] Epoch: [0]  [ 200/7125]  eta: 0:20:39  lr: 0.000000  loss: 0.0284 (0.0294)  time: 0.1581  data: 0.0002  max mem: 3372
[01:38:45.180820] Epoch: [0]  [ 250/7125]  eta: 0:20:06  lr: 0.000000  loss: 0.0296 (0.0291)  time: 0.1662  data: 0.0002  max mem: 3372
[01:38:53.025115] Epoch: [0]  [ 300/7125]  eta: 0:19:36  lr: 0.000000  loss: 0.0263 (0.0288)  time: 0.1492  data: 0.0002  max mem: 3372
[01:39:01.067523] Epoch: [0]  [ 350/7125]  eta: 0:19:16  lr: 0.000000  loss: 0.0309 (0.0290)  time: 0.1618  data: 0.0002  max mem: 3372
[01:39:09.278606] Epoch: [0]  [ 400/7125]  eta: 0:19:02  lr: 0.000000  loss: 0.0281 (0.0291)  time: 0.1582  data: 0.0002  max mem: 3372
[01:39:17.488892] Epoch: [0]  [ 450/7125]  eta: 0:18:50  lr: 0.000000  loss: 0.0283 (0.0292)  time: 0.1590  data: 0.0002  max mem: 3372
[01:39:25.501809] Epoch: [0]  [ 500/7125]  eta: 0:18:35  lr: 0.000000  loss: 0.0258 (0.0290)  time: 0.1619  data: 0.0002  max mem: 3466
[01:39:33.354207] Epoch: [0]  [ 550/7125]  eta: 0:18:20  lr: 0.000000  loss: 0.0278 (0.0291)  time: 0.1632  data: 0.0002  max mem: 3466
[01:39:41.178909] Epoch: [0]  [ 600/7125]  eta: 0:18:06  lr: 0.000000  loss: 0.0296 (0.0290)  time: 0.1618  data: 0.0002  max mem: 3466
[01:39:49.331385] Epoch: [0]  [ 650/7125]  eta: 0:17:56  lr: 0.000000  loss: 0.0266 (0.0290)  time: 0.1579  data: 0.0001  max mem: 3466
[01:39:57.125227] Epoch: [0]  [ 700/7125]  eta: 0:17:43  lr: 0.000000  loss: 0.0242 (0.0290)  time: 0.1565  data: 0.0002  max mem: 3466
[01:40:05.099251] Epoch: [0]  [ 750/7125]  eta: 0:17:32  lr: 0.000000  loss: 0.0239 (0.0287)  time: 0.1624  data: 0.0002  max mem: 3466
[01:40:12.798165] Epoch: [0]  [ 800/7125]  eta: 0:17:19  lr: 0.000000  loss: 0.0273 (0.0287)  time: 0.1542  data: 0.0002  max mem: 3466
[01:40:20.581560] Epoch: [0]  [ 850/7125]  eta: 0:17:08  lr: 0.000000  loss: 0.0297 (0.0287)  time: 0.1504  data: 0.0002  max mem: 3466
[01:40:28.294975] Epoch: [0]  [ 900/7125]  eta: 0:16:56  lr: 0.000000  loss: 0.0285 (0.0287)  time: 0.1507  data: 0.0002  max mem: 3466
[01:40:36.346682] Epoch: [0]  [ 950/7125]  eta: 0:16:47  lr: 0.000000  loss: 0.0289 (0.0288)  time: 0.1614  data: 0.0002  max mem: 3466
[01:40:44.245964] Epoch: [0]  [1000/7125]  eta: 0:16:37  lr: 0.000000  loss: 0.0272 (0.0288)  time: 0.1572  data: 0.0002  max mem: 3466
[01:40:52.384229] Epoch: [0]  [1050/7125]  eta: 0:16:29  lr: 0.000000  loss: 0.0272 (0.0287)  time: 0.1638  data: 0.0001  max mem: 3466
[01:41:00.524077] Epoch: [0]  [1100/7125]  eta: 0:16:21  lr: 0.000000  loss: 0.0256 (0.0286)  time: 0.1659  data: 0.0001  max mem: 3466
[01:41:08.523076] Epoch: [0]  [1150/7125]  eta: 0:16:12  lr: 0.000000  loss: 0.0309 (0.0286)  time: 0.1582  data: 0.0002  max mem: 3466
[01:41:16.531426] Epoch: [0]  [1200/7125]  eta: 0:16:03  lr: 0.000000  loss: 0.0253 (0.0286)  time: 0.1607  data: 0.0001  max mem: 3466
[01:41:24.370101] Epoch: [0]  [1250/7125]  eta: 0:15:54  lr: 0.000000  loss: 0.0246 (0.0285)  time: 0.1592  data: 0.0001  max mem: 3466
[01:41:32.373399] Epoch: [0]  [1300/7125]  eta: 0:15:45  lr: 0.000000  loss: 0.0268 (0.0286)  time: 0.1627  data: 0.0001  max mem: 3466
[01:41:40.101715] Epoch: [0]  [1350/7125]  eta: 0:15:35  lr: 0.000000  loss: 0.0248 (0.0286)  time: 0.1572  data: 0.0002  max mem: 3466
[01:41:48.070614] Epoch: [0]  [1400/7125]  eta: 0:15:27  lr: 0.000000  loss: 0.0271 (0.0286)  time: 0.1656  data: 0.0002  max mem: 3466
[01:41:56.007796] Epoch: [0]  [1450/7125]  eta: 0:15:18  lr: 0.000001  loss: 0.0274 (0.0286)  time: 0.1571  data: 0.0002  max mem: 3498
[01:42:03.631253] Epoch: [0]  [1500/7125]  eta: 0:15:08  lr: 0.000001  loss: 0.0277 (0.0286)  time: 0.1537  data: 0.0002  max mem: 3498
[01:42:11.962349] Epoch: [0]  [1550/7125]  eta: 0:15:01  lr: 0.000001  loss: 0.0267 (0.0286)  time: 0.1697  data: 0.0002  max mem: 3498
[01:42:19.834376] Epoch: [0]  [1600/7125]  eta: 0:14:52  lr: 0.000001  loss: 0.0305 (0.0286)  time: 0.1593  data: 0.0002  max mem: 3498
[01:42:27.770098] Epoch: [0]  [1650/7125]  eta: 0:14:44  lr: 0.000001  loss: 0.0288 (0.0286)  time: 0.1515  data: 0.0002  max mem: 3498
[01:42:35.963465] Epoch: [0]  [1700/7125]  eta: 0:14:36  lr: 0.000001  loss: 0.0278 (0.0287)  time: 0.1649  data: 0.0001  max mem: 3498
[01:42:44.043574] Epoch: [0]  [1750/7125]  eta: 0:14:28  lr: 0.000001  loss: 0.0283 (0.0287)  time: 0.1569  data: 0.0002  max mem: 3498
[01:42:52.171769] Epoch: [0]  [1800/7125]  eta: 0:14:20  lr: 0.000001  loss: 0.0278 (0.0287)  time: 0.1666  data: 0.0001  max mem: 3498
[01:43:00.358444] Epoch: [0]  [1850/7125]  eta: 0:14:12  lr: 0.000001  loss: 0.0280 (0.0287)  time: 0.1566  data: 0.0002  max mem: 3498
[01:43:08.121369] Epoch: [0]  [1900/7125]  eta: 0:14:03  lr: 0.000001  loss: 0.0292 (0.0287)  time: 0.1598  data: 0.0002  max mem: 3498
[01:43:15.664540] Epoch: [0]  [1950/7125]  eta: 0:13:54  lr: 0.000001  loss: 0.0320 (0.0287)  time: 0.1499  data: 0.0002  max mem: 3498
[01:43:23.452029] Epoch: [0]  [2000/7125]  eta: 0:13:45  lr: 0.000001  loss: 0.0255 (0.0287)  time: 0.1690  data: 0.0001  max mem: 3498
[01:43:31.347791] Epoch: [0]  [2050/7125]  eta: 0:13:36  lr: 0.000001  loss: 0.0243 (0.0287)  time: 0.1528  data: 0.0002  max mem: 3498
[01:43:39.379858] Epoch: [0]  [2100/7125]  eta: 0:13:28  lr: 0.000001  loss: 0.0256 (0.0286)  time: 0.1562  data: 0.0001  max mem: 3498
[01:43:47.308553] Epoch: [0]  [2150/7125]  eta: 0:13:20  lr: 0.000001  loss: 0.0275 (0.0286)  time: 0.1662  data: 0.0001  max mem: 3498
[01:43:55.244666] Epoch: [0]  [2200/7125]  eta: 0:13:12  lr: 0.000001  loss: 0.0257 (0.0286)  time: 0.1568  data: 0.0001  max mem: 3498
[01:44:03.171648] Epoch: [0]  [2250/7125]  eta: 0:13:03  lr: 0.000001  loss: 0.0276 (0.0286)  time: 0.1520  data: 0.0001  max mem: 3498
[01:44:11.357086] Epoch: [0]  [2300/7125]  eta: 0:12:56  lr: 0.000001  loss: 0.0276 (0.0286)  time: 0.1601  data: 0.0002  max mem: 3498
[01:44:19.101736] Epoch: [0]  [2350/7125]  eta: 0:12:47  lr: 0.000001  loss: 0.0284 (0.0286)  time: 0.1539  data: 0.0002  max mem: 3498
[01:44:26.977885] Epoch: [0]  [2400/7125]  eta: 0:12:38  lr: 0.000001  loss: 0.0273 (0.0286)  time: 0.1524  data: 0.0001  max mem: 3498
[01:44:34.839635] Epoch: [0]  [2450/7125]  eta: 0:12:30  lr: 0.000001  loss: 0.0265 (0.0286)  time: 0.1626  data: 0.0001  max mem: 3498
[01:44:42.997636] Epoch: [0]  [2500/7125]  eta: 0:12:22  lr: 0.000001  loss: 0.0333 (0.0286)  time: 0.1688  data: 0.0002  max mem: 3498
[01:44:51.066662] Epoch: [0]  [2550/7125]  eta: 0:12:14  lr: 0.000001  loss: 0.0267 (0.0286)  time: 0.1668  data: 0.0001  max mem: 3498
[01:44:58.971433] Epoch: [0]  [2600/7125]  eta: 0:12:06  lr: 0.000001  loss: 0.0261 (0.0286)  time: 0.1549  data: 0.0002  max mem: 3498
[01:45:06.986811] Epoch: [0]  [2650/7125]  eta: 0:11:58  lr: 0.000001  loss: 0.0305 (0.0286)  time: 0.1633  data: 0.0001  max mem: 3498
slurmstepd: error: *** JOB 29389300 ON gpu212-14 CANCELLED AT 2023-11-14T01:45:10 ***
