{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Chained evaluation of the models.\n",
    "\"\"\"\n",
    "import argparse\n",
    "import datetime\n",
    "import h5py\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, CLIPTextModel, BertTokenizer, BertModel\n",
    "\n",
    "import util.misc as misc\n",
    "from engine_node2node import get_text_embeddings\n",
    "from util.datasets import build_shape_surface_occupancy_dataset\n",
    "\n",
    "\n",
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser(\"Performing Chained Eval\", add_help=False)\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        default=32,\n",
    "        type=int,\n",
    "        help=\"Batch size per GPU (effective batch size is batch_size * accum_iter * # gpus\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--text_model_name\",\n",
    "        type=str,\n",
    "        help=\"Text model name to use\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ae\",\n",
    "        type=str,\n",
    "        metavar=\"MODEL\",\n",
    "        help=\"Name of autoencoder\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ae-latent-dim\",\n",
    "        type=int,\n",
    "        default=512*8,\n",
    "        help=\"AE latent dimension\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ae_pth\",\n",
    "        required=True,\n",
    "        help=\"Autoencoder checkpoint\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--point_cloud_size\",\n",
    "        default=2048,\n",
    "        type=int,\n",
    "        help=\"input size\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--fetch_keys\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_clip\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_embeds\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--intensity_loss\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Contrastive edit intensity loss using ground-truth labels.\",\n",
    "    )\n",
    "\n",
    "    # Dataset parameters\n",
    "    parser.add_argument(\n",
    "        \"--dataset\",\n",
    "        type=str,\n",
    "        choices=[\"graphedits\", \"graphedits_chained\"],\n",
    "        help=\"dataset name\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data_path\",\n",
    "        type=str,\n",
    "        help=\"dataset path\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data_type\",\n",
    "        type=str,\n",
    "        help=\"dataset type\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_edge_level\",\n",
    "        default=None,\n",
    "        type=int,\n",
    "        help=\"maximum edge level to use\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--chain_length\",\n",
    "        default=None,\n",
    "        type=int,\n",
    "        help=\"length of chains to load\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--device\", default=\"cuda\", help=\"device to use for training / testing\"\n",
    "    )\n",
    "    parser.add_argument(\"--seed\", default=0, type=int)\n",
    "    parser.add_argument(\"--num_workers\", default=60, type=int)\n",
    "    parser.add_argument(\n",
    "        \"--pin_mem\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--alt_ae_embeds\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Alternative autoencoder embeddings to use\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ft_bert\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Also fine-tune the BERT model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model\",\n",
    "        type=str,\n",
    "        metavar=\"MODEL\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--resume\",\n",
    "        default=\"\",\n",
    "        help=\"Resume from checkpoint\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--resume_full_weights\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Resume the full model weights with the EDM wrapper\",\n",
    "    )\n",
    "\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.mlp_mapper as mlp_mapper\n",
    "\n",
    "# Set dummy arg string to debug the parser\n",
    "call_string = \"\"\"--ae_pth /ibex/user/slimhy/Shape2VecSet/output/pc_ae/best_model.pt \\\n",
    "    --ae-latent-dim 256 \\\n",
    "    --text_model_name bert-base-uncased \\\n",
    "    --dataset graphedits_chained \\\n",
    "    --data_path /ibex/user/slimhy/ShapeWalk/ \\\n",
    "    --data_type release_chained \\\n",
    "    --batch_size 1 \\\n",
    "    --chain_length 10 \\\n",
    "    --num_workers 8 \\\n",
    "    --model mlp_mapper_bert_bneck_512_pcae \\\n",
    "    --resume /ibex/user/slimhy/Shape2VecSet/output/graph_edit/dm/mlp_mapper_bert_bneck_512_pcae__fine_chained/checkpoint-59.pth \\\n",
    "    --resume_full_weights \\\n",
    "    --device cuda \\\n",
    "    --fetch_keys \\\n",
    "    --use_embeds \\\n",
    "    --alt_ae_embeds pc_ae \\\n",
    "    --seed 0\"\"\"\n",
    "\n",
    "# Parse the arguments\n",
    "args = get_args_parser()\n",
    "args = args.parse_args(call_string.split())\n",
    "args.use_clip = \"clip\" in args.text_model_name\n",
    "device = torch.device(args.device)\n",
    "\n",
    "model = mlp_mapper.__dict__[args.model](use_linear_proj=not args.use_clip)\n",
    "model.to(device)\n",
    "\n",
    "# Load the checkpoint\n",
    "if args.resume:\n",
    "    print(\"Loading checkpoint [%s]...\" % args.resume)\n",
    "    checkpoint = torch.load(args.resume, map_location=\"cpu\")\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_train = build_shape_surface_occupancy_dataset(\"train\", args=args)\n",
    "dataset_val = build_shape_surface_occupancy_dataset(\"val\", args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Fix the seed for reproducibility\n",
    "seed = args.seed + misc.get_rank()\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "args.fetch_keys = True\n",
    "# --------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import pprint\n",
    "import warnings\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from models.mlp import MLP\n",
    "from models.point_net import PointNet\n",
    "from models.pointcloud_autoencoder import PointcloudAutoencoder\n",
    "\n",
    "\n",
    "def describe_pc_ae(args):\n",
    "    # Make an AE.\n",
    "    if args.encoder_net == \"pointnet\":\n",
    "        ae_encoder = PointNet(init_feat_dim=3, conv_dims=args.encoder_conv_layers)\n",
    "        encoder_latent_dim = args.encoder_conv_layers[-1]\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    if args.decoder_net == \"mlp\":\n",
    "        ae_decoder = MLP(\n",
    "            in_feat_dims=encoder_latent_dim,\n",
    "            out_channels=args.decoder_fc_neurons + [args.n_pc_points * 3],\n",
    "            b_norm=False,\n",
    "        )\n",
    "\n",
    "    model = PointcloudAutoencoder(ae_encoder, ae_decoder)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_state_dicts(checkpoint_file, map_location=None, **kwargs):\n",
    "    \"\"\" Load torch items from saved state_dictionaries\"\"\"\n",
    "    if map_location is None:\n",
    "        checkpoint = torch.load(checkpoint_file)\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint_file, map_location=map_location)\n",
    "\n",
    "    for key, value in kwargs.items():\n",
    "        value.load_state_dict(checkpoint[key])\n",
    "\n",
    "    epoch = checkpoint.get('epoch')\n",
    "    if epoch:\n",
    "        return epoch\n",
    "\n",
    "\n",
    "def read_saved_args(config_file, override_or_add_args=None, verbose=False):\n",
    "    \"\"\"\n",
    "    :param config_file: json file containing arguments\n",
    "    :param override_args: dict e.g., {'gpu': '0'} will set the resulting arg.gpu to be 0\n",
    "    :param verbose:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    parser = ArgumentParser()\n",
    "    args = parser.parse_args([])\n",
    "    with open(config_file, \"r\") as f_in:\n",
    "        args.__dict__ = json.load(f_in)\n",
    "\n",
    "    if override_or_add_args is not None:\n",
    "        for key, val in override_or_add_args.items():\n",
    "            args.__setattr__(key, val)\n",
    "\n",
    "    if verbose:\n",
    "        args_string = pprint.pformat(vars(args))\n",
    "        print(args_string)\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "def load_pretrained_pc_ae(model_file):\n",
    "    config_file = osp.join(osp.dirname(model_file), \"config.json.txt\")\n",
    "    pc_ae_args = read_saved_args(config_file)\n",
    "    pc_ae = describe_pc_ae(pc_ae_args)\n",
    "\n",
    "    if osp.join(pc_ae_args.log_dir, \"best_model.pt\") != osp.abspath(model_file):\n",
    "        warnings.warn(\n",
    "            \"The saved best_model.pt in the corresponding log_dir is not equal to the one requested.\"\n",
    "        )\n",
    "\n",
    "    best_epoch = load_state_dicts(model_file, model=pc_ae)\n",
    "    print(f\"Pretrained PC-AE is loaded at epoch {best_epoch}.\")\n",
    "    return pc_ae, pc_ae_args\n",
    "\n",
    "\n",
    "# Instantiate autoencoder\n",
    "print(\"Loading autoencoder [%s]...\" % args.ae_pth)\n",
    "pc_ae, pc_ae_args = load_pretrained_pc_ae(args.ae_pth)\n",
    "pc_ae = pc_ae.to(device)\n",
    "pc_ae = pc_ae.eval()\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_pc import plot_pointclouds\n",
    "\n",
    "def apply_edit(net, x_a, embed_ab):\n",
    "    # Concatenate the latent vector with the embedding\n",
    "    edit_vec = net(x_a, embed_ab)\n",
    "\n",
    "    # Add the edit vector to the latent vector\n",
    "    return x_a + edit_vec\n",
    "\n",
    "def apply_iterated_edits(model, embeds_a, embeds_b, embeds_text):\n",
    "    # Move all the garbage to CUDA\n",
    "    embeds_a = embeds_a.cuda()\n",
    "    embeds_b = embeds_b.cuda()\n",
    "    embeds_text = embeds_text.cuda()\n",
    "\n",
    "    x_b_edited = apply_edit(model, embeds_a, embeds_text)\n",
    "    x_b = embeds_b\n",
    "    x_a = embeds_a\n",
    "\n",
    "    # Decode the batch\n",
    "    b_size = x_b.shape[0]\n",
    "    n_points = args.point_cloud_size\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        orig = pc_ae.decoder(x_a).reshape([b_size, 4096, 3])\n",
    "        rec = pc_ae.decoder(x_b_edited).reshape([b_size, 4096, 3])\n",
    "        rec_gt = pc_ae.decoder(x_b).reshape([b_size, 4096, 3])\n",
    "\n",
    "    return (orig, rec, rec_gt), (x_a, x_b_edited, x_b)\n",
    "\n",
    "def apply_iterated_edits__SAFE(model, embeds_a, embeds_text):\n",
    "    # Move all the garbage to CUDA\n",
    "    embeds_a = embeds_a.cuda()\n",
    "    embeds_text = embeds_text.cuda()\n",
    "\n",
    "    x_b_edited = apply_edit(model, embeds_a, embeds_text)\n",
    "    x_a = embeds_a\n",
    "\n",
    "    # Decode the batch\n",
    "    b_size = x_a.shape[0]\n",
    "    n_points = args.point_cloud_size\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        orig = pc_ae.decoder(x_a).reshape([b_size, 4096, 3])\n",
    "        rec = pc_ae.decoder(x_b_edited).reshape([b_size, 4096, 3])\n",
    "\n",
    "    return (orig, rec), (x_a, x_b_edited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_f = \"/ibex/user/slimhy/ShapeWalk/release_chained/release_chained_val.json\"\n",
    "json_d = json.load(open(json_f))\n",
    "\n",
    "def get_prompt(edit_key):\n",
    "    return json_d[edit_key][\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading HDF5 data\n",
    "def decode_json_dset(dset):\n",
    "    dset = dset[:][0].decode(\"utf-8\")\n",
    "    return json.loads(dset)\n",
    "\n",
    "hdf5_file = os.path.join(\n",
    "    args.data_path, args.data_type, \"embeddings_val__pc_ae.hdf5\"\n",
    ")\n",
    "hdf5_f = h5py.File(hdf5_file, \"r\")\n",
    "\n",
    "# Load everything in RAM\n",
    "shape_embeds = (\n",
    "    torch.tensor(hdf5_f[\"shape_embeds\"][:]).to(\"cpu\").type(torch.float32)\n",
    ")\n",
    "D_shape_embeds = shape_embeds.cuda()\n",
    "key_to_shape_embeds = decode_json_dset(hdf5_f[\"key_to_shape_embeds\"])\n",
    "shape_embeds_to_key = {v: k for k, v in key_to_shape_embeds.items()}\n",
    "\n",
    "\n",
    "def query_embeds(query_embed, ignore_embed):\n",
    "    \"\"\"\n",
    "    Returns the index of the closest embedding in the dataset,\n",
    "    and the embedding itself.\n",
    "    Ignore the given embedding.\n",
    "    \"\"\"\n",
    "    # Get the index of the embedding to ignore\n",
    "    ignore_idx = torch.where(D_shape_embeds == ignore_embed)[0]\n",
    "\n",
    "    # Compute the L2 distance between the query and all embeddings,\n",
    "    # except the one to ignore\n",
    "    dists = torch.norm(D_shape_embeds - query_embed, dim=1)\n",
    "    dists[ignore_idx] = float('inf')\n",
    "\n",
    "    # Get the index of the closest embedding\n",
    "    idx = torch.argmin(dists)\n",
    "\n",
    "    # Return the index and the embedding\n",
    "    return idx, shape_embeds_to_key[int(idx)], shape_embeds[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    total_iter = 0\n",
    "    all_orig = []\n",
    "    skip_n = 0\n",
    "    to_skip = 10\n",
    "    k_filter = 0\n",
    "    for _, edit_keys, node_a, _, text_embeds in dataset_val:\n",
    "        skip_n += 1\n",
    "        if skip_n <= to_skip: continue\n",
    "        \n",
    "        if total_iter == 0:\n",
    "            prev_node = node_a\n",
    "\n",
    "        prev_node = prev_node.repeat(8,1)\n",
    "        text_embeds = text_embeds.repeat(8,1)\n",
    "\n",
    "        (orig, _), (x_a, x_b_edited) = apply_iterated_edits__SAFE(model, prev_node, text_embeds)\n",
    "        target_idx, target_key, x_b_edited = query_embeds(x_b_edited[0], x_a[0])\n",
    "\n",
    "        all_orig += [[target_key, edit_keys]]\n",
    "\n",
    "        prev_node = x_b_edited\n",
    "        total_iter += 1\n",
    "        if total_iter == 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shape2vecset",
   "language": "python",
   "name": "shape2vecset"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
