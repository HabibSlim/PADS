{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/slimhy/Documents/PADS/code\n",
      "Set seed to 0\n",
      "Loading autoencoder ckpt/ae_m512.pth\n"
     ]
    }
   ],
   "source": [
    "%cd /home/slimhy/Documents/PADS/code\n",
    "\"\"\"\n",
    "Extracting features into HDF5 files for each split.\n",
    "\"\"\"\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "import util.misc as misc\n",
    "import models.s2vs as ae_mods\n",
    "\n",
    "\n",
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser(\"Extracting Features\", add_help=False)\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        default=32,\n",
    "        type=int,\n",
    "        help=\"Batch size per GPU\"\n",
    "        \" (effective batch size is batch_size * accum_iter * # gpus\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--text_model_name\",\n",
    "        type=str,\n",
    "        help=\"Text model name to use\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ae\",\n",
    "        type=str,\n",
    "        metavar=\"MODEL\",\n",
    "        help=\"Name of autoencoder\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ae-latent-dim\",\n",
    "        type=int,\n",
    "        default=512*8,\n",
    "        help=\"AE latent dimension\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ae_pth\",\n",
    "        required=True,\n",
    "        help=\"Autoencoder checkpoint\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--point_cloud_size\",\n",
    "        default=2048,\n",
    "        type=int,\n",
    "        help=\"input size\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--fetch_keys\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_embeds\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--intensity_loss\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Contrastive edit intensity loss using ground-truth labels.\",\n",
    "    )\n",
    "\n",
    "    # Dataset parameters\n",
    "    parser.add_argument(\n",
    "        \"--dataset\",\n",
    "        type=str,\n",
    "        choices=[\"graphedits\"],\n",
    "        help=\"dataset name\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data_path\",\n",
    "        type=str,\n",
    "        help=\"dataset path\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data_type\",\n",
    "        type=str,\n",
    "        help=\"dataset type\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_edge_level\",\n",
    "        default=None,\n",
    "        type=int,\n",
    "        help=\"maximum edge level to use\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--device\", default=\"cuda\", help=\"device to use for training / testing\"\n",
    "    )\n",
    "    parser.add_argument(\"--seed\", default=0, type=int)\n",
    "    parser.add_argument(\"--num_workers\", default=60, type=int)\n",
    "    parser.add_argument(\n",
    "        \"--pin_mem\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Pin CPU memory in DataLoader for more efficient \"\n",
    "        \"(sometimes) transfer to GPU.\",\n",
    "    )\n",
    "\n",
    "    return parser\n",
    "\n",
    "\n",
    "# Set dummy arg string to debug the parser\n",
    "call_string = \"\"\"--ae_pth ckpt/ae_m512.pth \\\n",
    "    --ae kl_d512_m512_l8 \\\n",
    "    --ae-latent-dim 4096 \\\n",
    "    --batch_size 32 \\\n",
    "    --num_workers 8 \\\n",
    "    --device cuda\"\"\"\n",
    "    \n",
    "\n",
    "# Parse the arguments\n",
    "args = get_args_parser()\n",
    "args = args.parse_args(call_string.split())\n",
    "\n",
    "# --------------------\n",
    "device = torch.device(args.device)\n",
    "\n",
    "# Fix the seed for reproducibility\n",
    "misc.set_all_seeds(args.seed)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "# --------------------\n",
    "\n",
    "# Instantiate autoencoder\n",
    "ae = ae_mods.__dict__[args.ae]()\n",
    "ae.eval()\n",
    "print(\"Loading autoencoder %s\" % args.ae_pth)\n",
    "ae.load_state_dict(torch.load(args.ae_pth, map_location=\"cpu\")[\"model\"])\n",
    "ae = ae.to(device)\n",
    "\n",
    "# Compile using torch.compile\n",
    "ae = torch.compile(ae, mode=\"max-autotune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.latents import ShapeLatentDataset, ComposedPairedShapesLoader\n",
    "\n",
    "class PairType():\n",
    "    ROT_PAIR = \"rand_no_rot,rand_no_rot\"\n",
    "    PART_DROP = \"part_drop,orig\"\n",
    "\n",
    "latents_dir = \"/home/slimhy/Documents/datasets/PADS/3DCoMPaT\"\n",
    "\n",
    "# Create your dataset\n",
    "dataset = ShapeLatentDataset(latents_dir, split=\"train\", shuffle_parts=False)\n",
    "\n",
    "# Create the DataLoader using the sampler\n",
    "dataloader = ComposedPairedShapesLoader(\n",
    "    dataset,\n",
    "    batch_size=32,\n",
    "    pair_types_list=[PairType.ROT_PAIR, PairType.PART_DROP],\n",
    "    num_workers=0,\n",
    "    shuffle=False,\n",
    ") \n",
    "\n",
    "# Use the dataloader in your training loop\n",
    "k_break = 1\n",
    "k = 0\n",
    "for pair_types, (latent_A, bb_coords_A, bb_labels_A, meta_A), (\n",
    "    latent_B,\n",
    "    bb_coords_B,\n",
    "    bb_labels_B,\n",
    "    meta_B, \n",
    ") in dataloader:\n",
    "    k += 1\n",
    "    if k == k_break:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from datasets.metadata import COMPAT_FINE_PARTS\n",
    "from models.modules import (\n",
    "    Attention,\n",
    "    DiagonalGaussianDistribution,\n",
    "    FeedForward,\n",
    "    GEGLU,\n",
    "    PointEmbed,\n",
    "    PreNorm,\n",
    ")\n",
    "from util.misc import cache_fn\n",
    "\n",
    "\n",
    "# ================== MODULE\n",
    "class PartAwareAE(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim=512,\n",
    "            latent_dim=128,\n",
    "            max_parts=24,\n",
    "            heads=8,\n",
    "            dim_head=64,\n",
    "            depth=2,\n",
    "            weight_tie_layers=False,\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.max_parts = max_parts\n",
    "        self.heads = heads\n",
    "        self.dim_head = dim_head\n",
    "        self.depth = depth\n",
    "        self.weight_tie_layers = weight_tie_layers\n",
    "\n",
    "        cache_args = {\"_cache\": self.weight_tie_layers}\n",
    "    \n",
    "        # Point Embedding\n",
    "        self.point_embed = PointEmbed(dim=dim // 2)\n",
    "        # Label Embedding\n",
    "        self.part_label_embed = nn.Embedding(COMPAT_FINE_PARTS, dim // 2)\n",
    "\n",
    "        # Input/Output Cross-Attention Blocks\n",
    "        self.in_block = PreNorm(\n",
    "            dim, Attention(dim, dim, heads=1, dim_head=dim), context_dim=dim\n",
    "        )\n",
    "        self.out_proj = nn.Linear(24, 8)\n",
    "        \n",
    "        # Stacked Attention Layers\n",
    "        def get_latent_attn():\n",
    "            return PreNorm(\n",
    "                dim, Attention(dim, heads=heads, dim_head=dim_head, drop_path_rate=0.1)\n",
    "            )\n",
    "        def get_latent_ff():\n",
    "            return PreNorm(dim, FeedForward(dim, drop_path_rate=0.1))\n",
    "        get_latent_attn, get_latent_ff = map(cache_fn, (get_latent_attn, get_latent_ff))\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([])\n",
    "        for i in range(depth):\n",
    "            self.encoder_layers.append(\n",
    "                nn.ModuleList(\n",
    "                    [get_latent_attn(**cache_args), get_latent_ff(**cache_args)]\n",
    "                )\n",
    "            )\n",
    "        self.decoder_layers = nn.ModuleList([])\n",
    "        for i in range(depth):\n",
    "            self.decoder_layers.append(\n",
    "                nn.ModuleList(\n",
    "                    [get_latent_attn(**cache_args), get_latent_ff(**cache_args)]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Compress/Expand latents\n",
    "        self.compress_latents = nn.Sequential(\n",
    "            nn.Linear(dim, dim // 2),\n",
    "            GEGLU(),\n",
    "            nn.Linear(latent_dim, latent_dim),\n",
    "        )\n",
    "        self.expand_latents = nn.Sequential(\n",
    "            nn.Linear(latent_dim, dim),\n",
    "            GEGLU(),\n",
    "            nn.Linear(dim // 2, dim),\n",
    "        )\n",
    "\n",
    "    def encode(self, latents, part_bbs, part_labels):\n",
    "        # Compute the mask from batch labels (part labels equal to -1 are masked)\n",
    "        batch_mask = part_labels != -1\n",
    "        batch_mask = batch_mask.to(latents.device)\n",
    "\n",
    "        # Embed bounding boxes\n",
    "        bb_centroids = torch.mean(part_bbs, dim=-2) # B x 24 x 3\n",
    "        bb_embeds = self.point_embed(bb_centroids)  # B x 24 x 256\n",
    "\n",
    "        # Embed part labels (take mask into account)\n",
    "        part_labels = part_labels * batch_mask\n",
    "        part_labels_embed = self.part_label_embed(part_labels) # B x 24 x 256\n",
    "\n",
    "        # Repeat latents to match the number of parts\n",
    "        latents_in = latents.transpose(1,2).repeat(1, 3, 1)    # B x 512 x 8 -> B x 24 x 512\n",
    "        part_embeds = torch.cat((bb_embeds, part_labels_embed), dim=-1)\n",
    "\n",
    "        x = self.in_block(part_embeds, context=latents_in, mask=batch_mask)\n",
    "\n",
    "        # Stacked encoder layers\n",
    "        for attn, ff in self.encoder_layers:\n",
    "            x = attn(x) + x \n",
    "            x = ff(x) + x\n",
    "            \n",
    "        # Compress to desired reduced dimension\n",
    "        part_latents = self.compress_latents(x)\n",
    "\n",
    "        return part_latents\n",
    "    \n",
    "    def decode(self, part_latents):\n",
    "        # Expand latents to full dimension\n",
    "        x = self.expand_latents(part_latents)\n",
    "\n",
    "        # Stacked decoder layers\n",
    "        for attn, ff in self.decoder_layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "\n",
    "        # Apply output block\n",
    "        latents = self.out_proj(x.transpose(1,2))\n",
    "\n",
    "        return latents\n",
    "\n",
    "    def forward(self, latents, part_bbs, part_labels):\n",
    "        encoded = self.encode(latents, part_bbs, part_labels)\n",
    "        latents = self.decode(encoded)\n",
    "        return latents\n",
    "    \n",
    "\n",
    "class PartAwareVAE(PartAwareAE):\n",
    "    def __init__(\n",
    "            self,\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.mean_fc = nn.Linear(self.latent_dim, self.latent_dim)\n",
    "        self.logvar_fc = nn.Linear(self.latent_dim, self.latent_dim)\n",
    "\n",
    "    def encode(self, latents, part_bbs, part_labels):\n",
    "        x = super().encode(latents, part_bbs, part_labels)\n",
    "        \n",
    "        mean = self.mean_fc(x)\n",
    "        logvar = self.logvar_fc(x)\n",
    "\n",
    "        posterior = DiagonalGaussianDistribution(mean, logvar)\n",
    "        x = posterior.sample()\n",
    "        kl = posterior.kl()\n",
    "\n",
    "        return kl, x\n",
    "    \n",
    "    def decode(self, part_latents):\n",
    "        return super().decode(part_latents)\n",
    "\n",
    "    def forward(self, latents, part_bbs, part_labels):\n",
    "        kl, part_latents = self.encode(latents, part_bbs, part_labels)\n",
    "        logits = self.decode(part_latents).squeeze(-1)\n",
    "\n",
    "        return logits, kl\n",
    "\n",
    "# =================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = latent_A.to(device)        # B x 512 x 8\n",
    "part_bbs = bb_coords_A.to(device)    # B x 24 x 8 x 3\n",
    "part_labels = bb_labels_A.to(device) # B x 24\n",
    "\n",
    "dummy = PartAwareVAE().to(device)\n",
    "dummy = dummy.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the latents\n",
    "kl, sampled_part_latents = dummy.encode(latents, part_bbs, part_labels)\n",
    "latents = dummy.decode(sampled_part_latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 24, 128])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_part_latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_decoder = nn.Linear(512, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'18,319,432'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misc.count_params(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'106,128,913'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misc.count_params(ae)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
