{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ibex/user/slimhy/PADS/code\n",
      "env: CUBLAS_WORKSPACE_CONFIG=:4096:8\n",
      "Set seed to 0\n"
     ]
    }
   ],
   "source": [
    "%cd /ibex/user/slimhy/PADS/code\n",
    "%reload_ext autoreload\n",
    "%set_env CUBLAS_WORKSPACE_CONFIG=:4096:8\n",
    "\"\"\"\n",
    "Extracting features into HDF5 files for each split.\n",
    "\"\"\"\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "import util.misc as misc\n",
    "import models.s2vs as ae_mods\n",
    "\n",
    "\n",
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser(\"Extracting Features\", add_help=False)\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        default=32,\n",
    "        type=int,\n",
    "        help=\"Batch size per GPU\"\n",
    "        \" (effective batch size is batch_size * accum_iter * # gpus\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--text_model_name\",\n",
    "        type=str,\n",
    "        help=\"Text model name to use\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ae\",\n",
    "        type=str,\n",
    "        metavar=\"MODEL\",\n",
    "        help=\"Name of autoencoder\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ae-latent-dim\",\n",
    "        type=int,\n",
    "        default=512*8,\n",
    "        help=\"AE latent dimension\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ae_pth\",\n",
    "        required=True,\n",
    "        help=\"Autoencoder checkpoint\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--point_cloud_size\",\n",
    "        default=2048,\n",
    "        type=int,\n",
    "        help=\"input size\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--fetch_keys\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_embeds\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--intensity_loss\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Contrastive edit intensity loss using ground-truth labels.\",\n",
    "    )\n",
    "\n",
    "    # Dataset parameters\n",
    "    parser.add_argument(\n",
    "        \"--dataset\",\n",
    "        type=str,\n",
    "        choices=[\"graphedits\"],\n",
    "        help=\"dataset name\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data_path\",\n",
    "        type=str,\n",
    "        help=\"dataset path\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data_type\",\n",
    "        type=str,\n",
    "        help=\"dataset type\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_edge_level\",\n",
    "        default=None,\n",
    "        type=int,\n",
    "        help=\"maximum edge level to use\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--device\", default=\"cuda\", help=\"device to use for training / testing\"\n",
    "    )\n",
    "    parser.add_argument(\"--seed\", default=0, type=int)\n",
    "    parser.add_argument(\"--num_workers\", default=60, type=int)\n",
    "    parser.add_argument(\n",
    "        \"--pin_mem\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Pin CPU memory in DataLoader for more efficient \"\n",
    "        \"(sometimes) transfer to GPU.\",\n",
    "    )\n",
    "\n",
    "    return parser\n",
    "\n",
    "\n",
    "# Set dummy arg string to debug the parser\n",
    "call_string = \"\"\"--ae_pth ckpt/ae_m512.pth \\\n",
    "    --ae kl_d512_m512_l8 \\\n",
    "    --ae-latent-dim 4096 \\\n",
    "    --data_path /ibex/project/c2273/PADS/3DCoMPaT \\\n",
    "    --batch_size 32 \\\n",
    "    --num_workers 8 \\\n",
    "    --device cuda\"\"\"\n",
    "    \n",
    "\n",
    "# Parse the arguments\n",
    "args = get_args_parser()\n",
    "args = args.parse_args(call_string.split())\n",
    "\n",
    "# --------------------\n",
    "device = torch.device(args.device)\n",
    "misc.set_all_seeds(args.seed)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "# --------------------\n",
    "\n",
    "# Initialize and load autoencoder\n",
    "ae = ae_mods.__dict__[args.ae]()\n",
    "ae.load_state_dict(torch.load(args.ae_pth, map_location=\"cpu\")[\"model\"])\n",
    "ae = torch.compile(ae.eval().to(device), mode=\"max-autotune\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.latents import ShapeLatentDataset, ComposedPairedShapesLoader\n",
    "\n",
    "class PairType():\n",
    "    NO_ROT_PAIR = \"rand_no_rot,rand_no_rot\"\n",
    "    PART_DROP = \"part_drop,orig\"\n",
    "\n",
    "# Create your datasets\n",
    "dataset_train = ShapeLatentDataset(args.data_path, split=\"train\", shuffle_parts=True, filter_n_ids=2)\n",
    "dataset_val = ShapeLatentDataset(args.data_path, split=\"test\", shuffle_parts=False, filter_n_ids=2)\n",
    "\n",
    "# Create the DataLoader using the sampler\n",
    "data_loader_train = ComposedPairedShapesLoader(\n",
    "    dataset_train,\n",
    "    batch_size=4,\n",
    "    pair_types_list=[PairType.NO_ROT_PAIR],\n",
    "    num_workers=0,\n",
    "    shuffle=True,\n",
    "    use_distributed=False\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from models.modules import (\n",
    "    Attention,\n",
    "    PreNorm,\n",
    ")\n",
    "from models.partqueries import PartEmbed\n",
    "\n",
    "\n",
    "class PartQueriesGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    Generating a set of part-aware latents\n",
    "    from a set of part bounding boxes and part labels: \"part queries\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim=512,\n",
    "        latent_dim=128,\n",
    "        max_parts=24,\n",
    "        heads=8,\n",
    "        in_heads=1,\n",
    "        dim_head=64,\n",
    "        depth=2,\n",
    "        weight_tie_layers=False,\n",
    "        use_attention_masking=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.max_parts = max_parts\n",
    "        self.use_attention_masking = use_attention_masking\n",
    "\n",
    "        # Part Embeddings\n",
    "        self.part_embed = PartEmbed(dim)\n",
    "        self.embed_proj = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim),\n",
    "        )\n",
    "\n",
    "        # Input Cross-Attention Block\n",
    "        self.in_encode = PreNorm(\n",
    "            dim, Attention(dim, dim, heads=in_heads, dim_head=dim), context_dim=dim\n",
    "        )\n",
    "\n",
    "        # Compress latents to latent dimension\n",
    "        self.compress_latents = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, latent_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, latents, part_bbs, part_labels, batch_mask):\n",
    "        \"\"\"\n",
    "        :param latents:     B x 512 x 8\n",
    "        :param part_bbs:    B x 24 x 4 x 3\n",
    "        :param part_labels: B x 24\n",
    "        :param batch_mask:  B x 24\n",
    "        \"\"\"\n",
    "        # Embed part labels and bounding boxes\n",
    "        part_embeds, labels_embed, bb_embeds = self.part_embed(\n",
    "            part_bbs, part_labels, batch_mask\n",
    "        )\n",
    "        \n",
    "        latents_kv = latents.transpose(1, 2).repeat(1, 3, 1) # B x 24 x 512\n",
    "        \n",
    "        # Concatenate part embeddings with latents\n",
    "        part_embeds = self.embed_proj(part_embeds) # B x 24 x 1024\n",
    "\n",
    "        # Encode part embeddings\n",
    "        mask = batch_mask if self.use_attention_masking else None\n",
    "        x = self.in_encode(part_embeds, context=latents_kv, mask=mask) # B x 24 x 1024\n",
    "        part_latents = self.compress_latents(x)  # B x 24 x 128\n",
    "        return part_latents, part_embeds\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "pvae = PartQueriesGenerator(\n",
    "    dim=512,\n",
    "    latent_dim=512,\n",
    "    heads=8,\n",
    "    dim_head=64,\n",
    "    depth=4,\n",
    ").to(device)\n",
    "pvae = pvae.to(device)\n",
    "pvae.train(True)\n",
    "\n",
    "\n",
    "data_seen = False\n",
    "for data_tuple in data_loader_train:\n",
    "    data_seen = True\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute a single forward pass of the model.\n",
    "    \"\"\"\n",
    "    # Unpack the data tuple\n",
    "    pair_types, (l_a, bb_a, bb_l_a, meta_a), (l_b, bb_b, bb_l_b, meta_b) = data_tuple\n",
    "    device = pvae.device\n",
    "\n",
    "    # Compute the mask from batch labels\n",
    "    mask_a = (bb_l_a != -1).to(device)  # B x 24\n",
    "    mask_b = (bb_l_b != -1).to(device)  # B x 24\n",
    "\n",
    "    l_a, l_b = l_a.to(device), l_b.to(device)  # B x 8 x 512\n",
    "    bb_a, bb_b = bb_a.to(device), bb_b.to(device)  # B x 24 x 4 x 3\n",
    "    bb_l_a, bb_l_b = bb_l_a.to(device), bb_l_b.to(device)  # B x 24\n",
    "\n",
    "    part_latents, part_embeds = pvae(\n",
    "        latents=l_a, part_bbs=bb_a, part_labels=bb_l_a, batch_mask=mask_a\n",
    "    )\n",
    "assert data_seen, \"No data seen in the training loop.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part latents invariant to latents permutation: PASSED\n",
      "\n",
      "Part embeddings equivariant to parts permutation: PASSED\n",
      "\n",
      "Part latents equivariant to parts permutation: PASSED\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "def assert_close(tensor1, tensor2, rtol=1e-5, atol=1e-5):\n",
    "    assert torch.allclose(tensor1, tensor2, rtol=rtol, atol=atol), \\\n",
    "        f\"Tensors are not close: \\n{tensor1}\\n{tensor2}\"\n",
    "\n",
    "\n",
    "def test_latent_permutation_invariance(model):\n",
    "    batch_size, num_parts = 1,  24\n",
    "    \n",
    "    latents = torch.rand(batch_size, 512, 8)\n",
    "    part_bbs = torch.rand(batch_size, num_parts, 4, 3)\n",
    "    part_labels = torch.randint(0, 10, (batch_size, num_parts), dtype=torch.long)\n",
    "    batch_mask = torch.ones(batch_size, num_parts).bool()\n",
    "    \n",
    "    original_output, _ = model(latents, part_bbs, part_labels, batch_mask)\n",
    "    \n",
    "    perm = torch.randperm(8)\n",
    "    \n",
    "    part_latents, _ = model(latents[:, :, perm], part_bbs, part_labels, batch_mask)\n",
    "    \n",
    "    try:\n",
    "        assert_close(original_output, part_latents)\n",
    "        print(\"Part latents invariant to latents permutation: PASSED\")\n",
    "    except AssertionError as e:\n",
    "        print(\"Part latents invariant to latents permutation: FAILED\")\n",
    "        print(str(e))\n",
    "        \n",
    "        \n",
    "def test_part_embeddings_equivariance(model):\n",
    "    batch_size, num_parts = 1, 24\n",
    "    \n",
    "    latents = torch.rand(batch_size, 512, 8)\n",
    "    part_bbs = torch.rand(batch_size, num_parts, 4, 3)\n",
    "    part_labels = torch.randint(0, 10, (batch_size, num_parts), dtype=torch.long)\n",
    "    batch_mask = torch.ones(batch_size, num_parts).bool()\n",
    "    \n",
    "    part_latents, part_embeds = model(latents, part_bbs, part_labels, batch_mask)\n",
    "    \n",
    "    perm = torch.randperm(num_parts)\n",
    "    permuted_part_bbs = part_bbs[:, perm, :, :]\n",
    "    permuted_part_labels = part_labels[:, perm]\n",
    "    \n",
    "    permuted_part_latents, permuted_part_embeds = model(\n",
    "        latents,\n",
    "        permuted_part_bbs,\n",
    "        permuted_part_labels,\n",
    "        batch_mask\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        assert_close(part_embeds[:, perm, :], permuted_part_embeds)\n",
    "        print(\"Part embeddings equivariant to parts permutation: PASSED\")\n",
    "    except AssertionError as e:\n",
    "        print(\"Part embeddings equivariant to parts permutation: FAILED\")\n",
    "        print(str(e))\n",
    "\n",
    "\n",
    "def test_part_latents_equivariance(model):\n",
    "    batch_size, num_parts = 1, 24\n",
    "    \n",
    "    latents = torch.rand(batch_size, 512, 8)\n",
    "    part_bbs = torch.rand(batch_size, num_parts, 4, 3)\n",
    "    part_labels = torch.randint(0, 10, (batch_size, num_parts), dtype=torch.long)\n",
    "    batch_mask = torch.ones(batch_size, num_parts).bool()\n",
    "    \n",
    "    part_latents, part_embeds = model(latents, part_bbs, part_labels, batch_mask)\n",
    "    \n",
    "    perm = torch.randperm(num_parts)\n",
    "    permuted_part_bbs = part_bbs[:, perm, :, :]\n",
    "    permuted_part_labels = part_labels[:, perm]\n",
    "    \n",
    "    permuted_part_latents, permuted_part_embeds = model(\n",
    "        latents,\n",
    "        permuted_part_bbs,\n",
    "        permuted_part_labels,\n",
    "        batch_mask\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        assert_close(part_latents[:, perm, :], permuted_part_latents)\n",
    "        print(\"Part latents equivariant to parts permutation: PASSED\")\n",
    "    except AssertionError as e:\n",
    "        print(\"Part latents equivariant to parts permutation: FAILED\")\n",
    "        print(str(e))\n",
    "\n",
    "\n",
    "# Initialize your model\n",
    "model = PartQueriesGenerator(\n",
    "    dim=512,\n",
    "    latent_dim=128,\n",
    "    max_parts=24,\n",
    "    heads=8,\n",
    "    in_heads=1,\n",
    "    dim_head=64,\n",
    "    depth=2\n",
    ")\n",
    "\n",
    "# Run the tests\n",
    "test_latent_permutation_invariance(model)\n",
    "print()\n",
    "test_part_embeddings_equivariance(model)\n",
    "print()\n",
    "test_part_latents_equivariance(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
