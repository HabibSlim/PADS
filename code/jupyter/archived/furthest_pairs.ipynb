{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find furthest pairs of shapes in the dataset.\n",
    "\"\"\"\n",
    "import argparse\n",
    "import datetime\n",
    "import h5py\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, CLIPTextModel, BertTokenizer, BertModel\n",
    "\n",
    "import util.misc as misc\n",
    "from engine_node2node import get_text_embeddings\n",
    "from util.datasets import build_shape_surface_occupancy_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser(\"Latent Diffusion\", add_help=False)\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        default=64,\n",
    "        type=int,\n",
    "        help=\"Batch size per GPU (effective batch size is batch_size * accum_iter * # gpus\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--exp_name\",\n",
    "        type=str,\n",
    "        help=\"Experiment name to use\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--debug_mode\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Run in debug mode\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--debug_with_forward\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Run in debug mode, also run forward passes\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--plateau_scheduler\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Reduce LR on plateau\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--text_model_name\",\n",
    "        type=str,\n",
    "        help=\"Text model name to use\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--wandb_id\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"WandbID of the run to resume from\",\n",
    "    )\n",
    "    parser.add_argument(\"--epochs\", default=800, type=int)\n",
    "    parser.add_argument(\n",
    "        \"--accum_iter\",\n",
    "        default=1,\n",
    "        type=int,\n",
    "        help=\"Accumulate gradient iterations (for increasing the effective batch size under memory constraints)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--valid_step\",\n",
    "        default=5,\n",
    "        type=int,\n",
    "        help=\"Log validation metrics every N epochs\",\n",
    "    )\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument(\n",
    "        \"--model\",\n",
    "        default=\"kl_d512_m512_l8_edm\",\n",
    "        type=str,\n",
    "        metavar=\"MODEL\",\n",
    "        help=\"Name of model to train\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ae\",\n",
    "        default=\"kl_d512_m512_l8\",\n",
    "        type=str,\n",
    "        metavar=\"MODEL\",\n",
    "        help=\"Name of autoencoder\",\n",
    "    )\n",
    "    parser.add_argument(\"--ae_pth\", help=\"Autoencoder checkpoint\")\n",
    "    parser.add_argument(\n",
    "        \"--ft_bert\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Also fine-tune the BERT model\",\n",
    "    )\n",
    "\n",
    "    # Optimizer parameters\n",
    "    parser.add_argument(\n",
    "        \"--clip_grad\",\n",
    "        type=float,\n",
    "        default=None,\n",
    "        metavar=\"NORM\",\n",
    "        help=\"Clip gradient norm (default: None, no clipping)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--weight_decay\", type=float, default=0.05, help=\"weight decay (default: 0.05)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr\",\n",
    "        type=float,\n",
    "        default=None,\n",
    "        metavar=\"LR\",\n",
    "        help=\"learning rate (absolute lr)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--blr\",\n",
    "        type=float,\n",
    "        default=1e-4,\n",
    "        metavar=\"LR\",\n",
    "        help=\"base learning rate: absolute_lr = base_lr * total_batch_size / 256\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--layer_decay\",\n",
    "        type=float,\n",
    "        default=0.75,\n",
    "        help=\"layer-wise lr decay from ELECTRA/BEiT\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--min_lr\",\n",
    "        type=float,\n",
    "        default=1e-6,\n",
    "        metavar=\"LR\",\n",
    "        help=\"lower lr bound for cyclic schedulers that hit 0\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--intensity_loss\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Contrastive edit intensity loss using ground-truth labels.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_adam\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Use Adam instead of AdamW.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--start_epoch\", default=0, type=int, metavar=\"N\", help=\"Start epoch\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--warmup_epochs\", type=int, default=40, metavar=\"N\", help=\"epochs to warmup LR\"\n",
    "    )\n",
    "    parser.add_argument(\"--seed\", default=0, type=int)\n",
    "\n",
    "    # Dataset parameters\n",
    "    parser.add_argument(\n",
    "        \"--dataset\",\n",
    "        type=str,\n",
    "        choices=[\"graphedits\"],\n",
    "        help=\"Dataset name\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data_path\",\n",
    "        type=str,\n",
    "        help=\"Dataset path\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data_type\",\n",
    "        type=str,\n",
    "        help=\"dataset type\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_edge_level\",\n",
    "        default=1,\n",
    "        type=int,\n",
    "        help=\"Maximum edge level to use\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_embeds\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Use precomputed embeddings\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--fetch_keys\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Fetch node keys in the dataloader\",\n",
    "    )\n",
    "\n",
    "    # Checkpointing parameters\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        default=\"./output/\",\n",
    "        help=\"Path for saving weights/logs\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--log_dir\", default=\"./output/\", help=\"Path where to tensorboard log\"\n",
    "    )\n",
    "    parser.add_argument(\"--resume\", default=\"\", help=\"Resume from checkpoint\")\n",
    "    parser.add_argument(\n",
    "        \"--resume_weights\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Only resume weights, not optimizer state\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--resume_full_weights\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Resume the full model weights with the EDM wrapper\",\n",
    "    )\n",
    "    parser.add_argument(\"--eval\", action=\"store_true\", help=\"Perform evaluation only\")\n",
    "    parser.add_argument(\n",
    "        \"--dist_eval\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Enabling distributed evaluation (recommended during training for faster monitor\",\n",
    "    )\n",
    "\n",
    "    # Hardware parameters\n",
    "    parser.add_argument(\n",
    "        \"--device\", default=\"cuda\", help=\"Device to use for training / testing\"\n",
    "    )\n",
    "    parser.add_argument(\"--num_workers\", default=60, type=int)\n",
    "    parser.add_argument(\n",
    "        \"--pin_mem\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU\",\n",
    "    )\n",
    "    parser.add_argument(\"--no_pin_mem\", action=\"store_false\", dest=\"pin_mem\")\n",
    "    parser.set_defaults(pin_mem=True)\n",
    "\n",
    "    # Distributed training parameters\n",
    "    parser.add_argument(\n",
    "        \"--world_size\", default=1, type=int, help=\"Number of distributed processes\"\n",
    "    )\n",
    "    parser.add_argument(\"--local_rank\", default=-1, type=int)\n",
    "    parser.add_argument(\"--dist_on_itp\", action=\"store_true\")\n",
    "    parser.add_argument(\n",
    "        \"--dist_url\", default=\"env://\", help=\"url used to set up distributed training\"\n",
    "    )\n",
    "\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dummy arg string to debug the parser\n",
    "call_string = \"\"\"--dataset graphedits \\\n",
    "    --data_path /ibex/user/slimhy/ShapeWalk/ \\\n",
    "    --data_type release \\\n",
    "    --batch_size 32 \\\n",
    "    --num_workers 8 \\\n",
    "    --device cuda \\\n",
    "    --fetch_keys \\\n",
    "    --text_model_name bert-base-uncased \\\n",
    "    --use_embeds \\\n",
    "    --seed 0\"\"\"\n",
    "\n",
    "# Parse the arguments\n",
    "args = get_args_parser()\n",
    "args = args.parse_args(call_string.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slimhy/conda/envs/shape2vecset/lib/python3.10/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "# --------------------\n",
    "args.use_clip = \"clip\" in args.text_model_name\n",
    "device = torch.device(args.device)\n",
    "\n",
    "# Fix the seed for reproducibility\n",
    "seed = args.seed + misc.get_rank()\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "args.fetch_keys = True\n",
    "dataset_train = build_shape_surface_occupancy_dataset(\"train\", args=args)\n",
    "dataset_val = build_shape_surface_occupancy_dataset(\"val\", args=args)\n",
    "\n",
    "# Create data loaders\n",
    "data_loader_train = torch.utils.data.DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "# --------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39529/39529 [01:28<00:00, 444.65it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "closest_pairs = {}\n",
    "\n",
    "# Iterate over train set\n",
    "for edit_key, nodes_a, nodes_b, embeds_ab in tqdm(data_loader_train):\n",
    "    # Find the closest pair in embeddings from a to b\n",
    "    nodes_a = nodes_a.to(device)\n",
    "    nodes_b = nodes_b.to(device)\n",
    "    embeds_ab = embeds_ab.to(device)\n",
    "\n",
    "    # Compute pairwise distances between nodes a and b\n",
    "    dist_mat = torch.norm(nodes_a - nodes_b, dim=1)\n",
    "    \n",
    "    # Iterate over the batch\n",
    "    # map edit_key to distance in closest_pairs dict\n",
    "    for i in range(len(edit_key)):\n",
    "        closest_pairs[edit_key[i]] = dist_mat[i].item()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump to JSON\n",
    "with open(\"closest_pairs_CD.json\", \"w\") as f:\n",
    "    json.dump(closest_pairs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shape2vecset",
   "language": "python",
   "name": "shape2vecset"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
