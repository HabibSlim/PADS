{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8baaacac-613d-4166-8469-4e64a7330d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test demo for MLP mapper.\n",
    "\"\"\"\n",
    "import argparse\n",
    "import datetime\n",
    "import h5py\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, CLIPTextModel, BertTokenizer, BertModel\n",
    "\n",
    "import util.misc as misc\n",
    "from engine_node2node import get_text_embeddings\n",
    "from util.datasets import build_shape_surface_occupancy_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d12d84d6-1863-44ad-924d-5e3dd3e4ec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser(\"Latent Diffusion\", add_help=False)\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument(\n",
    "        \"--model\",\n",
    "        default=\"kl_d512_m512_l8_edm\",\n",
    "        type=str,\n",
    "        metavar=\"MODEL\",\n",
    "        help=\"Name of model to train\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        default=32,\n",
    "        type=int,\n",
    "        help=\"Batch size per GPU (effective batch size is batch_size * accum_iter * # gpus\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--text_model_name\",\n",
    "        type=str,\n",
    "        help=\"Text model name to use\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ae\",\n",
    "        type=str,\n",
    "        metavar=\"MODEL\",\n",
    "        help=\"Name of autoencoder\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ae-latent-dim\",\n",
    "        type=int,\n",
    "        default=512*8,\n",
    "        help=\"AE latent dimension\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ae_pth\",\n",
    "        required=True,\n",
    "        help=\"Autoencoder checkpoint\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--point_cloud_size\",\n",
    "        default=2048,\n",
    "        type=int,\n",
    "        help=\"input size\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--fetch_keys\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_embeds\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--intensity_loss\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Contrastive edit intensity loss using ground-truth labels.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--resume\",\n",
    "        default=\"\",\n",
    "        help=\"Resume from checkpoint\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--resume_weights\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Only resume weights, not optimizer state\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--resume_full_weights\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Resume the full model weights with the EDM wrapper\",\n",
    "    )\n",
    "\n",
    "    # Dataset parameters\n",
    "    parser.add_argument(\n",
    "        \"--dataset\",\n",
    "        type=str,\n",
    "        choices=[\"graphedits\"],\n",
    "        help=\"dataset name\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ft_bert\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Also fine-tune the BERT model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data_path\",\n",
    "        type=str,\n",
    "        help=\"dataset path\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data_type\",\n",
    "        type=str,\n",
    "        help=\"dataset type\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_edge_level\",\n",
    "        default=None,\n",
    "        type=int,\n",
    "        help=\"maximum edge level to use\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--alt_ae_embeds\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Alternative autoencoder embeddings to use\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--device\", default=\"cuda\", help=\"device to use for training / testing\"\n",
    "    )\n",
    "    parser.add_argument(\"--seed\", default=0, type=int)\n",
    "    parser.add_argument(\"--num_workers\", default=60, type=int)\n",
    "    parser.add_argument(\n",
    "        \"--pin_mem\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.\",\n",
    "    )\n",
    "\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d3da4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint [/ibex/user/slimhy/Shape2VecSet/output/graph_edit/dm/mlp_mapper_bert_bneck_1024_pcae/checkpoint-90.pth]...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import models.mlp_mapper as mlp_mapper\n",
    "\n",
    "# Set dummy arg string to debug the parser\n",
    "call_string = \"\"\"--ae_pth /ibex/user/slimhy/Shape2VecSet/output/pc_ae/best_model.pt \\\n",
    "    --ae-latent-dim 256 \\\n",
    "    --text_model_name bert-base-uncased \\\n",
    "    --dataset graphedits \\\n",
    "    --data_path /ibex/user/slimhy/ShapeWalk/ \\\n",
    "    --data_type release \\\n",
    "    --batch_size 32 \\\n",
    "    --num_workers 8 \\\n",
    "    --model mlp_mapper_bert_bneck_1024_pcae \\\n",
    "    --resume /ibex/user/slimhy/Shape2VecSet/output/graph_edit/dm/mlp_mapper_bert_bneck_1024_pcae/checkpoint-90.pth \\\n",
    "    --resume_full_weights \\\n",
    "    --device cuda \\\n",
    "    --fetch_keys \\\n",
    "    --use_embeds \\\n",
    "    --alt_ae_embeds pc_ae \\\n",
    "    --seed 0\"\"\"\n",
    "\n",
    "# Parse the arguments\n",
    "args = get_args_parser()\n",
    "args = args.parse_args(call_string.split())\n",
    "args.use_clip = \"clip\" in args.text_model_name\n",
    "device = torch.device(args.device)\n",
    "\n",
    "model = mlp_mapper.__dict__[args.model](use_linear_proj=not args.use_clip)\n",
    "model.to(device)\n",
    "\n",
    "# Load the checkpoint\n",
    "if args.resume:\n",
    "    print(\"Loading checkpoint [%s]...\" % args.resume)\n",
    "    checkpoint = torch.load(args.resume, map_location=\"cpu\")\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fbdd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.autoencoders as autoencoders\n",
    "\n",
    "\n",
    "# Instantiate autoencoder\n",
    "ae = autoencoders.__dict__[\"kl_d512_m512_l8\"]()\n",
    "ae.eval()\n",
    "ae_pth = \"/ibex/user/slimhy/Shape2VecSet/output/graph_edit/ae/ae_m512.pth\"\n",
    "print(\"Loading autoencoder %s\" % ae_pth)\n",
    "ae.load_state_dict(torch.load(ae_pth, map_location=\"cpu\")[\"model\"])\n",
    "ae.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116cb4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "\n",
    "# Fix the seed for reproducibility\n",
    "seed = args.seed + misc.get_rank()\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "args.fetch_keys = True\n",
    "dataset_train = build_shape_surface_occupancy_dataset(\"train\", args=args)\n",
    "dataset_val = build_shape_surface_occupancy_dataset(\"val\", args=args)\n",
    "\n",
    "# Create data loaders\n",
    "data_loader_train = torch.utils.data.DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "# --------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc0cdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import pprint\n",
    "import warnings\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from models.mlp import MLP\n",
    "from models.point_net import PointNet\n",
    "from models.pointcloud_autoencoder import PointcloudAutoencoder\n",
    "\n",
    "\n",
    "def describe_pc_ae(args):\n",
    "    # Make an AE.\n",
    "    if args.encoder_net == \"pointnet\":\n",
    "        ae_encoder = PointNet(init_feat_dim=3, conv_dims=args.encoder_conv_layers)\n",
    "        encoder_latent_dim = args.encoder_conv_layers[-1]\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    if args.decoder_net == \"mlp\":\n",
    "        ae_decoder = MLP(\n",
    "            in_feat_dims=encoder_latent_dim,\n",
    "            out_channels=args.decoder_fc_neurons + [args.n_pc_points * 3],\n",
    "            b_norm=False,\n",
    "        )\n",
    "\n",
    "    model = PointcloudAutoencoder(ae_encoder, ae_decoder)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_state_dicts(checkpoint_file, map_location=None, **kwargs):\n",
    "    \"\"\" Load torch items from saved state_dictionaries\"\"\"\n",
    "    if map_location is None:\n",
    "        checkpoint = torch.load(checkpoint_file)\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint_file, map_location=map_location)\n",
    "\n",
    "    for key, value in kwargs.items():\n",
    "        value.load_state_dict(checkpoint[key])\n",
    "\n",
    "    epoch = checkpoint.get('epoch')\n",
    "    if epoch:\n",
    "        return epoch\n",
    "\n",
    "\n",
    "def read_saved_args(config_file, override_or_add_args=None, verbose=False):\n",
    "    \"\"\"\n",
    "    :param config_file: json file containing arguments\n",
    "    :param override_args: dict e.g., {'gpu': '0'} will set the resulting arg.gpu to be 0\n",
    "    :param verbose:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    parser = ArgumentParser()\n",
    "    args = parser.parse_args([])\n",
    "    with open(config_file, \"r\") as f_in:\n",
    "        args.__dict__ = json.load(f_in)\n",
    "\n",
    "    if override_or_add_args is not None:\n",
    "        for key, val in override_or_add_args.items():\n",
    "            args.__setattr__(key, val)\n",
    "\n",
    "    if verbose:\n",
    "        args_string = pprint.pformat(vars(args))\n",
    "        print(args_string)\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "def load_pretrained_pc_ae(model_file):\n",
    "    config_file = osp.join(osp.dirname(model_file), \"config.json.txt\")\n",
    "    pc_ae_args = read_saved_args(config_file)\n",
    "    pc_ae = describe_pc_ae(pc_ae_args)\n",
    "\n",
    "    if osp.join(pc_ae_args.log_dir, \"best_model.pt\") != osp.abspath(model_file):\n",
    "        warnings.warn(\n",
    "            \"The saved best_model.pt in the corresponding log_dir is not equal to the one requested.\"\n",
    "        )\n",
    "\n",
    "    best_epoch = load_state_dicts(model_file, model=pc_ae)\n",
    "    print(f\"Pretrained PC-AE is loaded at epoch {best_epoch}.\")\n",
    "    return pc_ae, pc_ae_args\n",
    "\n",
    "\n",
    "# Instantiate autoencoder\n",
    "print(\"Loading autoencoder [%s]...\" % args.ae_pth)\n",
    "pc_ae, pc_ae_args = load_pretrained_pc_ae(args.ae_pth)\n",
    "pc_ae = pc_ae.to(device)\n",
    "pc_ae = pc_ae.eval()\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d342a429-09e9-4728-9a5a-add1cff72e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_edit(net, x_a, x_b, embed_ab):\n",
    "    # Reshape from (B, D, K) to (B, M)\n",
    "    x_a = x_a.flatten(1)\n",
    "    x_b = x_b.flatten(1)\n",
    "    embed_ab = embed_ab.flatten(1)\n",
    "\n",
    "    # Concatenate the latent vector with the embedding\n",
    "    edit_vec = net(x_a, embed_ab)\n",
    "    print(torch.mean(edit_vec)/torch.mean(x_a))\n",
    "    # Add the edit vector to the latent vector\n",
    "    return x_a + edit_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5051f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mcubes\n",
    "import trimesh\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def encode_latent(pointcloud):   \n",
    "    return ae.encode(pointcloud)\n",
    "    \n",
    "\n",
    "scale_factor = np.array([0.33408034, 0.39906635, 0.35794342])  # NORMALSHIET\n",
    "rotate_matrix = np.array(\n",
    "    [[1, 0, 0], [0, 1, 0], [0, 0, -1]]\n",
    ")\n",
    "\n",
    "# Apply both scaling and rotation\n",
    "transform = lambda x: center_in_unit_sphere(np.matmul(\n",
    "    rotate_matrix, x.T\n",
    ").T) * scale_factor\n",
    "\n",
    "# Inverse transform\n",
    "inv_transform = lambda x: np.matmul(\n",
    "    np.linalg.inv(rotate_matrix), x.T\n",
    ").T / scale_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26559fdb-fea3-4978-8605-d794ab70f729",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in data_loader_val:\n",
    "    break\n",
    "edit_key, embeds_a, embeds_b, embeds_text = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978fa6ba-a9a9-4b09-9f75-929a372f60e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move all the garbage to CUDA\n",
    "embeds_a = embeds_a.cuda()\n",
    "embeds_b = embeds_b.cuda()\n",
    "embeds_text = embeds_text.cuda()\n",
    "x_b_edited = apply_edit(model, embeds_a, embeds_b, embeds_text)\n",
    "x_b = embeds_b\n",
    "x_a = embeds_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8063458f-0f48-4d47-ad7e-a06b42033784",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_json = \"/ibex/user/slimhy/ShapeWalk/release/release_val.json\"\n",
    "dset_json = json.load(open(dset_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9718f78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_pc import plot_pointclouds\n",
    "\n",
    "# Decode the batch\n",
    "b_size = x_b.shape[0]\n",
    "n_points = args.point_cloud_size\n",
    "\n",
    "with torch.inference_mode():\n",
    "    orig = pc_ae.decoder(x_a).reshape([b_size, 4096, 3])\n",
    "    rec = pc_ae.decoder(x_b_edited).reshape([b_size, 4096, 3])\n",
    "    rec_gt = pc_ae.decoder(x_b).reshape([b_size, 4096, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d9fd4e-38dc-4bb2-aa43-ee2911c266fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pointclouds(orig[8:12].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0aea493-3bd2-4c13-aba0-26eeaa8593d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 8 reconstructions\n",
    "plot_pointclouds(rec[8:12].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddc5c5d-6023-4457-afbd-bc8a05c7b478",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pointclouds(rec_gt[8:12].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9242b8-8663-4729-aa15-5b49d21e7c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_pc(in_pc):\n",
    "    in_pc_detached = in_pc.detach().cpu().numpy()\n",
    "    to_show = np.zeros_like(in_pc_detached)\n",
    "    for k,pc in enumerate(in_pc_detached):\n",
    "        to_show[k] = inv_transform(pc)\n",
    "    return to_show\n",
    "\n",
    "rec_t = transform_pc(rec)\n",
    "plot_pointclouds(rec_t[8:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dcf097-bf84-4d3d-a45d-e2ec2bb7a325",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_t = transform_pc(orig)\n",
    "plot_pointclouds(orig_t[8:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "640233ca-d8df-4d3d-a00e-91c5dac7e778",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27647/467312786.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rec_t = torch.tensor(rec_t).to(device)\n",
      "/tmp/ipykernel_27647/467312786.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  orig_t = torch.tensor(orig_t).to(device)\n"
     ]
    }
   ],
   "source": [
    "rec_t = torch.tensor(rec_t).to(device)\n",
    "orig_t = torch.tensor(orig_t).to(device)\n",
    "\n",
    "_, latents_rec  = encode_latent(rec_t[:, :2048, :].contiguous())\n",
    "_, latents_orig = encode_latent(orig_t[:, :2048, :].contiguous())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d9d6f8b-da5e-412f-871d-377e2f341152",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def decode_latent(shape_k, latent, fcode=\"\", density=32):\n",
    "    latent = latent.clone().cuda().reshape(1,512,8).type(torch.float32)\n",
    "    density = density\n",
    "    gap = 2. / density\n",
    "    x = np.linspace(-1, 1, density+1)\n",
    "    y = np.linspace(-1, 1, density+1)\n",
    "    z = np.linspace(-1, 1, density+1)\n",
    "    xv, yv, zv = np.meshgrid(x, y, z)\n",
    "    grid = torch.from_numpy(np.stack([xv, yv, zv]).astype(np.float32)).view(3, -1).transpose(0, 1)[None].to(device, non_blocking=True)\n",
    " \n",
    "    logits = ae.decode(latent, grid)\n",
    "    logits = logits.detach()\n",
    "    \n",
    "    volume = logits.view(density+1, density+1, density+1).permute(1, 0, 2).cpu().numpy()\n",
    "    verts, faces = mcubes.marching_cubes(volume, 0)\n",
    " \n",
    "    verts *= gap\n",
    "    verts -= 1\n",
    " \n",
    "    m = trimesh.Trimesh(verts, faces)\n",
    "    fname = 'decoded_shapes/node_%d_%s.obj' % (shape_k, fcode)\n",
    "    m.export(fname)\n",
    "    print(\"Written to [%s].\" % fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af0c8c3d-da8c-4863-b581-e24eee03ef14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written to [decoded_shapes/node_0_rec.obj].\n",
      "Written to [decoded_shapes/node_1_rec.obj].\n",
      "Written to [decoded_shapes/node_2_rec.obj].\n",
      "Written to [decoded_shapes/node_3_rec.obj].\n"
     ]
    }
   ],
   "source": [
    "# Decode a few of the edited latent vectors\n",
    "for shape_k, latent_k in enumerate(latents_rec[8:12]):\n",
    "    decode_latent(shape_k,\n",
    "                  latent_k,\n",
    "                  fcode=\"rec\",\n",
    "                  density=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35f2e9ea-ecdf-480f-8d76-41c5005082f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written to [decoded_shapes/node_0_orig.obj].\n",
      "Written to [decoded_shapes/node_1_orig.obj].\n",
      "Written to [decoded_shapes/node_2_orig.obj].\n",
      "Written to [decoded_shapes/node_3_orig.obj].\n"
     ]
    }
   ],
   "source": [
    "for shape_k, latent_k in enumerate(latents_orig[8:12]):\n",
    "    decode_latent(shape_k,\n",
    "                  latent_k,\n",
    "                  fcode=\"orig\",\n",
    "                  density=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "987f7be6-a845-42b5-b664-24227894fca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_shape(shape_k):\n",
    "    # First load all the meshes\n",
    "    mesh_a = trimesh.load(\"decoded_shapes/node_%d_orig.obj\" % shape_k)\n",
    "    mesh_b = trimesh.load(\"decoded_shapes/node_%d_rec.obj\" % shape_k)\n",
    "\n",
    "    # Translate a to the left, b to the right\n",
    "    mesh_a.apply_translation([-1, 0, 0])\n",
    "    mesh_b.apply_translation([1, 0, 0])\n",
    "\n",
    "    # Combine them into a single scene\n",
    "    scene = trimesh.Scene([mesh_a, mesh_b])\n",
    "\n",
    "    return scene.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c024a1a0-2a61-45a1-b734-082ab88057b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shape2vecset",
   "language": "python",
   "name": "shape2vecset"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
