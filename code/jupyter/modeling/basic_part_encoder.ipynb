{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /ibex/user/slimhy/PADS/code\n",
    "%reload_ext autoreload\n",
    "%set_env CUBLAS_WORKSPACE_CONFIG=:4096:8\n",
    "\"\"\"\n",
    "Extracting features into HDF5 files for each split.\n",
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "def assert_close(tensor1, tensor2, rtol=1e-6, atol=1e-6):\n",
    "    assert torch.allclose(tensor1, tensor2, rtol=rtol, atol=atol), \\\n",
    "        f\"Tensors are not close: \\n{tensor1}\\n{tensor2}\"\n",
    "\n",
    "\n",
    "def test_latent_permutation_invariance(model):\n",
    "    batch_size, num_parts = 1,  24\n",
    "    \n",
    "    latents = torch.rand(batch_size, 512, 8)\n",
    "    part_bbs = torch.rand(batch_size, num_parts, 4, 3)\n",
    "    part_labels = torch.randint(0, 10, (batch_size, num_parts), dtype=torch.long)\n",
    "    batch_mask = torch.ones(batch_size, num_parts).bool()\n",
    "    \n",
    "    original_output, _ = model(latents, part_bbs, part_labels, batch_mask)\n",
    "    \n",
    "    perm = torch.randperm(8)\n",
    "    \n",
    "    part_latents, _ = model(latents[:, :, perm], part_bbs, part_labels, batch_mask)\n",
    "    \n",
    "    try:\n",
    "        assert_close(original_output, part_latents)\n",
    "        print(\"Part latents invariant to latents permutation: PASSED\")\n",
    "    except AssertionError as e:\n",
    "        print(\"Part latents invariant to latents permutation: FAILED\")\n",
    "        print(str(e))\n",
    "        \n",
    "        \n",
    "def test_part_embeddings_equivariance(model):\n",
    "    batch_size, num_parts = 1, 24\n",
    "    \n",
    "    latents = torch.rand(batch_size, 512, 8)\n",
    "    part_bbs = torch.rand(batch_size, num_parts, 4, 3)\n",
    "    part_labels = torch.randint(0, 10, (batch_size, num_parts), dtype=torch.long)\n",
    "    batch_mask = torch.ones(batch_size, num_parts).bool()\n",
    "    \n",
    "    part_latents, part_embeds = model(latents, part_bbs, part_labels, batch_mask)\n",
    "    \n",
    "    perm = torch.randperm(num_parts)\n",
    "    permuted_part_bbs = part_bbs[:, perm, :, :]\n",
    "    permuted_part_labels = part_labels[:, perm]\n",
    "    \n",
    "    permuted_part_latents, permuted_part_embeds = model(\n",
    "        latents,\n",
    "        permuted_part_bbs,\n",
    "        permuted_part_labels,\n",
    "        batch_mask\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        assert_close(part_embeds[:, perm, :], permuted_part_embeds)\n",
    "        print(\"Part embeddings equivariant to parts permutation: PASSED\")\n",
    "    except AssertionError as e:\n",
    "        print(\"Part embeddings equivariant to parts permutation: FAILED\")\n",
    "        print(str(e))\n",
    "\n",
    "\n",
    "def test_part_latents_equivariance(model):\n",
    "    batch_size, num_parts = 1, 24\n",
    "    \n",
    "    latents = torch.rand(batch_size, 512, 8)\n",
    "    part_bbs = torch.rand(batch_size, num_parts, 4, 3)\n",
    "    part_labels = torch.randint(0, 10, (batch_size, num_parts), dtype=torch.long)\n",
    "    batch_mask = torch.ones(batch_size, num_parts).bool()\n",
    "    \n",
    "    part_latents, part_embeds = model(latents, part_bbs, part_labels, batch_mask)\n",
    "    \n",
    "    perm = torch.randperm(part_latents.shape[1])\n",
    "    permuted_part_bbs = part_bbs[:, perm, :, :]\n",
    "    permuted_part_labels = part_labels[:, perm]\n",
    "    \n",
    "    permuted_part_latents, permuted_part_embeds = model(\n",
    "        latents,\n",
    "        permuted_part_bbs,\n",
    "        permuted_part_labels,\n",
    "        batch_mask\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        assert_close(part_latents[:, perm, :], permuted_part_latents)\n",
    "        print(\"Part latents equivariant to parts permutation: PASSED\")\n",
    "    except AssertionError as e:\n",
    "        print(\"Part latents equivariant to parts permutation: FAILED\")\n",
    "        print(str(e))\n",
    "        \n",
    "\n",
    "def print_return_shapes(model):\n",
    "    batch_size, num_parts = 1, 24\n",
    "    \n",
    "    latents = torch.rand(batch_size, 512, 8)\n",
    "    part_bbs = torch.rand(batch_size, num_parts, 4, 3)\n",
    "    part_labels = torch.randint(0, 10, (batch_size, num_parts), dtype=torch.long)\n",
    "    batch_mask = torch.ones(batch_size, num_parts).bool()\n",
    "    \n",
    "    part_latents, part_embeds = model(latents, part_bbs, part_labels, batch_mask)\n",
    "    print(\"Part latents shape:\", part_latents.shape)\n",
    "    print(\"Part embeddings shape:\", part_embeds.shape)\n",
    "\n",
    "\n",
    "def test_masked_elements_invariance(model):\n",
    "    batch_size, num_parts = 1, 24\n",
    "    \n",
    "    latents = torch.rand(batch_size, 512, 8)\n",
    "    part_bbs = torch.rand(batch_size, num_parts, 4, 3)\n",
    "    part_labels = torch.randint(0, 10, (batch_size, num_parts), dtype=torch.long)\n",
    "    \n",
    "    # Create a random mask\n",
    "    num_masked = torch.randint(1, num_parts, (1,)).item()  # Random number of masked elements\n",
    "    batch_mask = torch.ones(batch_size, num_parts, dtype=torch.bool)\n",
    "    masked_indices = torch.randperm(num_parts)[:num_masked]\n",
    "    batch_mask[:, masked_indices] = False\n",
    "    \n",
    "    # Get original output\n",
    "    original_part_latents, original_part_embeds = model(latents, part_bbs, part_labels, batch_mask)\n",
    "    \n",
    "    # Modify masked elements in part_bbs and part_labels\n",
    "    modified_part_bbs = part_bbs.clone()\n",
    "    modified_part_labels = part_labels.clone()\n",
    "    modified_part_bbs[:, masked_indices, :, :] = torch.rand_like(modified_part_bbs[:, masked_indices, :, :])\n",
    "    modified_part_labels[:, masked_indices] = torch.randint(0, 10, (batch_size, num_masked), dtype=torch.long)\n",
    "    \n",
    "    # Get output with modified masked elements\n",
    "    modified_part_latents, modified_part_embeds = model(latents, modified_part_bbs, modified_part_labels, batch_mask)\n",
    "    \n",
    "    try:\n",
    "        assert_close(original_part_latents, modified_part_latents)\n",
    "        assert_close(original_part_embeds, modified_part_embeds)\n",
    "        print(f\"Masked elements invariance (with {num_masked} masked elements): PASSED\")\n",
    "    except AssertionError as e:\n",
    "        print(f\"Masked elements invariance (with {num_masked} masked elements): FAILED\")\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generating a set of part-aware latents from a set of part bounding boxes and part labels: \"part queries\".\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch import nn\n",
    "from datasets.metadata import N_COMPAT_FINE_PARTS\n",
    "from models.modules import (\n",
    "    Attention,\n",
    "    PointEmbed,\n",
    "    PreNorm,\n",
    ")\n",
    "\n",
    "\n",
    "class PartEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    Part-aware embeddings for part labels and bounding boxes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        max_parts=24,\n",
    "        single_learnable_query=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = dim\n",
    "        self.max_parts = max_parts\n",
    "        self.single_learnable_query = single_learnable_query\n",
    "\n",
    "        # Embedding layers\n",
    "        self.centroid_embed = PointEmbed(dim=dim // 2)\n",
    "        self.vector_embed = PointEmbed(dim=dim // 2)\n",
    "        self.part_label_embed = nn.Embedding(N_COMPAT_FINE_PARTS, dim // 2)\n",
    "\n",
    "        # Projections\n",
    "        self.bb_embeds_proj = nn.Sequential(\n",
    "            nn.Linear(4 * dim // 2, dim // 2), nn.ReLU(), nn.Linear(dim // 2, dim // 2)\n",
    "        )\n",
    "        self.final_embeds_proj = nn.Linear(dim, dim)\n",
    "\n",
    "        # Learnable empty object queries\n",
    "        if single_learnable_query:\n",
    "            self.empty_object_query = nn.Parameter(torch.randn(dim))\n",
    "        else:\n",
    "            self.empty_queries = nn.Parameter(torch.randn(max_parts, dim))\n",
    "\n",
    "    def forward(self, part_bbs, part_labels, batch_mask):\n",
    "        B, N, _, _ = part_bbs.shape\n",
    "        D = self.embed_dim // 2\n",
    "\n",
    "        # Embed centroids and vectors coordinates\n",
    "        # ================================================\n",
    "        bb_centroids = part_bbs[:, :, 0, :]\n",
    "        bb_centroid_embeds = self.centroid_embed(bb_centroids)  # B x 24 x D\n",
    "\n",
    "        bb_vectors = part_bbs[:, :, 1:, :].reshape(-1, N * 3, 3)\n",
    "        bb_vector_embeds = self.vector_embed(bb_vectors)  # B x 72 x D\n",
    "\n",
    "        # Interleave the embeddings\n",
    "        bb_vector_embeds_reshaped = bb_vector_embeds.reshape(\n",
    "            bb_centroid_embeds.shape[0], N, 3, -1\n",
    "        )\n",
    "        bb_embeds = torch.empty(\n",
    "            (bb_centroid_embeds.shape[0], N * 4, bb_centroid_embeds.shape[2]),\n",
    "            device=bb_centroid_embeds.device,\n",
    "        )  # B x 96 x D\n",
    "        bb_embeds[:, 0::4, :] = bb_centroid_embeds\n",
    "        bb_embeds[:, 1::4, :] = bb_vector_embeds_reshaped[:, :, 0, :]\n",
    "        bb_embeds[:, 2::4, :] = bb_vector_embeds_reshaped[:, :, 1, :]\n",
    "        bb_embeds[:, 3::4, :] = bb_vector_embeds_reshaped[:, :, 2, :]\n",
    "\n",
    "        # Project the embeddings (vectors + centroids) to the same dimension\n",
    "        bb_embeds = bb_embeds.view(B, N, 4 * D)  # B x 24 x (D * 4)\n",
    "        bb_embeds = self.bb_embeds_proj(bb_embeds)  # B x 24 x D\n",
    "        # ================================================\n",
    "\n",
    "        # Embed part labels\n",
    "        # ================================================\n",
    "        labels_embed = self.part_label_embed(part_labels * batch_mask)  # B x 24 x D\n",
    "        # ================================================\n",
    "\n",
    "        # Final embeddings\n",
    "        # ================================================\n",
    "        part_embeds = torch.cat([labels_embed, bb_embeds], dim=-1)\n",
    "        part_embeds = self.final_embeds_proj(part_embeds)  # B x 24 x D\n",
    "\n",
    "        # Replace masked (empty) objects with the corresponding learnable empty object query\n",
    "        empty_mask = ~batch_mask.bool()\n",
    "        if self.single_learnable_query:\n",
    "            part_embeds[empty_mask] = self.empty_object_query.expand(\n",
    "                empty_mask.sum(), -1\n",
    "            )\n",
    "        else:\n",
    "            for i in range(self.max_parts):\n",
    "                part_embeds[:, i, :] = torch.where(\n",
    "                    empty_mask[:, i].unsqueeze(1),\n",
    "                    self.empty_queries[i].unsqueeze(0).expand(B, -1),\n",
    "                    part_embeds[:, i, :],\n",
    "                )\n",
    "        # ================================================\n",
    "\n",
    "        return part_embeds, labels_embed, bb_embeds\n",
    "\n",
    "\n",
    "class PQM(nn.Module):\n",
    "    \"\"\"\n",
    "    Generating a set of part-aware latents\n",
    "    from a set of part bounding boxes and part labels: \"part queries\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim=512,\n",
    "        max_parts=24,\n",
    "        heads=8,\n",
    "        in_heads=8,\n",
    "        dim_head=64,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_parts = max_parts\n",
    "\n",
    "        # Part Embeddings\n",
    "        self.part_embed = PartEmbed(dim, max_parts)\n",
    "        self.embed_proj = nn.Linear(dim, dim)\n",
    "\n",
    "        # Input Cross-Attention Block\n",
    "        self.in_encode = PreNorm(\n",
    "            dim, Attention(dim, dim, heads=in_heads, dim_head=dim), context_dim=dim\n",
    "        )\n",
    "\n",
    "    def forward(self, latents, part_bbs, part_labels, batch_mask):\n",
    "        \"\"\"\n",
    "        :param latents:     B x 512 x 8\n",
    "        :param part_bbs:    B x 24 x 4 x 3\n",
    "        :param part_labels: B x 24\n",
    "        :param batch_mask:  B x 24\n",
    "        \"\"\"\n",
    "        # Embed part labels and bounding boxes\n",
    "        part_embeds, labels_embed, bb_embeds = self.part_embed(\n",
    "            part_bbs, part_labels, batch_mask\n",
    "        )\n",
    "\n",
    "        # Apply learnable projection instead of repeat\n",
    "        latents_kv = latents.transpose(1, 2) # B x 8 x 512\n",
    "\n",
    "        # Concatenate part embeddings with latents\n",
    "        part_embeds = self.embed_proj(part_embeds)  # B x 24 x 512\n",
    "\n",
    "        # Encode part embeddings\n",
    "        part_queries = self.in_encode(part_embeds, context=latents_kv)  # B x 24 x 512\n",
    "        return part_queries, part_embeds\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Initialize your model\n",
    "part_latents_dim = 128\n",
    "pqm = PQM(\n",
    "    dim=512,\n",
    "    heads=8,\n",
    "    dim_head=64,\n",
    ")\n",
    "\n",
    "# Run the tests\n",
    "print_return_shapes(pqm)\n",
    "print()\n",
    "test_latent_permutation_invariance(pqm)\n",
    "print()\n",
    "test_part_embeddings_equivariance(pqm)\n",
    "print()\n",
    "test_part_latents_equivariance(pqm)\n",
    "print()\n",
    "test_masked_elements_invariance(pqm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.diffusion as dm\n",
    "model = dm.kl_d512_m512_l8_d24_pq()\n",
    "\n",
    "part_queries, part_embeds = pqm(\n",
    "    torch.rand(1, 512, 8),\n",
    "    torch.rand(1, 24, 4, 3),\n",
    "    torch.randint(0, 10, (1, 24), dtype=torch.long),\n",
    "    torch.ones(1, 24).bool(),\n",
    ")\n",
    "\n",
    "print(part_queries.shape)\n",
    "\n",
    "in_noised_latent = torch.zeros(1, 512, 8)\n",
    "in_timestep = torch.Tensor([5]).long()\n",
    "model.model(x=in_noised_latent, t=in_timestep, cond=part_queries, cond_mask=~torch.ones(1, 24).bool())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.diffusion as dm\n",
    "model = dm.kl_d512_m512_l8_d24_pq()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
