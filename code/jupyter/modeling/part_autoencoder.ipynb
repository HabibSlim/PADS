{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Autoencoder models for point clouds.\n",
    "\"\"\"\n",
    "\n",
    "from torch import nn\n",
    "from models.modules import (\n",
    "    Attention,\n",
    "    DiagonalGaussianDistribution,\n",
    "    FeedForward,\n",
    "    PointEmbed,\n",
    "    PreNorm,\n",
    ")\n",
    "from util.misc import cache_fn, fps_subsample\n",
    "\n",
    "\n",
    "class PartEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_inputs,\n",
    "        num_latents,\n",
    "        latent_dim,\n",
    "        point_embed,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_latents = num_latents\n",
    "        \n",
    "        self.cross_attend_blocks = nn.ModuleList(\n",
    "            [\n",
    "                PreNorm(\n",
    "                    dim, Attention(dim, dim, heads=1, dim_head=dim), context_dim=dim\n",
    "                ),\n",
    "                PreNorm(dim, FeedForward(dim)),\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.point_embed = point_embed\n",
    "        self.mean_fc = nn.Linear(dim, latent_dim)\n",
    "        self.logvar_fc = nn.Linear(dim, latent_dim)\n",
    "\n",
    "    def forward(self, pc):\n",
    "        B, N, D = pc.shape  # B x N x 3\n",
    "        assert N == self.num_inputs\n",
    "\n",
    "        ###### fps\n",
    "        ratio = 1.0 * self.num_latents / self.num_inputs\n",
    "        sampled_pc = fps_subsample(pc, ratio)  # B x 512 x 3 (M = 512)\n",
    "        ######\n",
    "\n",
    "        sampled_pc_embeddings = self.point_embed(sampled_pc)  # B x 512 x 512\n",
    "        pc_embeddings = self.point_embed(pc)  # B x N x 512\n",
    "\n",
    "        cross_attn, cross_ff = self.cross_attend_blocks\n",
    "\n",
    "        x = (\n",
    "            cross_attn(sampled_pc_embeddings, context=pc_embeddings, mask=None)\n",
    "            + sampled_pc_embeddings\n",
    "        )\n",
    "        x = cross_ff(x) + x  # B x 512 x 512\n",
    "\n",
    "        mean = self.mean_fc(x)  # B x 512 x 8\n",
    "        logvar = self.logvar_fc(x)  # B x 512 x 8\n",
    "\n",
    "        posterior = DiagonalGaussianDistribution(mean, logvar)\n",
    "        x = posterior.sample() # B x 512 x 8\n",
    "        kl = posterior.kl() # B x 512\n",
    "\n",
    "        return kl, x\n",
    "\n",
    "\n",
    "class PartDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        depth,\n",
    "        dim,\n",
    "        queries_dim,\n",
    "        output_dim,\n",
    "        latent_dim,\n",
    "        heads,\n",
    "        dim_head,\n",
    "        weight_tie_layers,\n",
    "        decoder_ff,\n",
    "        point_embed,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.depth = depth\n",
    "        \n",
    "        get_latent_attn = lambda: PreNorm(\n",
    "            dim, Attention(dim, heads=heads, dim_head=dim_head, drop_path_rate=0.1)\n",
    "        )\n",
    "        get_latent_ff = lambda: PreNorm(dim, FeedForward(dim, drop_path_rate=0.1))\n",
    "        get_latent_attn, get_latent_ff = map(cache_fn, (get_latent_attn, get_latent_ff))\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        cache_args = {\"_cache\": weight_tie_layers}\n",
    "\n",
    "        for i in range(depth):\n",
    "            self.layers.append(\n",
    "                nn.ModuleList(\n",
    "                    [get_latent_attn(**cache_args), get_latent_ff(**cache_args)]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.decoder_cross_attn = PreNorm(\n",
    "            queries_dim,\n",
    "            Attention(queries_dim, dim, heads=1, dim_head=dim),\n",
    "            context_dim=dim,\n",
    "        )\n",
    "        self.decoder_ff = (\n",
    "            PreNorm(queries_dim, FeedForward(queries_dim)) if decoder_ff else None\n",
    "        )\n",
    "\n",
    "        self.to_outputs = (\n",
    "            nn.Linear(queries_dim, output_dim)\n",
    "            if output_dim is not None\n",
    "            else nn.Identity()\n",
    "        )\n",
    "\n",
    "        self.proj = nn.Linear(latent_dim, dim)\n",
    "        self.point_embed = point_embed\n",
    "\n",
    "    def forward(self, x, queries):\n",
    "        x = self.proj(x)  # B x M x 512\n",
    "\n",
    "        for self_attn, self_ff in self.layers:\n",
    "            x = self_attn(x) + x\n",
    "            x = self_ff(x) + x # B x M x 512\n",
    "\n",
    "        # cross attend from decoder queries to latents\n",
    "        queries_embeddings = self.point_embed(queries) # B x N x 512\n",
    "        latents = self.decoder_cross_attn(queries_embeddings, context=x) # B x N x 512\n",
    "\n",
    "        # optional decoder feedforward\n",
    "        if self.decoder_ff is not None:\n",
    "            latents = latents + self.decoder_ff(latents) # B x N x 512\n",
    "\n",
    "        return self.to_outputs(latents) # B x N x 1\n",
    "\n",
    "\n",
    "class KLPartAutoEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        depth=24,\n",
    "        dim=512,\n",
    "        queries_dim=512,\n",
    "        output_dim=1,\n",
    "        num_inputs=2048,\n",
    "        num_latents=512,\n",
    "        latent_dim=64,\n",
    "        heads=8,\n",
    "        dim_head=64,\n",
    "        weight_tie_layers=False,\n",
    "        decoder_ff=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.depth = depth\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_latents = num_latents\n",
    "        \n",
    "        self.point_embed = PointEmbed(dim=dim)\n",
    "        \n",
    "        self.encoder = PartEncoder(\n",
    "            dim=dim,\n",
    "            num_inputs=num_inputs,\n",
    "            num_latents=num_latents,\n",
    "            latent_dim=latent_dim,\n",
    "            point_embed=self.point_embed,\n",
    "        )\n",
    "        \n",
    "        self.decoder = PartDecoder(\n",
    "            depth=depth,\n",
    "            dim=dim,\n",
    "            queries_dim=queries_dim,\n",
    "            output_dim=output_dim,\n",
    "            latent_dim=latent_dim,\n",
    "            heads=heads,\n",
    "            dim_head=dim_head,\n",
    "            weight_tie_layers=weight_tie_layers,\n",
    "            decoder_ff=decoder_ff,\n",
    "            point_embed=self.point_embed,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def encode(self, pc):\n",
    "        return self.encoder(pc)\n",
    "        \n",
    "    def decode(self, x, queries):\n",
    "        return self.decoder(x, queries)\n",
    "\n",
    "    def forward(self, pc, queries):\n",
    "        kl, x = self.encode(pc)\n",
    "        o = self.decode(x, queries).squeeze(-1)\n",
    "        return {\"logits\": o, \"kl\": kl}\n",
    "\n",
    "\n",
    "def create_part_autoencoder(dim=512, M=512, latent_dim=64, N=2048):\n",
    "    model = KLPartAutoEncoder(\n",
    "        depth=24,\n",
    "        dim=dim,\n",
    "        queries_dim=dim,\n",
    "        output_dim=1,\n",
    "        num_inputs=N,\n",
    "        num_latents=M,\n",
    "        latent_dim=latent_dim,\n",
    "        heads=8,\n",
    "        dim_head=64,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def kl_d512_m512_l8(N=2048):\n",
    "    return create_part_autoencoder(dim=512, M=512, latent_dim=8, N=N)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
