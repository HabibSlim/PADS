{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ibex/user/slimhy/PADS/code\n",
      "env: CUBLAS_WORKSPACE_CONFIG=:4096:8\n"
     ]
    }
   ],
   "source": [
    "%cd /ibex/user/slimhy/PADS/code\n",
    "%reload_ext autoreload\n",
    "%set_env CUBLAS_WORKSPACE_CONFIG=:4096:8\n",
    "\"\"\"\n",
    "Extracting features into HDF5 files for each split.\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "from util.misc import default\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MaskableCrossAttention(nn.Module):\n",
    "    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0.0):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "\n",
    "        if context_dim is None:\n",
    "            context_dim = query_dim\n",
    "\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.heads = heads\n",
    "\n",
    "        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n",
    "        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)\n",
    "        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, query_dim), nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, context=None, context_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (B, N, D) where N is the query sequence length\n",
    "            context: Context tensor of shape (B, L, D) where L is the context sequence length\n",
    "            context_mask: Boolean mask of shape (B, L) where True means the value is masked\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape (B, N, D)\n",
    "\n",
    "        Where:\n",
    "            B: batch size\n",
    "            N: sequence length of query\n",
    "            L: sequence length of context\n",
    "            D: dimension of input features\n",
    "        \"\"\"\n",
    "        h = self.heads\n",
    "        q = self.to_q(x)\n",
    "\n",
    "        context = default(context, x)\n",
    "        k = self.to_k(context)\n",
    "        v = self.to_v(context)\n",
    "\n",
    "        # where d = dim_head\n",
    "        q, k, v = map(lambda t: rearrange(t, \"b n (h d) -> (b h) n d\", h=h), (q, k, v))\n",
    "\n",
    "        # sim shape: (B*h, N, L)\n",
    "        sim = torch.einsum(\"b i d, b j d -> b i j\", q, k) * self.scale\n",
    "\n",
    "        if context_mask is not None:\n",
    "            # Expand mask for the heads dimension\n",
    "            # context_mask shape: (B, L) -> (B, 1, L) -> (B*h, 1, L)\n",
    "            mask = context_mask.unsqueeze(1).expand(-1, 1, -1)\n",
    "            mask = mask.repeat_interleave(h, dim=0)\n",
    "\n",
    "            # Expand mask for the query dimension\n",
    "            # mask shape: (B*h, 1, L) -> (B*h, N, L)\n",
    "            mask = mask.expand(-1, x.size(1), -1)\n",
    "\n",
    "            # Create a mask of -inf where context_mask is True\n",
    "            mask_value = -torch.finfo(sim.dtype).max\n",
    "            sim = sim.masked_fill(mask, mask_value)\n",
    "\n",
    "        # attn shape: (B*h, N, L)\n",
    "        attn = sim.softmax(dim=-1)\n",
    "\n",
    "        # out shape: (B*h, N, d) -> (B, N, h*d)\n",
    "        out = torch.einsum(\"b i j, b j d -> b i d\", attn, v)\n",
    "        out = rearrange(out, \"(b h) n d -> b n (h d)\", h=h)\n",
    "\n",
    "        return self.to_out(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query permutation equivariance test passed!\n",
      "Context permutation invariance test passed!\n",
      "Mask invariance test passed!\n",
      "Modifying non-masked values in context changes the output as expected!\n",
      "Modifying values in query changes the output as expected!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import randperm\n",
    "\n",
    "\n",
    "def assert_close(tensor1, tensor2, rtol=1e-7, atol=1e-7, revert=False):\n",
    "    cond = torch.allclose(tensor1, tensor2, rtol=rtol, atol=atol)\n",
    "    if revert:\n",
    "        assert not cond, f\"Tensors are close: \\n{tensor1}\\n{tensor2}\"\n",
    "    else:\n",
    "        assert cond, f\"Tensors are not close: \\n{tensor1}\\n{tensor2}\"\n",
    "\n",
    "def test_query_permutation_equivariance():\n",
    "    # Initialize model\n",
    "    model = MaskableCrossAttention(query_dim=64, context_dim=64, heads=4)\n",
    "    \n",
    "    # Create random input\n",
    "    batch_size, seq_len, dim = 2, 8, 64\n",
    "    x = torch.randn(batch_size, seq_len, dim)\n",
    "    context = torch.randn(batch_size, seq_len, dim)\n",
    "    \n",
    "    # Get output for original input\n",
    "    out1 = model(x, context)\n",
    "    \n",
    "    # Create random permutation for queries\n",
    "    perm = randperm(seq_len)\n",
    "    \n",
    "    # Permute input queries\n",
    "    x_perm = x[:, perm, :]\n",
    "    \n",
    "    # Get output for permuted input\n",
    "    out2 = model(x_perm, context)\n",
    "    \n",
    "    # The output should follow the same permutation\n",
    "    assert_close(out1[:, perm, :], out2)\n",
    "    print(\"Query permutation equivariance test passed!\")\n",
    "\n",
    "def test_context_permutation_invariance():\n",
    "    # Initialize model\n",
    "    model = MaskableCrossAttention(query_dim=64, context_dim=64, heads=4)\n",
    "    \n",
    "    # Create random input\n",
    "    batch_size, seq_len, dim = 2, 8, 64\n",
    "    x = torch.randn(batch_size, seq_len, dim)\n",
    "    context = torch.randn(batch_size, seq_len, dim)\n",
    "    \n",
    "    # Get output for original input\n",
    "    out1 = model(x, context)\n",
    "    \n",
    "    # Create random permutation for context\n",
    "    perm = randperm(seq_len)\n",
    "    \n",
    "    # Permute context\n",
    "    context_perm = context[:, perm, :]\n",
    "    \n",
    "    # Get output for permuted context\n",
    "    out2 = model(x, context_perm)\n",
    "    \n",
    "    # The output should be the same regardless of context permutation\n",
    "    assert_close(out1, out2)\n",
    "    print(\"Context permutation invariance test passed!\")\n",
    "\n",
    "def test_mask_invariance():\n",
    "    # Initialize model\n",
    "    model = MaskableCrossAttention(query_dim=64, context_dim=64, heads=4)\n",
    "    \n",
    "    # Create random input\n",
    "    batch_size, seq_len, dim = 2, 8, 64\n",
    "    x = torch.randn(batch_size, seq_len, dim)\n",
    "    context = torch.randn(batch_size, seq_len, dim)\n",
    "    \n",
    "    # Create random mask (True means masked)\n",
    "    mask = torch.rand(batch_size, seq_len) > 0.5\n",
    "    \n",
    "    # Get output with original context\n",
    "    out1 = model(x, context, mask)\n",
    "    \n",
    "    # Modify masked values in context\n",
    "    modified_context = context.clone()\n",
    "    modified_context[mask] = 100.0  # Set masked values to large number\n",
    "    \n",
    "    # Get output with modified context\n",
    "    out2 = model(x, modified_context, mask)\n",
    "    \n",
    "    # The output should be the same regardless of masked values\n",
    "    assert_close(out1, out2)\n",
    "    print(\"Mask invariance test passed!\")\n",
    "    \n",
    "    # Now modify non-masked values in context\n",
    "    modified_context = context.clone()\n",
    "    modified_context[~mask] = 100.0  # Set non-masked values to large number\n",
    "    \n",
    "    # Get output with modified context\n",
    "    out3 = model(x, modified_context, mask)\n",
    "    \n",
    "    # The output should be different when non-masked values are modified\n",
    "    assert_close(out1, out3, revert=True)\n",
    "    print(\"Modifying non-masked values in context changes the output as expected!\")\n",
    "    \n",
    "    # Now modify the query tensor and keep the context tensor the same\n",
    "    modified_x = x.clone()\n",
    "    \n",
    "    # Modify masked values in query\n",
    "    modified_x[mask] = 100.0  # Set masked values to large number\n",
    "    \n",
    "    # Get output with modified query\n",
    "    out4 = model(modified_x, context, mask)\n",
    "    \n",
    "    # The output should be different when masked values in query are modified\n",
    "    assert_close(out1, out4, revert=True)\n",
    "    print(\"Modifying values in query changes the output as expected!\")\n",
    "\n",
    "\n",
    "torch.manual_seed(0) \n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "# Run all tests\n",
    "test_query_permutation_equivariance()\n",
    "test_context_permutation_invariance()\n",
    "test_mask_invariance()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
