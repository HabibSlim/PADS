{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ibex/user/slimhy/PADS/code\n",
      "env: CUBLAS_WORKSPACE_CONFIG=:4096:8\n"
     ]
    }
   ],
   "source": [
    "%cd /ibex/user/slimhy/PADS/code\n",
    "%reload_ext autoreload\n",
    "%set_env CUBLAS_WORKSPACE_CONFIG=:4096:8\n",
    "\"\"\"\n",
    "Latents dataset.\n",
    "\"\"\"\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, BatchSampler, DataLoader\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class PairType:\n",
    "    NO_ROT_PAIR = \"rand_no_rot,rand_no_rot\"\n",
    "    PART_DROP = \"part_drop,orig\"\n",
    "\n",
    "\n",
    "TEST_V = None\n",
    "class ShapeLatentDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Shape latent dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    PART_CAP = 24\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir,\n",
    "        exclude_types=None,\n",
    "        cap_parts=True,\n",
    "        shuffle_parts=True,\n",
    "        class_code=None,\n",
    "        split=None,\n",
    "        filter_n_ids=None,\n",
    "        get_part_points=False,\n",
    "    ):\n",
    "        exclude_types = set(exclude_types) if exclude_types else set()\n",
    "        self.shuffle_parts = shuffle_parts\n",
    "        self.get_part_points = get_part_points\n",
    "\n",
    "        # Load file list\n",
    "        file_list = \"capped_list.json\" if cap_parts else \"full_list.json\"\n",
    "        file_list = json.load(open(os.path.join(data_dir, file_list)))\n",
    "        latents_dir = os.path.join(data_dir, \"latents\")\n",
    "        bbs_dir = os.path.join(data_dir, \"bounding_boxes\")\n",
    "        points_dir = os.path.join(data_dir, \"part_points\")\n",
    "\n",
    "        # Load the split\n",
    "        if split is not None:\n",
    "            split = json.load(open(os.path.join(data_dir, \"split_\" + split + \".json\")))\n",
    "            split = set(split)\n",
    "\n",
    "        final_list = []\n",
    "        for f in file_list:\n",
    "            if split is not None and f[:6] not in split:\n",
    "                continue\n",
    "\n",
    "            file_type = \"_\".join(f.split(\"_\")[2:-1])\n",
    "\n",
    "            # Filter by class code\n",
    "            valid_cls = class_code is None or f.startswith(class_code)\n",
    "            if not valid_cls:\n",
    "                continue\n",
    "\n",
    "            # Filter by file type\n",
    "            if file_type not in exclude_types:\n",
    "                bb_coords_f = f + \"_part_bbs\"\n",
    "                bb_labels_f = f + \"_part_labels\"\n",
    "                part_points_f = f[:6]\n",
    "                if self.get_part_points:\n",
    "                    final_list += [[k + \".npy\" for k in [f, bb_coords_f, bb_labels_f, part_points_f]]]\n",
    "                else:\n",
    "                    final_list += [[k + \".npy\" for k in [f, bb_coords_f, bb_labels_f]]]\n",
    "\n",
    "        # Create a list of file paths\n",
    "        file_list = final_list\n",
    "        file_list.sort()\n",
    "        self.file_list = file_list\n",
    "        self.file_tuples = []\n",
    "        for idx in range(len(file_list)):\n",
    "            # Unpack file paths\n",
    "            file_paths = [os.path.join(latents_dir, file_list[idx][0])]\n",
    "            file_paths = file_paths + [\n",
    "                os.path.join(bbs_dir, f) for f in file_list[idx][1:3]\n",
    "            ]\n",
    "            if self.get_part_points:\n",
    "                file_paths.append(os.path.join(points_dir, file_list[idx][3]))\n",
    "            \n",
    "            latent_f, bb_coords_f, bb_labels_f = file_paths[:3]\n",
    "            part_points_f = file_paths[3] if self.get_part_points else None\n",
    "\n",
    "            # Extract model ID from the filename\n",
    "            basename = os.path.basename(latent_f)\n",
    "            model_id = basename.split(\"_\")[0:2][0] + basename.split(\"_\")[0:2][1]\n",
    "            model_id = int(model_id, 16)\n",
    "            self.file_tuples += [(latent_f, bb_coords_f, bb_labels_f, part_points_f, model_id)]\n",
    "\n",
    "        if filter_n_ids is not None:\n",
    "            # Only keep the samples corresponding to N=filter_n_ids distinct model IDs\n",
    "            unique_model_ids = list(set(tup[4] for tup in self.file_tuples))\n",
    "            random.shuffle(unique_model_ids)\n",
    "            selected_model_ids = set(unique_model_ids[:filter_n_ids])\n",
    "\n",
    "            self.file_tuples = [\n",
    "                tuple for tuple in self.file_tuples if tuple[4] in selected_model_ids\n",
    "            ]\n",
    "            filtered_file_paths = [tuple[0] for tuple in self.file_tuples]\n",
    "            self.file_list = [\n",
    "                sublist\n",
    "                for sublist in self.file_list\n",
    "                if os.path.join(latents_dir, sublist[0]) in filtered_file_paths\n",
    "            ]\n",
    "\n",
    "        self.rng = torch.Generator()\n",
    "        self.rng_counter = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_tuples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        global TEST_V\n",
    "\n",
    "        # Unpack file paths\n",
    "        latent_f, bb_coords_f, bb_labels_f, part_points_f, model_id = self.file_tuples[idx]\n",
    "\n",
    "        # Loading latent and bounding box data\n",
    "        latent = np.load(latent_f)\n",
    "        bb_coords = np.load(bb_coords_f)\n",
    "        bb_labels = np.load(bb_labels_f)\n",
    "\n",
    "        # Convert numpy array to torch tensor\n",
    "        latent_tensor = torch.from_numpy(latent).float()\n",
    "        bb_coords_tensor = torch.from_numpy(bb_coords).float()\n",
    "        bb_labels_tensor = torch.from_numpy(bb_labels).long()\n",
    "\n",
    "        if self.get_part_points:\n",
    "            part_points = np.load(part_points_f, allow_pickle=True)\n",
    "            TEST_V = part_points\n",
    "            # print(part_points[0])\n",
    "            # part_points_stacked = np.stack(part_points.values())\n",
    "            # part_points_tensor = torch.from_numpy(part_points_stacked).float()\n",
    "\n",
    "        # Shuffle the order of parts if self.shuffle is True\n",
    "        if self.shuffle_parts:\n",
    "            self.rng.manual_seed(model_id + self.rng_counter)\n",
    "\n",
    "            num_parts = bb_coords_tensor.size(0)\n",
    "            shuffle_indices = torch.randperm(num_parts, generator=self.rng)\n",
    "            bb_coords_tensor = bb_coords_tensor[shuffle_indices]\n",
    "            bb_labels_tensor = bb_labels_tensor[shuffle_indices]\n",
    "            if self.get_part_points:\n",
    "                part_points_tensor = part_points_tensor[shuffle_indices]\n",
    "\n",
    "        # Pad bb coords and labels\n",
    "        pad_size = self.PART_CAP - bb_coords_tensor.size(0)\n",
    "\n",
    "        # Pad the tensors\n",
    "        bb_coords_tensor = F.pad(bb_coords_tensor, (0, 0, 0, 0, 0, pad_size))\n",
    "        bb_labels_tensor = F.pad(bb_labels_tensor, (0, pad_size), value=-1)\n",
    "        if self.get_part_points:\n",
    "            part_points_tensor = F.pad(part_points_tensor, (0, 0, 0, pad_size))\n",
    "\n",
    "        # Extract metadata from filename\n",
    "        meta = os.path.basename(latent_f).split(\".\")[0]\n",
    "\n",
    "        if self.get_part_points:\n",
    "            return (\n",
    "                latent_tensor,\n",
    "                bb_coords_tensor,\n",
    "                bb_labels_tensor,\n",
    "                part_points_tensor,\n",
    "                meta,\n",
    "            )\n",
    "        else:\n",
    "            return (\n",
    "                latent_tensor,\n",
    "                bb_coords_tensor,\n",
    "                bb_labels_tensor,\n",
    "                meta,\n",
    "            )\n",
    "\n",
    "class PairedSampler(BatchSampler):\n",
    "    \"\"\"\n",
    "    Sampling augmented shape pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, batch_size, pair_types, shuffle=True, drop_last=False):\n",
    "        pair_types = [t.strip() for t in pair_types.split(\",\")]\n",
    "        if len(pair_types) != 2:\n",
    "            raise ValueError(\n",
    "                \"pair_types should contain exactly two types separated by a comma\"\n",
    "            )\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        # Group indices by ID and type\n",
    "        id_to_indices = defaultdict(lambda: defaultdict(list))\n",
    "        for idx, file_info in enumerate(dataset.file_list):\n",
    "            if dataset.get_part_points:\n",
    "                filename, _, _, _ = file_info  # Unpack considering part points file\n",
    "            else:\n",
    "                filename, _, _ = file_info  # Unpack without part points file\n",
    "            \n",
    "            parts = filename.split(\"_\")\n",
    "            id_part = \"_\".join(parts[:2])\n",
    "            type_part = \"_\".join(parts[2:-1])\n",
    "            \n",
    "            id_to_indices[id_part][type_part].append(idx)\n",
    "    \n",
    "        # Filter out IDs that don't have both required types\n",
    "        valid_ids = [\n",
    "            id_part\n",
    "            for id_part, type_dict in id_to_indices.items()\n",
    "            if all(p_type in type_dict for p_type in pair_types)\n",
    "        ]\n",
    "\n",
    "        self.paired_indices = self._create_paired_indices(\n",
    "            id_to_indices, valid_ids, pair_types\n",
    "        )\n",
    "\n",
    "    def _create_paired_indices(self, id_to_indices, valid_ids, pair_types):\n",
    "        paired_indices = []\n",
    "\n",
    "        if self.shuffle:\n",
    "            random.shuffle(valid_ids)\n",
    "\n",
    "        for id_part in valid_ids:\n",
    "            type1, type2 = pair_types\n",
    "            indices1 = id_to_indices[id_part][type1]\n",
    "            indices2 = id_to_indices[id_part][type2]\n",
    "\n",
    "            if type1 == type2:\n",
    "                # If the same type is requested, ensure we have at least 2 files\n",
    "                if len(indices1) < 2:\n",
    "                    continue\n",
    "                pair = random.sample(indices1, 2)\n",
    "            else:\n",
    "                # If different types, take one from each\n",
    "                pair = [random.choice(indices1), random.choice(indices2)]\n",
    "\n",
    "            paired_indices.extend(pair)\n",
    "\n",
    "        return paired_indices\n",
    "\n",
    "    def __iter__(self):\n",
    "        batch = []\n",
    "        for idx in self.paired_indices:\n",
    "            batch.append(idx)\n",
    "            if len(batch) == self.batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paired_indices) * 2 // self.batch_size\n",
    "\n",
    "\n",
    "class DistributedPairedSampler(BatchSampler):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size,\n",
    "        pair_types,\n",
    "        num_replicas=None,\n",
    "        rank=None,\n",
    "        seed=0,\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "    ):\n",
    "        pair_types = [t.strip() for t in pair_types.split(\",\")]\n",
    "        if len(pair_types) != 2:\n",
    "            raise ValueError(\n",
    "                \"pair_types should contain exactly two types separated by a comma\"\n",
    "            )\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch = 0\n",
    "        self.num_replicas = num_replicas\n",
    "        self.rank = rank\n",
    "        self.seed = seed\n",
    "        self.shuffle = shuffle\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "        # Create paired indices\n",
    "        self.paired_indices = self._create_paired_indices(dataset.file_list, pair_types)\n",
    "        self.num_samples = len(self.paired_indices) // self.num_replicas\n",
    "\n",
    "        # Create RNG\n",
    "        self.rng = torch.Generator()\n",
    "        self.rng.manual_seed(self.seed + self.epoch)\n",
    "        self.indices = None\n",
    "\n",
    "    def _create_paired_indices(self, file_list, pair_types):\n",
    "        \"\"\"\n",
    "        Initialize a list of paired indices.\n",
    "        \"\"\"\n",
    "\n",
    "        # Group indices by ID and type\n",
    "        id_to_indices = defaultdict(lambda: defaultdict(list))\n",
    "        for idx, (filename, _, _) in enumerate(file_list):\n",
    "            parts = filename.split(\"_\")\n",
    "            id_part = \"_\".join(parts[:2])\n",
    "            file_type = \"_\".join(parts[2:-1])\n",
    "            id_to_indices[id_part][file_type].append(idx)\n",
    "\n",
    "        # Filter out IDs that don't have both required types\n",
    "        valid_ids = [\n",
    "            id_part\n",
    "            for id_part, type_dict in id_to_indices.items()\n",
    "            if all(type in type_dict for type in pair_types)\n",
    "        ]\n",
    "\n",
    "        paired_indices = []\n",
    "\n",
    "        for id_part in valid_ids:\n",
    "            type1, type2 = pair_types\n",
    "            indices1 = id_to_indices[id_part][type1]\n",
    "            indices2 = id_to_indices[id_part][type2]\n",
    "\n",
    "            if type1 == type2:\n",
    "                # If the same type is requested, ensure we have at least 2 files\n",
    "                if len(indices1) < 2:\n",
    "                    continue\n",
    "                pair = random.sample(indices1, 2)\n",
    "            else:\n",
    "                # If different types, take one from each\n",
    "                pair = [random.choice(indices1), random.choice(indices2)]\n",
    "\n",
    "            paired_indices.extend(pair)\n",
    "\n",
    "        return paired_indices\n",
    "\n",
    "    def sample_indices(self):\n",
    "        \"\"\"\n",
    "        Sample indices for the current epoch.\n",
    "        \"\"\"\n",
    "        # Deterministically shuffle based on epoch and seed\n",
    "        if self.shuffle:\n",
    "            n = len(self.paired_indices)\n",
    "\n",
    "            # Generate a permutation for N/2 pairs\n",
    "            pair_perm = torch.randperm(n // 2, generator=self.rng).tolist()\n",
    "\n",
    "            # Use the permutation to reindex the paired list\n",
    "            indices = [j for i in pair_perm for j in (2 * i, 2 * i + 1)]\n",
    "        else:\n",
    "            indices = list(range(len(self.paired_indices)))\n",
    "\n",
    "        # Subsample while preserving pairs\n",
    "        n_pairs = len(indices) // 2\n",
    "        pair_indices = list(range(n_pairs))\n",
    "        subsampled_pair_indices = pair_indices[self.rank : n_pairs : self.num_replicas]\n",
    "\n",
    "        self.indices = [\n",
    "            idx\n",
    "            for pair_idx in subsampled_pair_indices\n",
    "            for idx in indices[2 * pair_idx : 2 * pair_idx + 2]\n",
    "        ]\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.indices is None:\n",
    "            self.sample_indices()\n",
    "\n",
    "        # Create batches\n",
    "        batches = []\n",
    "        batch = []\n",
    "        for idx in self.indices:\n",
    "            batch.append(self.paired_indices[idx])\n",
    "            if len(batch) == self.batch_size:\n",
    "                batches.append(batch)\n",
    "                batch = []\n",
    "\n",
    "        return iter(batches)\n",
    "\n",
    "    def __len__(self):\n",
    "        num_samples = len(self.paired_indices)\n",
    "        return num_samples // self.batch_size\n",
    "\n",
    "    def set_epoch(self, epoch):\n",
    "        self.epoch = epoch\n",
    "        self.rng.manual_seed(self.seed + self.epoch)\n",
    "        self.sample_indices()\n",
    "\n",
    "\n",
    "class PairedShapesLoader:\n",
    "    \"\"\"\n",
    "    Paired shapes loader.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size,\n",
    "        pair_types,\n",
    "        num_workers,\n",
    "        shuffle,\n",
    "        use_distributed=False,\n",
    "        num_replicas=None,\n",
    "        rank=None,\n",
    "        get_part_points=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # Filter out keys from kwargs that are not DataLoader arguments\n",
    "        valid_keys = set(DataLoader.__init__.__code__.co_varnames)\n",
    "        kwargs = {k: v for k, v in kwargs.items() if k in valid_keys}\n",
    "        self.kwargs = kwargs\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.pair_types = pair_types\n",
    "        self.num_workers = num_workers\n",
    "        self.shuffle = shuffle\n",
    "        self.use_distributed = use_distributed\n",
    "        self.num_replicas = num_replicas\n",
    "        self.rank = rank\n",
    "        self.get_part_points = get_part_points\n",
    "        self.create_dataloader()\n",
    "\n",
    "    def create_dataloader(self):\n",
    "        if self.use_distributed:\n",
    "            batch_sampler = DistributedPairedSampler(\n",
    "                self.dataset,\n",
    "                self.batch_size,\n",
    "                self.pair_types,\n",
    "                shuffle=self.shuffle,\n",
    "                num_replicas=self.num_replicas,\n",
    "                rank=self.rank,\n",
    "            )\n",
    "        else:\n",
    "            batch_sampler = PairedSampler(\n",
    "                self.dataset,\n",
    "                pair_types=self.pair_types,\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=self.shuffle,\n",
    "            )\n",
    "        self.sampler = batch_sampler\n",
    "        self.dataloader = DataLoader(\n",
    "            self.dataset,\n",
    "            batch_sampler=batch_sampler,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "        self.iterator = iter(self.dataloader)\n",
    "\n",
    "    def set_epoch(self, epoch):\n",
    "        if self.use_distributed:\n",
    "            self.sampler.set_epoch(epoch)\n",
    "            self.iterator = iter(self.dataloader)\n",
    "        else:\n",
    "            raise ValueError(\"set_epoch is only supported in distributed mode\")\n",
    "\n",
    "    def split_tensor(self, tensor):\n",
    "        tensor_A = tensor[::2]\n",
    "        tensor_B = tensor[1::2]\n",
    "        return tensor_A, tensor_B\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        try:\n",
    "            tuple_data = next(self.iterator)\n",
    "            tuple_A, tuple_B = zip(*(self.split_tensor(t) for t in tuple_data))\n",
    "            return tuple_A, tuple_B\n",
    "\n",
    "        except StopIteration:\n",
    "            self.iterator = iter(self.dataloader)\n",
    "            raise StopIteration\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataloader)\n",
    "\n",
    "\n",
    "class ComposedPairedShapesLoader:\n",
    "    \"\"\"\n",
    "    Composed loader that alternates between batches of multiple shape pair types.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size,\n",
    "        pair_types_list,\n",
    "        num_workers,\n",
    "        shuffle=False,\n",
    "        use_distributed=False,\n",
    "        num_replicas=None,\n",
    "        rank=None,\n",
    "        reset_every=100,\n",
    "        get_part_points=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.pair_types_list = pair_types_list\n",
    "        self.num_workers = num_workers\n",
    "        self.shuffle = shuffle\n",
    "        self.use_distributed = use_distributed\n",
    "        self.num_replicas = num_replicas\n",
    "        self.rank = rank\n",
    "        self.kwargs = kwargs\n",
    "        self.reset_every = reset_every\n",
    "        self.get_part_points = get_part_points\n",
    "        self.loaders = None\n",
    "\n",
    "    def create_loaders(self):\n",
    "        self.loaders = [\n",
    "            (\n",
    "                pair_types,\n",
    "                PairedShapesLoader(\n",
    "                    self.dataset,\n",
    "                    self.batch_size,\n",
    "                    pair_types,\n",
    "                    self.num_workers,\n",
    "                    shuffle=self.shuffle,\n",
    "                    use_distributed=self.use_distributed,\n",
    "                    num_replicas=self.num_replicas,\n",
    "                    rank=self.rank,\n",
    "                    get_part_points=self.get_part_points,\n",
    "                    **self.kwargs,\n",
    "                ),\n",
    "            )\n",
    "            for pair_types in self.pair_types_list\n",
    "        ]\n",
    "        self.num_loaders = len(self.loaders)\n",
    "\n",
    "    def set_epoch(self, epoch, force_reset=False):\n",
    "        if epoch % self.reset_every == 0 or force_reset:\n",
    "            self.create_loaders()\n",
    "        for _, loader in self.loaders:\n",
    "            loader.set_epoch(epoch)\n",
    "\n",
    "    def get_tuple(self, device=None, return_single=True):\n",
    "        \"\"\"\n",
    "        Get a data tuple for debugging.\n",
    "        \"\"\"\n",
    "        for pair_types, tuple_a, tuple_b in self:\n",
    "            if device is not None:\n",
    "                tuple_a = tuple(t.to(device) if isinstance(t, torch.Tensor) else t for t in tuple_a)\n",
    "                tuple_b = tuple(t.to(device) if isinstance(t, torch.Tensor) else t for t in tuple_b)\n",
    "            \n",
    "            if return_single:\n",
    "                return tuple_a\n",
    "            else:\n",
    "                return tuple_a, tuple_b\n",
    "\n",
    "        raise ValueError(\"No data available\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.loaders is None:\n",
    "            self.create_loaders()\n",
    "        while True:\n",
    "            for pair_types, loader in [\n",
    "                (pair_types, loader) for pair_types, loader in self.loaders\n",
    "            ]:\n",
    "                try:\n",
    "                    yield pair_types, *next(loader)\n",
    "                except StopIteration:\n",
    "                    self.dataset.rng_counter += 1\n",
    "                    return\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.loaders is None:\n",
    "            self.create_loaders()\n",
    "        return max(len(loader) for _, loader in self.loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading autoencoder ckpt/ae_m512.pth\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test demo for MLP mapper.\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import models.s2vs as autoencoders\n",
    "import util.misc as misc\n",
    "\n",
    "\n",
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser(\"Latent Diffusion\", add_help=False)\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument(\n",
    "        \"--model\",\n",
    "        default=\"kl_d512_m512_l8_edm\",\n",
    "        type=str,\n",
    "        metavar=\"MODEL\",\n",
    "        help=\"Name of model to train\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        default=32,\n",
    "        type=int,\n",
    "        help=\"Batch size per GPU (effective batch size is batch_size * accum_iter * # gpus\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--text_model_name\",\n",
    "        type=str,\n",
    "        help=\"Text model name to use\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ae\",\n",
    "        type=str,\n",
    "        metavar=\"MODEL\",\n",
    "        help=\"Name of autoencoder\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ae-latent-dim\",\n",
    "        type=int,\n",
    "        default=512*8,\n",
    "        help=\"AE latent dimension\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ae_pth\",\n",
    "        required=True,\n",
    "        help=\"Autoencoder checkpoint\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--point_cloud_size\",\n",
    "        default=2048,\n",
    "        type=int,\n",
    "        help=\"input size\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--fetch_keys\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_embeds\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--resume\",\n",
    "        default=\"\",\n",
    "        help=\"Resume from checkpoint\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--resume_weights\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Only resume weights, not optimizer state\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--resume_full_weights\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Resume the full model weights with the EDM wrapper\",\n",
    "    )\n",
    "\n",
    "    # Dataset parameters\n",
    "    parser.add_argument(\n",
    "        \"--device\", default=\"cuda\", help=\"device to use for training / testing\"\n",
    "    )\n",
    "    parser.add_argument(\"--seed\", default=0, type=int)\n",
    "    parser.add_argument(\"--num_workers\", default=60, type=int)\n",
    "    parser.add_argument(\n",
    "        \"--pin_mem\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.\",\n",
    "    )\n",
    "\n",
    "    return parser\n",
    "\n",
    "\n",
    "# Set dummy arg string to debug the parser\n",
    "call_string = \"\"\"--ae_pth ckpt/ae_m512.pth \\\n",
    "    --ae kl_d512_m512_l8 \\\n",
    "    --ae-latent-dim 4096 \\\n",
    "    --num_workers 8 \\\n",
    "    --batch_size 2 \\\n",
    "    --device cuda \\\n",
    "    --fetch_keys \\\n",
    "    --use_embeds \\\n",
    "    --seed 0\"\"\"\n",
    "\n",
    "# Parse the arguments\n",
    "args = get_args_parser()\n",
    "args = args.parse_args(call_string.split())\n",
    "args.data_path = \"/ibex/project/c2273/PADS/3DCoMPaT/\"\n",
    "\n",
    "# --------------------\n",
    "device = torch.device(args.device)\n",
    "\n",
    "# Fix the seed for reproducibility\n",
    "seed = args.seed + misc.get_rank()\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "args.fetch_keys = True\n",
    "# --------------------\n",
    "\n",
    "# Instantiate autoencoder\n",
    "ae = autoencoders.__dict__[args.ae]()\n",
    "ae.eval()\n",
    "print(\"Loading autoencoder %s\" % args.ae_pth)\n",
    "ae.load_state_dict(torch.load(args.ae_pth, map_location=\"cpu\")[\"model\"])\n",
    "ae = ae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slimhy/conda/envs/3D2VS_flexicubes/lib/python3.10/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "Caught UnboundLocalError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/slimhy/conda/envs/3D2VS_flexicubes/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/slimhy/conda/envs/3D2VS_flexicubes/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/slimhy/conda/envs/3D2VS_flexicubes/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_2142383/4266975062.py\", line 157, in __getitem__\n    part_points_tensor = part_points_tensor[shuffle_indices]\nUnboundLocalError: local variable 'part_points_tensor' referenced before assignment\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 38\u001b[0m\n\u001b[1;32m     19\u001b[0m dataset_train \u001b[38;5;241m=\u001b[39m ShapeLatentDataset(\n\u001b[1;32m     20\u001b[0m     args\u001b[38;5;241m.\u001b[39mdata_path,\n\u001b[1;32m     21\u001b[0m     class_code\u001b[38;5;241m=\u001b[39mclass_to_hex(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchair\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     filter_n_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     28\u001b[0m loader \u001b[38;5;241m=\u001b[39m ComposedPairedShapesLoader(\n\u001b[1;32m     29\u001b[0m     dataset_train,\n\u001b[1;32m     30\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     36\u001b[0m )\n\u001b[0;32m---> 38\u001b[0m (l_a, bb_a, bb_l_a, part_pts_a, meta_a), (l_b, bb_b, bb_l_b, part_pts_b, meta_b) \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_single\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 542\u001b[0m, in \u001b[0;36mComposedPairedShapesLoader.get_tuple\u001b[0;34m(self, device, return_single)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_tuple\u001b[39m(\u001b[38;5;28mself\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, return_single\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    539\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;124;03m    Get a data tuple for debugging.\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 542\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pair_types, tuple_a, tuple_b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    544\u001b[0m             tuple_a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(t\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tuple_a)\n",
      "Cell \u001b[0;32mIn[13], line 562\u001b[0m, in \u001b[0;36mComposedPairedShapesLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pair_types, loader \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m    559\u001b[0m     (pair_types, loader) \u001b[38;5;28;01mfor\u001b[39;00m pair_types, loader \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaders\n\u001b[1;32m    560\u001b[0m ]:\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 562\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m pair_types, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    564\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mrng_counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[13], line 467\u001b[0m, in \u001b[0;36mPairedShapesLoader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 467\u001b[0m         tuple_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m         tuple_A, tuple_B \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_tensor(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tuple_data))\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tuple_A, tuple_B\n",
      "File \u001b[0;32m~/conda/envs/3D2VS_flexicubes/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/conda/envs/3D2VS_flexicubes/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/3D2VS_flexicubes/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/conda/envs/3D2VS_flexicubes/lib/python3.10/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: Caught UnboundLocalError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/slimhy/conda/envs/3D2VS_flexicubes/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/slimhy/conda/envs/3D2VS_flexicubes/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/slimhy/conda/envs/3D2VS_flexicubes/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_2142383/4266975062.py\", line 157, in __getitem__\n    part_points_tensor = part_points_tensor[shuffle_indices]\nUnboundLocalError: local variable 'part_points_tensor' referenced before assignment\n"
     ]
    }
   ],
   "source": [
    "from util.misc import visualize_bounding_boxes\n",
    "from datasets.metadata import class_to_hex\n",
    "import util.s2vs as s2vs\n",
    "\n",
    "\n",
    "def decode_latents(model, latents, grid_density=256, batch_size=128**3):\n",
    "    # Decode the latents\n",
    "    with torch.no_grad():\n",
    "        mesh = s2vs.decode_latents(\n",
    "            ae=model,\n",
    "            latent=latents[0].unsqueeze(0),\n",
    "            grid_density=grid_density,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "    return mesh\n",
    "\n",
    "\n",
    "# Create your datasets\n",
    "dataset_train = ShapeLatentDataset(\n",
    "    args.data_path,\n",
    "    class_code=class_to_hex(\"chair\"),\n",
    "    split=\"train\",\n",
    "    get_part_points=True,\n",
    "    shuffle_parts=True,\n",
    "    filter_n_ids=8\n",
    ")\n",
    "\n",
    "loader = ComposedPairedShapesLoader(\n",
    "    dataset_train,\n",
    "    batch_size=args.batch_size,\n",
    "    pair_types_list=[PairType.NO_ROT_PAIR],\n",
    "    num_workers=4,\n",
    "    seed=0,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "(l_a, bb_a, bb_l_a, part_pts_a, meta_a), (l_b, bb_b, bb_l_b, part_pts_b, meta_b) = loader.get_tuple(device=device, return_single=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_boxes(bb_coords, latents):\n",
    "    bounding_boxes = np.array(bb_coords.cpu()).squeeze()\n",
    "    mesh = decode_latents(ae, latents.float()).trimesh_mesh\n",
    "    \n",
    "    return visualize_bounding_boxes(mesh, bounding_boxes, box_type='lines', colormap='hsv').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_boxes(bb_a, l_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_boxes(bb_b, l_b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
