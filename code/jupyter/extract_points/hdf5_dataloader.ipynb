{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "\n",
    "class PartOccupancyDataset(Dataset):\n",
    "    \"\"\"Dataset for part-based occupancy prediction with part dropping.\"\"\"\n",
    "    \n",
    "    def __init__(self, hdf5_path, split='train', num_queries=None, seed=None):\n",
    "        \"\"\"\n",
    "        Initialize the dataset by loading HDF5 matrices into memory.\n",
    "        \n",
    "        Args:\n",
    "            hdf5_path: Path to the HDF5 file containing the dataset\n",
    "            split: Dataset split ('train', 'val', 'test')\n",
    "            num_queries: Number of query points to sample (if None, uses all points)\n",
    "            seed: Random seed for query point sampling\n",
    "        \"\"\"\n",
    "        self.num_queries = num_queries\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "            \n",
    "        # Load and validate HDF5 data\n",
    "        with h5py.File(hdf5_path, 'r') as f:\n",
    "            # Verify required datasets exist\n",
    "            required_keys = [\n",
    "                'model_ids', 'part_slices', 'part_drops',\n",
    "                'part_points_matrix', 'part_bbs_matrix',\n",
    "                'query_points_matrix', 'query_labels_matrix'\n",
    "            ]\n",
    "            missing_keys = [key for key in required_keys if key not in f]\n",
    "            if missing_keys:\n",
    "                raise ValueError(f\"Missing required datasets: {missing_keys}\")\n",
    "            \n",
    "            # Load data into memory\n",
    "            self.model_ids = f['model_ids'][:].astype('U')\n",
    "            self.part_slices = f['part_slices'][:]\n",
    "            self.part_drops = f['part_drops'][:]\n",
    "            self.part_points = f['part_points_matrix'][:]\n",
    "            self.part_bbs = f['part_bbs_matrix'][:]\n",
    "            self.query_points = f['query_points_matrix'][:]\n",
    "            self.query_labels = f['query_labels_matrix'][:]\n",
    "            \n",
    "            # Validate dimensions\n",
    "            n_models = len(self.model_ids)\n",
    "            total_configs = self.query_points.shape[0]\n",
    "            \n",
    "            expected_configs = n_models * (MAX_PART_DROP + 1)\n",
    "            assert total_configs >= n_models, \"Need at least one config per model\"\n",
    "            assert total_configs <= expected_configs, \\\n",
    "                f\"Too many configs: got {total_configs}, expected <= {expected_configs}\"\n",
    "            \n",
    "            # Verify array shapes\n",
    "            assert self.part_slices.shape == (n_models + 1,), \\\n",
    "                f\"Invalid part_slices shape: {self.part_slices.shape}\"\n",
    "            assert self.part_drops.shape == (n_models, MAX_PART_DROP), \\\n",
    "                f\"Invalid part_drops shape: {self.part_drops.shape}\"\n",
    "            assert self.part_points.shape[1:] == (N_SUB_POINTS, 3), \\\n",
    "                f\"Invalid part_points shape: {self.part_points.shape}\"\n",
    "            assert self.part_bbs.shape[1:] == (8, 3), \\\n",
    "                f\"Invalid part_bbs shape: {self.part_bbs.shape}\"\n",
    "            assert self.query_points.shape[1:] == (5, N_SUB_POINTS, 3), \\\n",
    "                f\"Invalid query_points shape: {self.query_points.shape}\"\n",
    "            assert self.query_labels.shape[1:] == (5, N_SUB_POINTS), \\\n",
    "                f\"Invalid query_labels shape: {self.query_labels.shape}\"\n",
    "\n",
    "        # Create sample configurations\n",
    "        self.sample_configs = []\n",
    "        \n",
    "        for model_idx, model_id in enumerate(self.model_ids):\n",
    "            # Get part information\n",
    "            start_idx = self.part_slices[model_idx]\n",
    "            end_idx = self.part_slices[model_idx + 1]\n",
    "            n_parts = end_idx - start_idx\n",
    "            \n",
    "            # Calculate query configuration index\n",
    "            query_config_idx = model_idx * (MAX_PART_DROP + 1)\n",
    "            \n",
    "            # Add original configuration (no dropped parts)\n",
    "            self.sample_configs.append({\n",
    "                'model_idx': model_idx,\n",
    "                'query_config_idx': query_config_idx,\n",
    "                'part_slice': (start_idx, end_idx),\n",
    "                'dropped_part_idx': None,\n",
    "                'n_parts': n_parts\n",
    "            })\n",
    "            \n",
    "            # Add part-drop configurations\n",
    "            for drop_idx in range(MAX_PART_DROP):\n",
    "                dropped_part_idx = self.part_drops[model_idx, drop_idx]\n",
    "                if dropped_part_idx != -1:  # Valid part drop\n",
    "                    self.sample_configs.append({\n",
    "                        'model_idx': model_idx,\n",
    "                        'query_config_idx': query_config_idx + drop_idx + 1,\n",
    "                        'part_slice': (start_idx, end_idx),\n",
    "                        'dropped_part_idx': dropped_part_idx,\n",
    "                        'n_parts': n_parts - 1\n",
    "                    })\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of samples in the dataset.\"\"\"\n",
    "        return len(self.sample_configs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a sample from the dataset.\n",
    "        \n",
    "        Args:\n",
    "            idx: Index of the sample to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing:\n",
    "                - part_points: Part point clouds [N_parts, N_points, 3]\n",
    "                - part_bbs: Part bounding boxes [N_parts, 8, 3]\n",
    "                - query_points: Query points [N_queries, 3]\n",
    "                - query_labels: Occupancy labels [N_queries]\n",
    "                - model_id: Model identifier\n",
    "                - n_parts: Number of parts\n",
    "        \"\"\"\n",
    "        config = self.sample_configs[idx]\n",
    "        \n",
    "        # Get part data\n",
    "        start_idx, end_idx = config['part_slice']\n",
    "        part_points = self.part_points[start_idx:end_idx].copy()\n",
    "        part_bbs = self.part_bbs[start_idx:end_idx].copy()\n",
    "        \n",
    "        # Handle dropped part\n",
    "        if config['dropped_part_idx'] is not None:\n",
    "            mask = np.ones(end_idx - start_idx, dtype=bool)\n",
    "            mask[config['dropped_part_idx']] = False\n",
    "            part_points = part_points[mask]\n",
    "            part_bbs = part_bbs[mask]\n",
    "            \n",
    "            # Verify part removal\n",
    "            assert len(part_points) == config['n_parts'], \\\n",
    "                \"Mismatch in number of parts after dropping\"\n",
    "        \n",
    "        # Get query data\n",
    "        query_points = self.query_points[config['query_config_idx']].copy()\n",
    "        query_labels = self.query_labels[config['query_config_idx']].copy()\n",
    "        \n",
    "        # Sample queries if specified\n",
    "        if self.num_queries is not None and self.num_queries < N_POINTS:\n",
    "            indices = np.random.choice(N_POINTS, self.num_queries, replace=False)\n",
    "            query_points = query_points[indices]\n",
    "            query_labels = query_labels[indices]\n",
    "        \n",
    "        return {\n",
    "            'part_points': torch.from_numpy(part_points).float(),\n",
    "            'part_bbs': torch.from_numpy(part_bbs).float(),\n",
    "            'query_points': torch.from_numpy(query_points).float(),\n",
    "            'query_labels': torch.from_numpy(query_labels).float(),\n",
    "            'model_id': self.model_ids[config['model_idx']],\n",
    "            'n_parts': config['n_parts']\n",
    "        }\n",
    "        \n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for batching samples with variable numbers of parts.\n",
    "    Pads part-related tensors to match the maximum number of parts in the batch.\n",
    "    \n",
    "    Args:\n",
    "        batch: List of sample dictionaries from the dataset\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing batched and padded tensors:\n",
    "            - part_points: [batch_size, max_parts, N_points, 3]\n",
    "            - part_bbs: [batch_size, max_parts, 8, 3]\n",
    "            - query_points: [batch_size, 5, N_queries, 3]\n",
    "            - query_labels: [batch_size, 5, N_queries]\n",
    "            - model_ids: List of str, length batch_size\n",
    "            - n_parts: [batch_size] tensor containing actual number of parts\n",
    "            - padding_mask: [batch_size, max_parts] boolean tensor\n",
    "    \"\"\"\n",
    "    # Handle empty batch\n",
    "    if not batch:\n",
    "        raise ValueError(\"Empty batch received\")\n",
    "    \n",
    "    # Validate input tensors\n",
    "    sample_shapes = {\n",
    "        'part_points': batch[0]['part_points'].shape[1:],  # [N_points, 3]\n",
    "        'part_bbs': batch[0]['part_bbs'].shape[1:],       # [8, 3]\n",
    "        'query_points': batch[0]['query_points'].shape,    # [5, N_queries, 3]\n",
    "        'query_labels': batch[0]['query_labels'].shape,    # [5, N_queries]\n",
    "    }\n",
    "    \n",
    "    # Verify consistent shapes across batch\n",
    "    for sample in batch:\n",
    "        assert sample['part_points'].shape[1:] == sample_shapes['part_points'], \\\n",
    "            \"Inconsistent part points shape in batch\"\n",
    "        assert sample['part_bbs'].shape[1:] == sample_shapes['part_bbs'], \\\n",
    "            \"Inconsistent bounding boxes shape in batch\"\n",
    "        assert sample['query_points'].shape == sample_shapes['query_points'], \\\n",
    "            \"Inconsistent query points shape in batch\"\n",
    "        assert sample['query_labels'].shape == sample_shapes['query_labels'], \\\n",
    "            \"Inconsistent query labels shape in batch\"\n",
    "    \n",
    "    # Get batch information\n",
    "    batch_size = len(batch)\n",
    "    max_parts = max(sample['n_parts'] for sample in batch)\n",
    "    n_points = sample_shapes['part_points'][0]\n",
    "    n_queries = sample_shapes['query_points'][1]\n",
    "    \n",
    "    # Pre-allocate tensors with correct shapes and types\n",
    "    part_points_batch = torch.zeros(\n",
    "        batch_size, max_parts, n_points, 3, \n",
    "        dtype=batch[0]['part_points'].dtype\n",
    "    )\n",
    "    part_bbs_batch = torch.zeros(\n",
    "        batch_size, max_parts, 8, 3,\n",
    "        dtype=batch[0]['part_bbs'].dtype\n",
    "    )\n",
    "    query_points_batch = torch.zeros(\n",
    "        batch_size, 5, n_queries, 3,\n",
    "        dtype=batch[0]['query_points'].dtype\n",
    "    )\n",
    "    query_labels_batch = torch.zeros(\n",
    "        batch_size, 5, n_queries,\n",
    "        dtype=batch[0]['query_labels'].dtype\n",
    "    )\n",
    "    padding_mask = torch.ones(\n",
    "        batch_size, max_parts,\n",
    "        dtype=torch.bool\n",
    "    )\n",
    "    n_parts = torch.zeros(\n",
    "        batch_size,\n",
    "        dtype=torch.long\n",
    "    )\n",
    "    \n",
    "    # Fill tensors\n",
    "    for i, sample in enumerate(batch):\n",
    "        n_sample_parts = sample['n_parts']\n",
    "        \n",
    "        # Fill part-related tensors up to n_sample_parts\n",
    "        part_points_batch[i, :n_sample_parts] = sample['part_points']\n",
    "        part_bbs_batch[i, :n_sample_parts] = sample['part_bbs']\n",
    "        \n",
    "        # Update padding mask (False = real part, True = padding)\n",
    "        padding_mask[i, :n_sample_parts] = False\n",
    "        \n",
    "        # Fill query-related tensors (no padding needed)\n",
    "        query_points_batch[i] = sample['query_points']\n",
    "        query_labels_batch[i] = sample['query_labels']\n",
    "        \n",
    "        # Store number of parts\n",
    "        n_parts[i] = n_sample_parts\n",
    "    \n",
    "    return {\n",
    "        'part_points': part_points_batch,     # [B, max_parts, N_points, 3]\n",
    "        'part_bbs': part_bbs_batch,           # [B, max_parts, 8, 3]\n",
    "        'query_points': query_points_batch,   # [B, 5, N_queries, 3]\n",
    "        'query_labels': query_labels_batch,   # [B, 5, N_queries]\n",
    "        'model_ids': [sample['model_id'] for sample in batch],\n",
    "        'n_parts': n_parts,                   # [B]\n",
    "        'padding_mask': padding_mask          # [B, max_parts]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "dataset = PartOccupancyDataset(output_path)\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=True, \n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    print(batch['part_points'].shape)\n",
    "    print(batch['part_bbs'].shape)\n",
    "    print(batch['query_points'].shape)\n",
    "    print(batch['query_labels'].shape)\n",
    "    print(batch['n_parts'])\n",
    "    print(batch['padding_mask'])\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
