{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /ibex/user/slimhy/PADS/code\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Global constants\n",
    "PARTS_DIR = \"/ibex/project/c2273/PADS/3DCoMPaT_occ/parts\"\n",
    "SAMPLES_DIR = \"/ibex/project/c2273/PADS/3DCoMPaT_occ/samples\"\n",
    "MAX_PART_DROP = 16\n",
    "\n",
    "N_POINTS = 131072  # Number of points in each point cloud\n",
    "RATIO_SUB_POINTS = 1/4.  # Number of points in each sub-point cloud\n",
    "N_SUB_POINTS = int(N_POINTS * RATIO_SUB_POINTS)\n",
    "\n",
    "RATIO_SUB_QUERIES = 1/8.\n",
    "N_SUB_QUERIES = int(N_POINTS * RATIO_SUB_QUERIES)\n",
    "\n",
    "\n",
    "def load_part_bbs(model_id):\n",
    "    \"\"\"\n",
    "    Load part bounding boxes for a given model ID.\n",
    "    \n",
    "    Args:\n",
    "        model_id (str): The model identifier\n",
    "    \n",
    "    Returns:\n",
    "        dict: Mapping of part keys to vertex arrays [8, 3] representing the 8 corners of each box\n",
    "    \"\"\"\n",
    "    bb_file = os.path.join(SAMPLES_DIR, f\"{model_id}_orig_0_bbs.pkl\")\n",
    "    bb_data = np.load(bb_file, allow_pickle=True)\n",
    "    bb_data = {k:v for k,v in bb_data}\n",
    "    bb_data = {k:np.array(v.vertices) for k,v in bb_data.items()}\n",
    "    return bb_data\n",
    "\n",
    "\n",
    "def load_occs(model_id, part_drop_id=None):\n",
    "    \"\"\"\n",
    "    Load queries and occupancies for a given model ID.\n",
    "    \n",
    "    Args:\n",
    "        model_id (str): The model identifier\n",
    "        part_drop_id (int, optional): The part drop identifier. If None, loads the original \n",
    "                                    version with no parts dropped.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (queries, occupancies)\n",
    "            - queries: array of shape [5, N_SUB_POINTS, 3]\n",
    "            - occupancies: array of shape [5, N_SUB_POINTS]\n",
    "    \"\"\"\n",
    "    if part_drop_id is None:\n",
    "        occs = os.path.join(SAMPLES_DIR, f\"{model_id}_orig_0_occs.npy\")\n",
    "        queries = os.path.join(SAMPLES_DIR, f\"{model_id}_orig_0_points.npy\")\n",
    "    else:\n",
    "        occs = os.path.join(SAMPLES_DIR, f\"{model_id}_part_drop_{part_drop_id}_occs.npy\")\n",
    "        queries = os.path.join(SAMPLES_DIR, f\"{model_id}_part_drop_{part_drop_id}_points.npy\")\n",
    "    \n",
    "    occs = np.load(occs)\n",
    "    queries = np.load(queries)\n",
    "    \n",
    "    return queries, occs.reshape(*queries.shape[:2], -1).squeeze()\n",
    "\n",
    "\n",
    "def load_part_surf_points(model_id):\n",
    "    \"\"\"\n",
    "    Load part surface points for a given model ID.\n",
    "    \n",
    "    Args:\n",
    "        model_id (str): The model identifier\n",
    "    \n",
    "    Returns:\n",
    "        dict: Mapping of part keys to point arrays [N_SUB_POINTS, 3]\n",
    "    \"\"\"\n",
    "    part_file = os.path.join(PARTS_DIR, f\"{model_id}.npy\")\n",
    "    part_data = np.load(part_file, allow_pickle=True).item()\n",
    "    part_data = {k:np.array(v).squeeze() for k,v in part_data.items()}\n",
    "    return part_data\n",
    "\n",
    "\n",
    "def get_dropped_part_key(model_id, part_drop_id):\n",
    "    \"\"\"\n",
    "    Get the key for the dropped part by comparing original and dropped configurations.\n",
    "    \n",
    "    Args:\n",
    "        model_id (str): The model identifier\n",
    "        part_drop_id (int): The part drop identifier\n",
    "    \n",
    "    Returns:\n",
    "        str: Key of the dropped part\n",
    "    \"\"\"\n",
    "    bb_file = os.path.join(SAMPLES_DIR, f\"{model_id}_part_drop_{part_drop_id}_bbs.pkl\")\n",
    "    bb_data = np.load(bb_file, allow_pickle=True)\n",
    "    bb_data = {k:v for k,v in bb_data}\n",
    "    \n",
    "    bb_file_orig = os.path.join(SAMPLES_DIR, f\"{model_id}_orig_0_bbs.pkl\")\n",
    "    bb_data_orig = np.load(bb_file_orig, allow_pickle=True)\n",
    "    bb_data_orig = {k:v for k,v in bb_data_orig}\n",
    "    \n",
    "    dropped_part_key = set(bb_data_orig.keys()) - set(bb_data.keys())\n",
    "    assert len(dropped_part_key) == 1, f\"Expected exactly one dropped part for {model_id}, drop {part_drop_id}\"\n",
    "    return list(dropped_part_key)[0]\n",
    "\n",
    "\n",
    "def subsample_points(p, labels=None, max_abs_value=1.0):\n",
    "    \"\"\"\n",
    "    Subsample points using random sampling with a fixed ratio.\n",
    "    For query points (with labels), first filters points within [-max_abs_value, max_abs_value]^3.\n",
    "    \n",
    "    Args:\n",
    "        p: Points array of shape [N, 3]\n",
    "        labels: Optional labels array of shape [N]. If provided, indicates query point processing.\n",
    "        max_abs_value: Maximum absolute value for point coordinates when filtering. Defaults to 1.0.\n",
    "        \n",
    "    Returns:\n",
    "        Subsampled points (and labels if provided)\n",
    "    \"\"\"\n",
    "    p = torch.as_tensor(p)\n",
    "    \n",
    "    if labels is not None:\n",
    "        # Query points - filter to bounded cube first\n",
    "        mask = torch.all(torch.abs(p) <= max_abs_value, dim=1)\n",
    "        p = p[mask]\n",
    "        labels = labels[mask]\n",
    "        \n",
    "        # Get exact number of samples\n",
    "        n_samples = N_SUB_QUERIES\n",
    "        idx = torch.randperm(len(p))[:n_samples]\n",
    "        \n",
    "        p = p[idx]\n",
    "        labels = labels[idx]\n",
    "        \n",
    "        return p.numpy(), labels\n",
    "    else:\n",
    "        print(len(p), N_SUB_POINTS)\n",
    "        # Part points - just random sampling\n",
    "        idx = torch.randperm(len(p))[:N_SUB_POINTS]\n",
    "        assert len(idx) == N_SUB_POINTS, f\"Invalid subsampling length: {len(idx)}\"\n",
    "        \n",
    "        return p[idx].numpy()\n",
    "    \n",
    "    \n",
    "def create_stacked_matrices(model_ids):\n",
    "    \"\"\"\n",
    "    Create stacked matrices for part points, bounding boxes, query points, and occupancies.\n",
    "    \n",
    "    Args:\n",
    "        model_ids (list): List of model identifiers to process\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing all stacked matrices and metadata arrays\n",
    "    \"\"\"\n",
    "    # First pass: determine total sizes\n",
    "    total_parts = 0\n",
    "    total_query_configs = 0\n",
    "    part_slices = []\n",
    "    \n",
    "    print(\"Calculating matrix dimensions...\")\n",
    "    for model_id in tqdm(model_ids):\n",
    "        part_data = load_part_surf_points(model_id)\n",
    "        n_parts = len(part_data)\n",
    "        part_slices.append(total_parts)\n",
    "        total_parts += n_parts\n",
    "        total_query_configs += (MAX_PART_DROP + 1)  # Include original configuration\n",
    "    \n",
    "    part_slices.append(total_parts)\n",
    "    part_slices = np.array(part_slices, dtype=np.int32)\n",
    "    \n",
    "    # Initialize matrices with proper data types\n",
    "    part_points_matrix = np.zeros((total_parts, N_SUB_POINTS, 3), dtype=np.float32)\n",
    "    part_bbs_matrix = np.zeros((total_parts, 8, 3), dtype=np.float32)\n",
    "    query_points_matrix = np.zeros((total_query_configs, 5, N_SUB_QUERIES, 3), dtype=np.float32)\n",
    "    query_labels_matrix = np.zeros((total_query_configs, 5, N_SUB_QUERIES), dtype=np.float32)\n",
    "    part_drops = np.full((len(model_ids), MAX_PART_DROP), -1, dtype=np.int32)\n",
    "    model_ids_array = np.array(model_ids, dtype='S')\n",
    "    \n",
    "    print(\"Filling matrices...\")\n",
    "    part_idx = 0\n",
    "    query_config_idx = 0\n",
    "    \n",
    "    for model_idx, model_id in enumerate(tqdm(model_ids)):\n",
    "        # Load and validate data\n",
    "        part_points = load_part_surf_points(model_id)\n",
    "        part_bbs = load_part_bbs(model_id)\n",
    "        \n",
    "        # Validate data consistency\n",
    "        assert set(part_points.keys()) == set(part_bbs.keys()), \\\n",
    "            f\"Mismatch in part keys for model {model_id}\"\n",
    "        \n",
    "        # Fill part matrices\n",
    "        for part_key in sorted(part_points.keys()):\n",
    "            points = part_points[part_key]\n",
    "            bbs = part_bbs[part_key]\n",
    "            \n",
    "            # Validate shapes\n",
    "            assert points.shape == (N_POINTS, 3), \\\n",
    "                f\"Invalid point shape for model {model_id}, part {part_key}: {points.shape}\"\n",
    "            # And update the validation in the filling section:\n",
    "            assert bbs.shape == (8, 3), \\\n",
    "                f\"Invalid BB shape for model {model_id}, part {part_key}: {bbs.shape}\"\n",
    "                \n",
    "            part_points_matrix[part_idx] = subsample_points(points)\n",
    "            part_bbs_matrix[part_idx] = bbs\n",
    "            part_idx += 1\n",
    "        \n",
    "        # Fill query matrices - first the original configuration\n",
    "        queries_orig, occs_orig = load_occs(model_id, part_drop_id=None)\n",
    "        assert queries_orig.shape == (5, N_POINTS, 3), \\\n",
    "            f\"Invalid query shape for original config of model {model_id}: {queries_orig.shape}\"\n",
    "        assert occs_orig.shape == (5, N_POINTS), \\\n",
    "            f\"Invalid occupancy shape for original config of model {model_id}: {occs_orig.shape}\"\n",
    "        \n",
    "        for i in range(5):\n",
    "            max_range = 0.8\n",
    "            queries_orig_sub, occs_orig_sub = subsample_points(queries_orig[i], occs_orig[i], max_abs_value=max_range)\n",
    "            query_points_matrix[query_config_idx][i] = queries_orig_sub\n",
    "            query_labels_matrix[query_config_idx][i] = occs_orig_sub\n",
    "\n",
    "        query_config_idx += 1\n",
    "        \n",
    "        # Then process part drop configurations\n",
    "        for part_drop_id in range(MAX_PART_DROP):\n",
    "            try:\n",
    "                # Record dropped part\n",
    "                dropped_key = get_dropped_part_key(model_id, part_drop_id)\n",
    "                dropped_idx = list(sorted(part_points.keys())).index(dropped_key)\n",
    "                part_drops[model_idx, part_drop_id] = dropped_idx\n",
    "                \n",
    "                # Load and validate query data\n",
    "                queries, occs = load_occs(model_id, part_drop_id)\n",
    "                assert queries.shape == (5, N_POINTS, 3), \\\n",
    "                    f\"Invalid query shape for model {model_id}, drop {part_drop_id}: {queries.shape}\"\n",
    "                assert occs.shape == (5, N_POINTS), \\\n",
    "                    f\"Invalid occupancy shape for model {model_id}, drop {part_drop_id}: {occs.shape}\"\n",
    "                    \n",
    "                for i in range(5):\n",
    "                    max_range = 0.75 if i <= 3 else 0.5\n",
    "                    queries_sub, occs_sub = subsample_points(queries[i], occs[i], max_abs_value=max_range)\n",
    "                    query_points_matrix[query_config_idx][i] = queries_sub\n",
    "                    query_labels_matrix[query_config_idx][i] = occs_sub\n",
    "                \n",
    "                query_config_idx += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Failed to process part drop {part_drop_id} \"\n",
    "                      f\"for model {model_id}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    # Verify final counts match expected values\n",
    "    assert part_idx == total_parts, \\\n",
    "        f\"Mismatch in part count: got {part_idx}, expected {total_parts}\"\n",
    "    assert query_config_idx <= total_query_configs, \\\n",
    "        f\"Mismatch in query config count: got {query_config_idx}, expected {total_query_configs}\"\n",
    "    \n",
    "    return {\n",
    "        'model_ids': model_ids_array,\n",
    "        'part_slices': part_slices,\n",
    "        'part_drops': part_drops,\n",
    "        'part_points_matrix': part_points_matrix,\n",
    "        'part_bbs_matrix': part_bbs_matrix,\n",
    "        'query_points_matrix': query_points_matrix[:query_config_idx],\n",
    "        'query_labels_matrix': query_labels_matrix[:query_config_idx]\n",
    "    }\n",
    "\n",
    "def save_to_hdf5(matrices, output_path):\n",
    "    \"\"\"\n",
    "    Save the stacked matrices to a single HDF5 file.\n",
    "    \n",
    "    Args:\n",
    "        matrices (dict): Dictionary containing matrices to save\n",
    "        output_path (str): Path where to save the HDF5 file\n",
    "    \"\"\"\n",
    "    print(\"Saving matrices to HDF5...\")\n",
    "    with h5py.File(output_path, 'w') as f:\n",
    "        for key, matrix in matrices.items():\n",
    "            f.create_dataset(key, data=matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_processed_models(out_path):\n",
    "    \"\"\"\n",
    "    Initialize the set of processed model IDs by including only model IDs\n",
    "    that appear exactly 68 times in the file listing.\n",
    "\n",
    "    Args:\n",
    "        out_path (str): Path to the directory containing the files\n",
    "\n",
    "    Global Effects:\n",
    "        Updates the PROCESSED_MODELS global set with qualifying model IDs\n",
    "    \"\"\"\n",
    "    all_files = os.listdir(out_path)\n",
    "\n",
    "    # Count occurrences of each model ID\n",
    "    model_counts = {}\n",
    "    for filename in all_files:\n",
    "        if len(filename) >= 6:  # Ensure filename is long enough\n",
    "            model_id = filename[:6]\n",
    "            model_counts[model_id] = model_counts.get(model_id, 0) + 1\n",
    "\n",
    "    # Add only model IDs that appear exactly 68 times\n",
    "    return {\n",
    "        model_id for model_id, count in model_counts.items() if count == 68\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ids = initialize_processed_models(SAMPLES_DIR)\n",
    "\n",
    "error_ids = [\"25_41d\", \"10_01d\"]\n",
    "\n",
    "# Remove models with errors\n",
    "model_ids = sorted(list(set(model_ids) - set(error_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stacked matrices\n",
    "matrices = create_stacked_matrices(list(model_ids)[:100])\n",
    "\n",
    "# Save to HDF5\n",
    "output_path = '/ibex/project/c2273/PADS/3DCoMPaT_occ/dataset__debug.h5'\n",
    "save_to_hdf5(matrices, output_path)\n",
    "print(f\"Dataset created successfully at {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure output size in GB\n",
    "output_size = os.path.getsize(output_path) / 1e9\n",
    "# Estimate size for full dataset\n",
    "full_size = output_size * len(model_ids) / 100\n",
    "print(f\"Output size: {output_size:.3f} GB\")\n",
    "print(f\"Estimated full size: {full_size:.3f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a file to confirm that the dataset was created successfully\n",
    "with open('/ibex/project/c2273/PADS/3DCoMPaT_occ/dataset__debug.txt', 'w') as f:\n",
    "    f.write(\"Dataset created successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
