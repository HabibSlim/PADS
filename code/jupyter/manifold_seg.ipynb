{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ibex/user/slimhy/PADS/code\n"
     ]
    }
   ],
   "source": [
    "%cd /ibex/user/slimhy/PADS/code/\n",
    "\"\"\"\n",
    "Extracting features into HDF5 files for each split.\n",
    "\"\"\"\n",
    "import argparse\n",
    "import torch\n",
    "import trimesh\n",
    "\n",
    "import util.misc as misc\n",
    "import util.s2vs as s2vs\n",
    "\n",
    "from datasets.shapeloaders import CoMPaTManifoldDataset, PartNetManifoldDataset, SingleManifoldDataset\n",
    "from util.misc import d_GPU, show_side_by_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.metadata import (\n",
    "    COMPAT_CLASSES,\n",
    "    int_to_hex,\n",
    ")\n",
    "import os\n",
    "from util.misc import CUDAMesh\n",
    "\n",
    "\n",
    "class CoMPaTSegDataset(SingleManifoldDataset):\n",
    "    \"\"\"\n",
    "    Sampling from a 3DCoMPaT manifold mesh dataset with segmentation labels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args,\n",
    "        seg_dir,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.seg_dir = seg_dir\n",
    "        super().__init__(*args, **kwargs)\n",
    "        if self.normalize:\n",
    "            print(\n",
    "                \"normalize=True but 3DCoMPaT shapes are already normalized to their bounding boxes.\"\n",
    "            )\n",
    "\n",
    "    def get_mesh(self, idx=None):\n",
    "        \"\"\"\n",
    "        Load the mesh from the given index.\n",
    "        \"\"\"\n",
    "        if idx is None:\n",
    "            idx = self.mesh_idx\n",
    "\n",
    "        if self.mesh is None:\n",
    "            self.mesh = CUDAMesh.load(self.obj_files[idx], to_cuda=self.to_cuda)\n",
    "\n",
    "            # Print an alert if the mesh is not watertight\n",
    "            if not self.mesh.is_watertight:\n",
    "                print(\"Mesh is not watertight! Performing robust conversion...\")\n",
    "                obj_base_name = os.path.basename(self.obj_files[idx])\n",
    "                robust_pcu_to_manifold(self.obj_files[idx], \"/tmp/\" + obj_base_name)\n",
    "                # Try to load and test if watertight\n",
    "                self.mesh = CUDAMesh.load(\"/tmp/\" + obj_base_name, to_cuda=self.to_cuda)\n",
    "                if not self.mesh.is_watertight:\n",
    "                    raise ValueError(\"Watertight conversion failed!\")\n",
    "\n",
    "                # Replace the original mesh with the watertight one\n",
    "                # Write to original file\n",
    "                self.mesh.export(self.obj_files[idx])\n",
    "                print(\"Watertight conversion successful!\")\n",
    "\n",
    "            # Decimate the mesh if it has too many faces\n",
    "            if self.decimate and len(self.mesh.faces) > self.MAX_FACES:\n",
    "                # The ratio is the percentage of faces to REMOVE\n",
    "                ratio = 1 - self.MAX_FACES / len(self.mesh.faces)\n",
    "                self.mesh = decimate_mesh(self.mesh, ratio)\n",
    "\n",
    "        return self.mesh\n",
    "    \n",
    "    def set_seg(self, idx=None):\n",
    "        \"\"\"\n",
    "        Load the segmentation from the given index.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mesh_idx != idx or self.mesh is None:\n",
    "            self.mesh_idx = idx\n",
    "            self.get_mesh(idx)\n",
    "\n",
    "        # Optionally: first sample n_points first\n",
    "        # And simply serve the same points for the rest of the iterations\n",
    "        if self.sample_first:\n",
    "            # Use batch sampling\n",
    "            n_batches = self.n_points // self.MAX_SAMPLE_SIZE\n",
    "            all_points, all_occs = [], []\n",
    "            for k in range(n_batches):\n",
    "                if k % 4 == 0:\n",
    "                    print(\"Sampling batch [%d/%d]\" % (k + 1, n_batches))\n",
    "                points, occs = self.sampling_fn(self.mesh, self.MAX_SAMPLE_SIZE)\n",
    "                all_points += [points]\n",
    "                all_occs += [occs]\n",
    "            print()\n",
    "            points_idx = list(range(len(all_points)))\n",
    "\n",
    "        # Resample the mesh\n",
    "        for _ in range(self.max_it):\n",
    "            if self.sample_first:\n",
    "                rnd_idx = np.random.choice(points_idx)\n",
    "                points = all_points[rnd_idx]\n",
    "                occs = all_occs[rnd_idx]\n",
    "            else:\n",
    "                points, occs = self.sampling_fn(self.mesh, self.n_points)\n",
    "\n",
    "            # Optionally: normalize the point cloud\n",
    "            if self.normalize:\n",
    "                points = normalize_pc(points)\n",
    "            yield points, occs\n",
    "\n",
    "    def init_class_objs(self):\n",
    "        \"\"\"\n",
    "        Set the list of objects for a given class/split.\n",
    "        \"\"\"\n",
    "\n",
    "        def join_all(in_dir, files):\n",
    "            return sorted([os.path.join(in_dir, f) for f in files])\n",
    "\n",
    "        compat_cls_code = int_to_hex(COMPAT_CLASSES[self.shape_cls])\n",
    "        obj_files = os.listdir(self.obj_dir)\n",
    "        # obj_files = [os.path.join(self.obj_dir, f) for f in obj_files]\n",
    "        obj_files = [\n",
    "            f for f in obj_files if f.endswith(\".obj\") and compat_cls_code + \"_\" in f\n",
    "        ]\n",
    "\n",
    "        if self.split == \"all\":\n",
    "            self.obj_files = join_all(self.obj_dir, obj_files)\n",
    "            self.seg_files = join_all(self.seg_dir, [f.replace(\".obj\", \".gltf\") for f in obj_files])\n",
    "            return\n",
    "\n",
    "        # Open the split metadata\n",
    "        pwd = os.path.dirname(os.path.realpath(__file__))\n",
    "        split_dict = json.load(open(os.path.join(pwd, \"CoMPaT\", \"split.json\")))\n",
    "\n",
    "        # Filter split meshes\n",
    "        obj_files = [\n",
    "            f\n",
    "            for f in obj_files\n",
    "            if f.split(\".\")[0] in split_dict[self.split]\n",
    "        ]\n",
    "\n",
    "        self.obj_files = join_all(self.obj_dir, obj_files)\n",
    "        self.seg_files = join_all(self.seg_dir, [f.replace(\".obj\", \".gltf\") for f in obj_files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVE_CLASS = \"chair\"\n",
    "OBJ_DIR  = \"/ibex/project/c2273/3DCoMPaT/obj_manifold/\"\n",
    "SEG_DIR  = \"/ibex/project/c2273/3DCoMPaT/gltf/\"\n",
    "ZIP_PATH = \"/ibex/project/c2273/3DCoMPaT/3DCoMPaT_ZIP.zip\"\n",
    "OBJ_ID = 0\n",
    "\n",
    "dataset = CoMPaTSegDataset(\n",
    "    OBJ_DIR,\n",
    "    ACTIVE_CLASS,\n",
    "    10000,\n",
    "    seg_dir=SEG_DIR,\n",
    "    normalize=False,\n",
    "    sampling_method=\"surface\",\n",
    "    to_cuda=False\n",
    ")\n",
    "surface_points, _ = next(dataset[OBJ_ID])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import json\n",
    "from datasets.CoMPaT.utils3D import gltf\n",
    "\n",
    "zip_f = zipfile.ZipFile(ZIP_PATH, \"r\")\n",
    "textures_map = json.load(zip_f.open(\"textures_map.json\", \"r\"))\n",
    "\n",
    "class ZipTextureResolver(trimesh.resolvers.FilePathResolver):\n",
    "    \"\"\"\n",
    "    Resolve texture files from the input zip.\n",
    "    \"\"\"\n",
    "    def __init__(self, zip_f):\n",
    "        self.zip_f = zip_f\n",
    "\n",
    "    def get(self, file_path):\n",
    "        return self.zip_f.open(file_path, \"r\").read()\n",
    "\n",
    "def load_gltf(gltf_f):\n",
    "    gltf_f = gltf.apply_placeholder(open(gltf_f), textures_map)\n",
    "    return trimesh.load(\n",
    "        gltf_f,\n",
    "        file_type=\".gltf\",\n",
    "        force=\"scene\",\n",
    "        resolver=ZipTextureResolver(zip_f),\n",
    "    )\n",
    "    \n",
    "gltf_file = os.path.join(SEG_DIR, dataset.obj_files[0].split(\"/\")[-1].split(\".\")[0] + \".gltf\")\n",
    "gltf_model = load_gltf(gltf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b0d6eb74938483ba7f051e0329967b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import k3d\n",
    "\n",
    "# Silence traittypes warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Create the K3D plot\n",
    "plot = k3d.plot()\n",
    "mesh = dataset.get_mesh().trimesh_mesh\n",
    "plot += k3d.mesh(mesh.vertices, mesh.faces, color=0xe1e0df)\n",
    "plot += k3d.points(surface_points, point_size=0.01, color=0xe1e0df)\n",
    "\n",
    "# Display the plot\n",
    "plot.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
