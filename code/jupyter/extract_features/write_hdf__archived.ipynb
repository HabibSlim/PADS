{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ibex/user/slimhy/PADS/code\n"
     ]
    }
   ],
   "source": [
    "%cd /ibex/user/slimhy/PADS/code\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Global constants\n",
    "PARTS_DIR = \"/ibex/project/c2273/PADS/3DCoMPaT_occ/parts\"\n",
    "SAMPLES_DIR = \"/ibex/project/c2273/PADS/3DCoMPaT_occ/samples\"\n",
    "MAX_PART_DROP = 16\n",
    "\n",
    "N_POINTS = 131072  # Number of points in each point cloud\n",
    "RATIO_SUB_POINTS = 1/4.  # Number of points in each sub-point cloud\n",
    "N_SUB_POINTS = int(N_POINTS * RATIO_SUB_POINTS)\n",
    "\n",
    "RATIO_SUB_QUERIES = 1/8.\n",
    "N_SUB_QUERIES = int(N_POINTS * RATIO_SUB_QUERIES)\n",
    "\n",
    "\n",
    "def load_part_bbs(model_id):\n",
    "    \"\"\"\n",
    "    Load part bounding boxes for a given model ID.\n",
    "    \n",
    "    Args:\n",
    "        model_id (str): The model identifier\n",
    "    \n",
    "    Returns:\n",
    "        dict: Mapping of part keys to vertex arrays [8, 3] representing the 8 corners of each box\n",
    "    \"\"\"\n",
    "    bb_file = os.path.join(SAMPLES_DIR, f\"{model_id}_orig_0_bbs.pkl\")\n",
    "    bb_data = np.load(bb_file, allow_pickle=True)\n",
    "    bb_data = {k:v for k,v in bb_data}\n",
    "    bb_data = {k:np.array(v.vertices) for k,v in bb_data.items()}\n",
    "    return bb_data\n",
    "\n",
    "\n",
    "def load_occs(model_id, part_drop_id=None):\n",
    "    \"\"\"\n",
    "    Load queries and occupancies for a given model ID.\n",
    "    \n",
    "    Args:\n",
    "        model_id (str): The model identifier\n",
    "        part_drop_id (int, optional): The part drop identifier. If None, loads the original \n",
    "                                    version with no parts dropped.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (queries, occupancies)\n",
    "            - queries: array of shape [5, N_SUB_POINTS, 3]\n",
    "            - occupancies: array of shape [5, N_SUB_POINTS]\n",
    "    \"\"\"\n",
    "    if part_drop_id is None:\n",
    "        occs = os.path.join(SAMPLES_DIR, f\"{model_id}_orig_0_occs.npy\")\n",
    "        queries = os.path.join(SAMPLES_DIR, f\"{model_id}_orig_0_points.npy\")\n",
    "    else:\n",
    "        occs = os.path.join(SAMPLES_DIR, f\"{model_id}_part_drop_{part_drop_id}_occs.npy\")\n",
    "        queries = os.path.join(SAMPLES_DIR, f\"{model_id}_part_drop_{part_drop_id}_points.npy\")\n",
    "    \n",
    "    occs = np.load(occs)\n",
    "    queries = np.load(queries)\n",
    "    \n",
    "    return queries, occs.reshape(*queries.shape[:2], -1).squeeze()\n",
    "\n",
    "\n",
    "def load_part_surf_points(model_id):\n",
    "    \"\"\"\n",
    "    Load part surface points for a given model ID.\n",
    "    \n",
    "    Args:\n",
    "        model_id (str): The model identifier\n",
    "    \n",
    "    Returns:\n",
    "        dict: Mapping of part keys to point arrays [N_SUB_POINTS, 3]\n",
    "    \"\"\"\n",
    "    part_file = os.path.join(PARTS_DIR, f\"{model_id}.npy\")\n",
    "    part_data = np.load(part_file, allow_pickle=True).item()\n",
    "    part_data = {k:np.array(v).squeeze() for k,v in part_data.items()}\n",
    "    return part_data\n",
    "\n",
    "\n",
    "def get_dropped_part_key(model_id, part_drop_id):\n",
    "    \"\"\"\n",
    "    Get the key for the dropped part by comparing original and dropped configurations.\n",
    "    \n",
    "    Args:\n",
    "        model_id (str): The model identifier\n",
    "        part_drop_id (int): The part drop identifier\n",
    "    \n",
    "    Returns:\n",
    "        str: Key of the dropped part\n",
    "    \"\"\"\n",
    "    bb_file = os.path.join(SAMPLES_DIR, f\"{model_id}_part_drop_{part_drop_id}_bbs.pkl\")\n",
    "    bb_data = np.load(bb_file, allow_pickle=True)\n",
    "    bb_data = {k:v for k,v in bb_data}\n",
    "    \n",
    "    bb_file_orig = os.path.join(SAMPLES_DIR, f\"{model_id}_orig_0_bbs.pkl\")\n",
    "    bb_data_orig = np.load(bb_file_orig, allow_pickle=True)\n",
    "    bb_data_orig = {k:v for k,v in bb_data_orig}\n",
    "    \n",
    "    dropped_part_key = set(bb_data_orig.keys()) - set(bb_data.keys())\n",
    "    assert len(dropped_part_key) == 1, f\"Expected exactly one dropped part for {model_id}, drop {part_drop_id}\"\n",
    "    return list(dropped_part_key)[0]\n",
    "\n",
    "\n",
    "def subsample_points(p, labels=None, max_abs_value=1.0):\n",
    "    \"\"\"\n",
    "    Subsample points using random sampling with a fixed ratio.\n",
    "    For query points (with labels), first filters points within [-max_abs_value, max_abs_value]^3.\n",
    "    \n",
    "    Args:\n",
    "        p: Points array of shape [N, 3]\n",
    "        labels: Optional labels array of shape [N]. If provided, indicates query point processing.\n",
    "        max_abs_value: Maximum absolute value for point coordinates when filtering. Defaults to 1.0.\n",
    "        \n",
    "    Returns:\n",
    "        Subsampled points (and labels if provided)\n",
    "    \"\"\"\n",
    "    p = torch.as_tensor(p)\n",
    "    \n",
    "    if labels is not None:\n",
    "        # Query points - filter to bounded cube first\n",
    "        mask = torch.all(torch.abs(p) <= max_abs_value, dim=1)\n",
    "        p = p[mask]\n",
    "        labels = labels[mask]\n",
    "        \n",
    "        # Get exact number of samples\n",
    "        n_samples = N_SUB_QUERIES\n",
    "        idx = torch.randperm(len(p))[:n_samples]\n",
    "        \n",
    "        p = p[idx]\n",
    "        labels = labels[idx]\n",
    "        \n",
    "        return p.numpy(), labels\n",
    "    else:\n",
    "        print(len(p), N_SUB_POINTS)\n",
    "        # Part points - just random sampling\n",
    "        idx = torch.randperm(len(p))[:N_SUB_POINTS]\n",
    "        assert len(idx) == N_SUB_POINTS, f\"Invalid subsampling length: {len(idx)}\"\n",
    "        \n",
    "        return p[idx].numpy()\n",
    "    \n",
    "    \n",
    "def create_stacked_matrices(model_ids):\n",
    "    \"\"\"\n",
    "    Create stacked matrices for part points, bounding boxes, query points, and occupancies.\n",
    "    \n",
    "    Args:\n",
    "        model_ids (list): List of model identifiers to process\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing all stacked matrices and metadata arrays\n",
    "    \"\"\"\n",
    "    # First pass: determine total sizes\n",
    "    total_parts = 0\n",
    "    total_query_configs = 0\n",
    "    part_slices = []\n",
    "    \n",
    "    print(\"Calculating matrix dimensions...\")\n",
    "    for model_id in tqdm(model_ids):\n",
    "        part_data = load_part_surf_points(model_id)\n",
    "        n_parts = len(part_data)\n",
    "        part_slices.append(total_parts)\n",
    "        total_parts += n_parts\n",
    "        total_query_configs += (MAX_PART_DROP + 1)  # Include original configuration\n",
    "    \n",
    "    part_slices.append(total_parts)\n",
    "    part_slices = np.array(part_slices, dtype=np.int32)\n",
    "    \n",
    "    # Initialize matrices with proper data types\n",
    "    part_points_matrix = np.zeros((total_parts, N_SUB_POINTS, 3), dtype=np.float32)\n",
    "    part_bbs_matrix = np.zeros((total_parts, 8, 3), dtype=np.float32)\n",
    "    query_points_matrix = np.zeros((total_query_configs, 5, N_SUB_QUERIES, 3), dtype=np.float32)\n",
    "    query_labels_matrix = np.zeros((total_query_configs, 5, N_SUB_QUERIES), dtype=np.float32)\n",
    "    part_drops = np.full((len(model_ids), MAX_PART_DROP), -1, dtype=np.int32)\n",
    "    model_ids_array = np.array(model_ids, dtype='S')\n",
    "    \n",
    "    print(\"Filling matrices...\")\n",
    "    part_idx = 0\n",
    "    query_config_idx = 0\n",
    "    \n",
    "    for model_idx, model_id in enumerate(tqdm(model_ids)):\n",
    "        # Load and validate data\n",
    "        part_points = load_part_surf_points(model_id)\n",
    "        part_bbs = load_part_bbs(model_id)\n",
    "        \n",
    "        # Validate data consistency\n",
    "        assert set(part_points.keys()) == set(part_bbs.keys()), \\\n",
    "            f\"Mismatch in part keys for model {model_id}\"\n",
    "        \n",
    "        # Fill part matrices\n",
    "        for part_key in sorted(part_points.keys()):\n",
    "            points = part_points[part_key]\n",
    "            bbs = part_bbs[part_key]\n",
    "            \n",
    "            # Validate shapes\n",
    "            assert points.shape == (N_POINTS, 3), \\\n",
    "                f\"Invalid point shape for model {model_id}, part {part_key}: {points.shape}\"\n",
    "            # And update the validation in the filling section:\n",
    "            assert bbs.shape == (8, 3), \\\n",
    "                f\"Invalid BB shape for model {model_id}, part {part_key}: {bbs.shape}\"\n",
    "                \n",
    "            part_points_matrix[part_idx] = subsample_points(points)\n",
    "            part_bbs_matrix[part_idx] = bbs\n",
    "            part_idx += 1\n",
    "        \n",
    "        # Fill query matrices - first the original configuration\n",
    "        queries_orig, occs_orig = load_occs(model_id, part_drop_id=None)\n",
    "        assert queries_orig.shape == (5, N_POINTS, 3), \\\n",
    "            f\"Invalid query shape for original config of model {model_id}: {queries_orig.shape}\"\n",
    "        assert occs_orig.shape == (5, N_POINTS), \\\n",
    "            f\"Invalid occupancy shape for original config of model {model_id}: {occs_orig.shape}\"\n",
    "        \n",
    "        for i in range(5):\n",
    "            max_range = 0.8\n",
    "            queries_orig_sub, occs_orig_sub = subsample_points(queries_orig[i], occs_orig[i], max_abs_value=max_range)\n",
    "            query_points_matrix[query_config_idx][i] = queries_orig_sub\n",
    "            query_labels_matrix[query_config_idx][i] = occs_orig_sub\n",
    "\n",
    "        query_config_idx += 1\n",
    "        \n",
    "        # Then process part drop configurations\n",
    "        for part_drop_id in range(MAX_PART_DROP):\n",
    "            try:\n",
    "                # Record dropped part\n",
    "                dropped_key = get_dropped_part_key(model_id, part_drop_id)\n",
    "                dropped_idx = list(sorted(part_points.keys())).index(dropped_key)\n",
    "                part_drops[model_idx, part_drop_id] = dropped_idx\n",
    "                \n",
    "                # Load and validate query data\n",
    "                queries, occs = load_occs(model_id, part_drop_id)\n",
    "                assert queries.shape == (5, N_POINTS, 3), \\\n",
    "                    f\"Invalid query shape for model {model_id}, drop {part_drop_id}: {queries.shape}\"\n",
    "                assert occs.shape == (5, N_POINTS), \\\n",
    "                    f\"Invalid occupancy shape for model {model_id}, drop {part_drop_id}: {occs.shape}\"\n",
    "                    \n",
    "                for i in range(5):\n",
    "                    max_range = 0.75 if i <= 3 else 0.5\n",
    "                    queries_sub, occs_sub = subsample_points(queries[i], occs[i], max_abs_value=max_range)\n",
    "                    query_points_matrix[query_config_idx][i] = queries_sub\n",
    "                    query_labels_matrix[query_config_idx][i] = occs_sub\n",
    "                \n",
    "                query_config_idx += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Failed to process part drop {part_drop_id} \"\n",
    "                      f\"for model {model_id}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    # Verify final counts match expected values\n",
    "    assert part_idx == total_parts, \\\n",
    "        f\"Mismatch in part count: got {part_idx}, expected {total_parts}\"\n",
    "    assert query_config_idx <= total_query_configs, \\\n",
    "        f\"Mismatch in query config count: got {query_config_idx}, expected {total_query_configs}\"\n",
    "    \n",
    "    return {\n",
    "        'model_ids': model_ids_array,\n",
    "        'part_slices': part_slices,\n",
    "        'part_drops': part_drops,\n",
    "        'part_points_matrix': part_points_matrix,\n",
    "        'part_bbs_matrix': part_bbs_matrix,\n",
    "        'query_points_matrix': query_points_matrix[:query_config_idx],\n",
    "        'query_labels_matrix': query_labels_matrix[:query_config_idx]\n",
    "    }\n",
    "\n",
    "def save_to_hdf5(matrices, output_path):\n",
    "    \"\"\"\n",
    "    Save the stacked matrices to a single HDF5 file.\n",
    "    \n",
    "    Args:\n",
    "        matrices (dict): Dictionary containing matrices to save\n",
    "        output_path (str): Path where to save the HDF5 file\n",
    "    \"\"\"\n",
    "    print(\"Saving matrices to HDF5...\")\n",
    "    with h5py.File(output_path, 'w') as f:\n",
    "        for key, matrix in matrices.items():\n",
    "            f.create_dataset(key, data=matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_processed_models(out_path):\n",
    "    \"\"\"\n",
    "    Initialize the set of processed model IDs by including only model IDs\n",
    "    that appear exactly 68 times in the file listing.\n",
    "\n",
    "    Args:\n",
    "        out_path (str): Path to the directory containing the files\n",
    "\n",
    "    Global Effects:\n",
    "        Updates the PROCESSED_MODELS global set with qualifying model IDs\n",
    "    \"\"\"\n",
    "    all_files = os.listdir(out_path)\n",
    "\n",
    "    # Count occurrences of each model ID\n",
    "    model_counts = {}\n",
    "    for filename in all_files:\n",
    "        if len(filename) >= 6:  # Ensure filename is long enough\n",
    "            model_id = filename[:6]\n",
    "            model_counts[model_id] = model_counts.get(model_id, 0) + 1\n",
    "\n",
    "    # Add only model IDs that appear exactly 68 times\n",
    "    return {\n",
    "        model_id for model_id, count in model_counts.items() if count == 68\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ids = initialize_processed_models(SAMPLES_DIR)\n",
    "\n",
    "error_ids = [\"25_41d\", \"10_01d\"]\n",
    "\n",
    "# Remove models with errors\n",
    "model_ids = sorted(list(set(model_ids) - set(error_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating matrix dimensions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 34.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling matrices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131072 32768\n",
      "131072 32768\n",
      "131072 32768\n",
      "131072 32768\n",
      "131072 32768\n",
      "131072 32768\n",
      "131072 32768\n",
      "131072 32768\n",
      "131072 32768\n",
      "131072 32768\n",
      "131072 32768\n",
      "131072 32768\n",
      "131072 32768\n",
      "131072 32768\n",
      "131072 32768\n",
      "131072 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Invalid query shape for original config of model 00_000: (3, 131072, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create stacked matrices\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m matrices \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_stacked_matrices\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Save to HDF5\u001b[39;00m\n\u001b[1;32m      5\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/ibex/project/c2273/PADS/3DCoMPaT_occ/dataset__debug.h5\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[1], line 208\u001b[0m, in \u001b[0;36mcreate_stacked_matrices\u001b[0;34m(model_ids)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# Fill query matrices - first the original configuration\u001b[39;00m\n\u001b[1;32m    207\u001b[0m queries_orig, occs_orig \u001b[38;5;241m=\u001b[39m load_occs(model_id, part_drop_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 208\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m queries_orig\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (\u001b[38;5;241m5\u001b[39m, N_POINTS, \u001b[38;5;241m3\u001b[39m), \\\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid query shape for original config of model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mqueries_orig\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m occs_orig\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (\u001b[38;5;241m5\u001b[39m, N_POINTS), \\\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid occupancy shape for original config of model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moccs_orig\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n",
      "\u001b[0;31mAssertionError\u001b[0m: Invalid query shape for original config of model 00_000: (3, 131072, 3)"
     ]
    }
   ],
   "source": [
    "# Create stacked matrices\n",
    "matrices = create_stacked_matrices(list(model_ids)[:100])\n",
    "\n",
    "# Save to HDF5\n",
    "output_path = '/ibex/project/c2273/PADS/3DCoMPaT_occ/dataset__debug.h5'\n",
    "save_to_hdf5(matrices, output_path)\n",
    "print(f\"Dataset created successfully at {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure output size in GB\n",
    "output_size = os.path.getsize(output_path) / 1e9\n",
    "# Estimate size for full dataset\n",
    "full_size = output_size * len(model_ids) / 100\n",
    "print(f\"Output size: {output_size:.3f} GB\")\n",
    "print(f\"Estimated full size: {full_size:.3f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a file to confirm that the dataset was created successfully\n",
    "with open('/ibex/project/c2273/PADS/3DCoMPaT_occ/dataset__debug.txt', 'w') as f:\n",
    "    f.write(\"Dataset created successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
