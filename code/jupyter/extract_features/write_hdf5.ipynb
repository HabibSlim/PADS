{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ibex/user/slimhy/PADS/code\n"
     ]
    }
   ],
   "source": [
    "%cd /ibex/user/slimhy/PADS/code\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Global constants\n",
    "PARTS_DIR = \"/ibex/project/c2273/PADS/3DCoMPaT_occ/parts\"\n",
    "SAMPLES_DIR = \"/ibex/project/c2273/PADS/3DCoMPaT_occ/samples\"\n",
    "MAX_PART_DROP = 16\n",
    "\n",
    "N_SAMPLING_FNS = 3 # Number of sampling functions\n",
    "N_POINTS = 131072  # Number of points in all point clouds\n",
    "\n",
    "RATIO_SUB_POINTS = 1/4.  # Number of points in each part point cloud\n",
    "N_SUB_POINTS = int(N_POINTS * RATIO_SUB_POINTS)\n",
    "\n",
    "RATIO_SUB_QUERIES = 1/2. # Number of points in each query point cloud\n",
    "N_SUB_QUERIES = int(N_POINTS * RATIO_SUB_QUERIES)\n",
    "\n",
    "\n",
    "def load_part_bbs(model_id):\n",
    "    \"\"\"\n",
    "    Load part bounding boxes for a given model ID.\n",
    "    \n",
    "    Args:\n",
    "        model_id (str): The model identifier\n",
    "    \n",
    "    Returns:\n",
    "        dict: Mapping of part keys to vertex arrays [8, 3] representing the 8 corners of each box\n",
    "    \"\"\"\n",
    "    bb_file = os.path.join(SAMPLES_DIR, f\"{model_id}_orig_0_bbs.pkl\")\n",
    "    bb_data = np.load(bb_file, allow_pickle=True)\n",
    "    bb_data = {k:v for k,v in bb_data}\n",
    "    bb_data = {k:np.array(v.vertices).astype(np.float16) for k,v in bb_data.items()}\n",
    "    return bb_data\n",
    "\n",
    "\n",
    "def load_occs(model_id, part_drop_id=None):\n",
    "    \"\"\"\n",
    "    Load queries and occupancies for a given model ID.\n",
    "    \n",
    "    Args:\n",
    "        model_id (str): The model identifier\n",
    "        part_drop_id (int, optional): The part drop identifier. If None, loads the original \n",
    "                                    version with no parts dropped.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (queries, occupancies)\n",
    "            - queries: array of shape [5, N_SUB_POINTS, 3]\n",
    "            - occupancies: array of shape [5, N_SUB_POINTS]\n",
    "    \"\"\"\n",
    "    if part_drop_id is None:\n",
    "        occs = os.path.join(SAMPLES_DIR, f\"{model_id}_orig_0_occs.npy\")\n",
    "        queries = os.path.join(SAMPLES_DIR, f\"{model_id}_orig_0_points.npy\")\n",
    "    else:\n",
    "        occs = os.path.join(SAMPLES_DIR, f\"{model_id}_part_drop_{part_drop_id}_occs.npy\")\n",
    "        queries = os.path.join(SAMPLES_DIR, f\"{model_id}_part_drop_{part_drop_id}_points.npy\")\n",
    "    \n",
    "    occs = np.load(occs).astype(np.float16)\n",
    "    queries = np.load(queries).astype(np.float16)\n",
    "    \n",
    "    return queries, occs.reshape(*queries.shape[:2], -1).squeeze()\n",
    "\n",
    "\n",
    "def load_part_surf_points(model_id):\n",
    "    \"\"\"\n",
    "    Load part surface points for a given model ID.\n",
    "    \n",
    "    Args:\n",
    "        model_id (str): The model identifier\n",
    "    \n",
    "    Returns:\n",
    "        dict: Mapping of part keys to point arrays [N_SUB_POINTS, 3]\n",
    "    \"\"\"\n",
    "    part_file = os.path.join(PARTS_DIR, f\"{model_id}.npy\")\n",
    "    part_data = np.load(part_file, allow_pickle=True).item()\n",
    "    part_data = {k:np.array(v).squeeze().astype(np.float16) for k,v in part_data.items()}\n",
    "    return part_data\n",
    "\n",
    "\n",
    "def get_dropped_part_key(model_id, part_drop_id):\n",
    "    \"\"\"\n",
    "    Get the key for the dropped part by comparing original and dropped configurations.\n",
    "    \n",
    "    Args:\n",
    "        model_id (str): The model identifier\n",
    "        part_drop_id (int): The part drop identifier\n",
    "    \n",
    "    Returns:\n",
    "        str: Key of the dropped part\n",
    "    \"\"\"\n",
    "    bb_file = os.path.join(SAMPLES_DIR, f\"{model_id}_part_drop_{part_drop_id}_bbs.pkl\")\n",
    "    bb_data = np.load(bb_file, allow_pickle=True)\n",
    "    bb_data = {k:v for k,v in bb_data}\n",
    "    \n",
    "    bb_file_orig = os.path.join(SAMPLES_DIR, f\"{model_id}_orig_0_bbs.pkl\")\n",
    "    bb_data_orig = np.load(bb_file_orig, allow_pickle=True)\n",
    "    bb_data_orig = {k:v for k,v in bb_data_orig}\n",
    "    \n",
    "    dropped_part_key = set(bb_data_orig.keys()) - set(bb_data.keys())\n",
    "    assert len(dropped_part_key) == 1, f\"Expected exactly one dropped part for {model_id}, drop {part_drop_id}\"\n",
    "    return list(dropped_part_key)[0]\n",
    "\n",
    "\n",
    "def subsample_points(p, labels=None):\n",
    "    \"\"\"\n",
    "    Subsample points using random sampling with a fixed ratio.\n",
    "    Ensures balanced labels (equal 1s and 0s) when labels are provided.\n",
    "    \n",
    "    Args:\n",
    "        p: Points array of shape [N, 3]\n",
    "        labels: Optional labels array of shape [N]. If provided, indicates query point processing.\n",
    "        \n",
    "    Returns:\n",
    "        Subsampled points (and labels if provided)\n",
    "    \"\"\"\n",
    "    p = torch.as_tensor(p)\n",
    "    \n",
    "    if labels is not None:\n",
    "        # Convert labels to tensor if needed\n",
    "        labels = torch.as_tensor(labels)\n",
    "        \n",
    "        # Get indices for each class\n",
    "        idx_0 = torch.where(labels == 0)[0]\n",
    "        idx_1 = torch.where(labels == 1)[0]\n",
    "        \n",
    "        assert len(idx_0) == len(idx_1) == N_POINTS // 2, \\\n",
    "            f\"Invalid label distribution: {len(idx_0)} 0s, {len(idx_1)} 1s\"\n",
    "\n",
    "        # Sample equal numbers from each class\n",
    "        n_per_class = N_SUB_QUERIES // 2\n",
    "        idx_0 = idx_0[torch.randperm(len(idx_0))[:n_per_class]]\n",
    "        idx_1 = idx_1[torch.randperm(len(idx_1))[:n_per_class]]\n",
    "        \n",
    "        # Combine and shuffle indices\n",
    "        idx = torch.cat([idx_0, idx_1])\n",
    "        idx = idx[torch.randperm(len(idx))]\n",
    "        \n",
    "        p = p[idx]\n",
    "        labels = labels[idx]\n",
    "        \n",
    "        assert len(p) == N_SUB_QUERIES, f\"Invalid subsampling length: {len(p)}\"\n",
    "        \n",
    "        return p.numpy(), labels.numpy()\n",
    "    else:\n",
    "        # Part points - just random sampling\n",
    "        idx = torch.randperm(len(p))[:N_SUB_POINTS]\n",
    "        assert len(idx) == N_SUB_POINTS, f\"Invalid subsampling length: {len(idx)}\"\n",
    "        \n",
    "        return p[idx].numpy()\n",
    "\n",
    "    \n",
    "def create_stacked_matrices(model_ids):\n",
    "    \"\"\"\n",
    "    Create stacked matrices for part points, bounding boxes, query points, and occupancies.\n",
    "    \n",
    "    Args:\n",
    "        model_ids (list): List of model identifiers to process\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing all stacked matrices and metadata arrays\n",
    "    \"\"\"\n",
    "    # First pass: determine total sizes\n",
    "    total_parts = 0\n",
    "    total_query_configs = 0\n",
    "    part_slices = []\n",
    "    \n",
    "    print(\"Calculating matrix dimensions...\")\n",
    "    for model_id in tqdm(model_ids):\n",
    "        part_data = load_part_surf_points(model_id)\n",
    "        n_parts = len(part_data)\n",
    "        part_slices.append(total_parts)\n",
    "        total_parts += n_parts\n",
    "        total_query_configs += (MAX_PART_DROP + 1)  # Include original configuration\n",
    "    \n",
    "    part_slices.append(total_parts)\n",
    "    part_slices = np.array(part_slices, dtype=np.int32)\n",
    "    \n",
    "    # Initialize matrices with proper data types\n",
    "    part_points_matrix = np.zeros((total_parts, N_SUB_POINTS, 3), dtype=np.float16)\n",
    "    part_bbs_matrix = np.zeros((total_parts, 8, 3), dtype=np.float16)\n",
    "    query_points_matrix = np.zeros((total_query_configs, N_SAMPLING_FNS, N_SUB_QUERIES, 3), dtype=np.float16)\n",
    "    query_labels_matrix = np.zeros((total_query_configs, N_SAMPLING_FNS, N_SUB_QUERIES), dtype=np.float16)\n",
    "    part_drops = np.full((len(model_ids), MAX_PART_DROP), -1, dtype=np.int32)\n",
    "    model_ids_array = np.array(model_ids, dtype='S')\n",
    "    \n",
    "    print(\"Filling matrices...\")\n",
    "    part_idx = 0\n",
    "    query_config_idx = 0\n",
    "    \n",
    "    for model_idx, model_id in enumerate(tqdm(model_ids)):\n",
    "        # Load and validate data\n",
    "        part_points = load_part_surf_points(model_id)\n",
    "        part_bbs = load_part_bbs(model_id)\n",
    "        \n",
    "        # Validate data consistency\n",
    "        assert set(part_points.keys()) == set(part_bbs.keys()), \\\n",
    "            f\"Mismatch in part keys for model {model_id}\"\n",
    "        \n",
    "        # Fill part matrices\n",
    "        for part_key in sorted(part_points.keys()):\n",
    "            points = part_points[part_key]\n",
    "            bbs = part_bbs[part_key]\n",
    "            \n",
    "            # Validate shapes\n",
    "            assert points.shape == (N_POINTS, 3), \\\n",
    "                f\"Invalid point shape for model {model_id}, part {part_key}: {points.shape}\"\n",
    "            # And update the validation in the filling section:\n",
    "            assert bbs.shape == (8, 3), \\\n",
    "                f\"Invalid BB shape for model {model_id}, part {part_key}: {bbs.shape}\"\n",
    "                \n",
    "            part_points_matrix[part_idx] = subsample_points(points)\n",
    "            part_bbs_matrix[part_idx] = bbs\n",
    "            part_idx += 1\n",
    "        \n",
    "        # Fill query matrices - first the original configuration\n",
    "        queries_orig, occs_orig = load_occs(model_id, part_drop_id=None)\n",
    "        assert queries_orig.shape == (N_SAMPLING_FNS, N_POINTS, 3), \\\n",
    "            f\"Invalid query shape for original config of model {model_id}: {queries_orig.shape}\"\n",
    "        assert occs_orig.shape == (N_SAMPLING_FNS, N_POINTS), \\\n",
    "            f\"Invalid occupancy shape for original config of model {model_id}: {occs_orig.shape}\"\n",
    "        \n",
    "        for i in range(N_SAMPLING_FNS):\n",
    "            try:\n",
    "                queries_orig_sub, occs_orig_sub = subsample_points(queries_orig[i], occs_orig[i])\n",
    "                query_points_matrix[query_config_idx][i] = queries_orig_sub\n",
    "                query_labels_matrix[query_config_idx][i] = occs_orig_sub\n",
    "            except:\n",
    "                print(f\"Error processing model {model_id}, original configuration\")\n",
    "\n",
    "        query_config_idx += 1\n",
    "        \n",
    "        # Then process part drop configurations\n",
    "        for part_drop_id in range(MAX_PART_DROP):\n",
    "            try:\n",
    "                # Record dropped part\n",
    "                dropped_key = get_dropped_part_key(model_id, part_drop_id)\n",
    "                dropped_idx = list(sorted(part_points.keys())).index(dropped_key)\n",
    "                part_drops[model_idx, part_drop_id] = dropped_idx\n",
    "                \n",
    "                # Load and validate query data\n",
    "                queries, occs = load_occs(model_id, part_drop_id)\n",
    "                assert queries.shape == (N_SAMPLING_FNS, N_POINTS, 3), \\\n",
    "                    f\"Invalid query shape for model {model_id}, drop {part_drop_id}: {queries.shape}\"\n",
    "                assert occs.shape == (N_SAMPLING_FNS, N_POINTS), \\\n",
    "                    f\"Invalid occupancy shape for model {model_id}, drop {part_drop_id}: {occs.shape}\"\n",
    "                \n",
    "                for i in range(N_SAMPLING_FNS):\n",
    "                    queries_sub, occs_sub = subsample_points(queries[i], occs[i])\n",
    "                    query_points_matrix[query_config_idx][i] = queries_sub\n",
    "                    query_labels_matrix[query_config_idx][i] = occs_sub\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Failed to process part drop {part_drop_id} \"\n",
    "                      f\"for model {model_id}: {str(e)}\")\n",
    "                continue\n",
    "            \n",
    "            query_config_idx += 1\n",
    "    \n",
    "    # Verify final counts match expected values\n",
    "    assert part_idx == total_parts, \\\n",
    "        f\"Mismatch in part count: got {part_idx}, expected {total_parts}\"\n",
    "    assert query_config_idx <= total_query_configs, \\\n",
    "        f\"Mismatch in query config count: got {query_config_idx}, expected {total_query_configs}\"\n",
    "    \n",
    "    return {\n",
    "        'model_ids': model_ids_array,\n",
    "        'part_slices': part_slices,\n",
    "        'part_drops': part_drops,\n",
    "        'part_points_matrix': part_points_matrix,\n",
    "        'part_bbs_matrix': part_bbs_matrix,\n",
    "        'query_points_matrix': query_points_matrix[:query_config_idx],\n",
    "        'query_labels_matrix': query_labels_matrix[:query_config_idx]\n",
    "    }\n",
    "\n",
    "def save_to_hdf5(matrices, output_path):\n",
    "    \"\"\"\n",
    "    Save the stacked matrices to a single HDF5 file.\n",
    "    \n",
    "    Args:\n",
    "        matrices (dict): Dictionary containing matrices to save\n",
    "        output_path (str): Path where to save the HDF5 file\n",
    "    \"\"\"\n",
    "    print(\"Saving matrices to HDF5...\")\n",
    "    with h5py.File(output_path, 'w') as f:\n",
    "        for key, matrix in matrices.items():\n",
    "            f.create_dataset(key, data=matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_processed_models(out_path):\n",
    "    \"\"\"\n",
    "    Initialize the set of processed model IDs by including only model IDs\n",
    "    that appear exactly 68 times in the file listing.\n",
    "\n",
    "    Args:\n",
    "        out_path (str): Path to the directory containing the files\n",
    "\n",
    "    Global Effects:\n",
    "        Updates the PROCESSED_MODELS global set with qualifying model IDs\n",
    "    \"\"\"\n",
    "    all_files = os.listdir(out_path)\n",
    "\n",
    "    # Count occurrences of each model ID\n",
    "    model_counts = {}\n",
    "    for filename in all_files:\n",
    "        if len(filename) >= 6:  # Ensure filename is long enough\n",
    "            model_id = filename[:6]\n",
    "            model_counts[model_id] = model_counts.get(model_id, 0) + 1\n",
    "\n",
    "    # Add only model IDs that appear exactly 68 times\n",
    "    return {\n",
    "        model_id for model_id, count in model_counts.items() if count == 68\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ids = initialize_processed_models(SAMPLES_DIR)\n",
    "\n",
    "error_ids = [\"25_41d\", \"10_01d\"]\n",
    "\n",
    "# Remove models with errors\n",
    "model_ids = sorted(list(set(model_ids) - set(error_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating matrix dimensions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 22.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling matrices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:09<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving matrices to HDF5...\n",
      "Dataset created successfully at /ibex/project/c2273/PADS/3DCoMPaT_occ/dataset__debug.h5\n",
      "Output size: 0.291 GB\n",
      "Estimated full size: 280.982 GB\n"
     ]
    }
   ],
   "source": [
    "DEBUG_MODE = True\n",
    "\n",
    "# DEBUG HDF5 CREATION\n",
    "if DEBUG_MODE:\n",
    "    # Create stacked matrices\n",
    "    n_model_samples = 10\n",
    "    shuffled_ids = np.random.permutation(model_ids)\n",
    "    matrices = create_stacked_matrices(list(shuffled_ids)[:n_model_samples])\n",
    "\n",
    "    # Save to HDF5\n",
    "    output_path = '/ibex/project/c2273/PADS/3DCoMPaT_occ/dataset__debug.h5'\n",
    "    save_to_hdf5(matrices, output_path)\n",
    "    print(f\"Dataset created successfully at {output_path}\")\n",
    "\n",
    "    # Measure output size in GB\n",
    "    output_size = os.path.getsize(output_path) / 1e9\n",
    "    # Estimate size for full dataset\n",
    "    full_size = output_size * len(model_ids) / n_model_samples\n",
    "    print(f\"Output size: {output_size:.3f} GB\")\n",
    "    print(f\"Estimated full size: {full_size:.3f} GB\")\n",
    "\n",
    "    # Write a file to confirm that the dataset was created successfully\n",
    "    with open('/ibex/project/c2273/PADS/3DCoMPaT_occ/dataset__debug.txt', 'w') as f:\n",
    "        f.write(\"Dataset created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DEBUG_MODE:\n",
    "    # Create stacked matrices\n",
    "    matrices = create_stacked_matrices(list(model_ids))\n",
    "    \n",
    "    # Save to HDF5\n",
    "    output_path = '/ibex/project/c2273/PADS/3DCoMPaT_occ/dataset__full.h5'\n",
    "    save_to_hdf5(matrices, output_path)\n",
    "    print(f\"Dataset created successfully at {output_path}\")\n",
    "    \n",
    "    # Write a file to confirm that the dataset was created successfully\n",
    "    with open('/ibex/project/c2273/PADS/3DCoMPaT_occ/dataset__full.txt', 'w') as f:\n",
    "        f.write(\"Dataset created successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
