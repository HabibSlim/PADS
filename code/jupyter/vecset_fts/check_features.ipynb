{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d766df8a-c11c-4f2a-bb8c-a6b6abcd3120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ibex/user/slimhy/Shape2VecSet/code\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slimhy/conda/envs/shape2vecset/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /ibex/user/slimhy/Shape2VecSet/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8baaacac-613d-4166-8469-4e64a7330d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Some experiments with embeddings proximity when sampling pointclouds.\n",
    "\"\"\"\n",
    "import argparse\n",
    "import h5py\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer, CLIPTextModel, BertTokenizer, BertModel\n",
    "\n",
    "import util.misc as misc\n",
    "import models.autoencoders as ae_mods\n",
    "from engine_node2node import get_text_embeddings\n",
    "from util.datasets import build_shape_surface_occupancy_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d12d84d6-1863-44ad-924d-5e3dd3e4ec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser(\"Extracting Features\", add_help=False)\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        default=32,\n",
    "        type=int,\n",
    "        help=\"Batch size per GPU (effective batch size is batch_size * accum_iter * # gpus\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--text_model_name\",\n",
    "        type=str,\n",
    "        help=\"Text model name to use\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ae\",\n",
    "        type=str,\n",
    "        metavar=\"MODEL\",\n",
    "        help=\"Name of autoencoder\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ae-latent-dim\",\n",
    "        type=int,\n",
    "        default=512*8,\n",
    "        help=\"AE latent dimension\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ae_pth\",\n",
    "        required=True,\n",
    "        help=\"Autoencoder checkpoint\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--point_cloud_size\",\n",
    "        default=2048,\n",
    "        type=int,\n",
    "        help=\"input size\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--fetch_keys\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_embeds\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--intensity_loss\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Contrastive edit intensity loss using ground-truth labels.\",\n",
    "    )\n",
    "\n",
    "    # Dataset parameters\n",
    "    parser.add_argument(\n",
    "        \"--dataset\",\n",
    "        type=str,\n",
    "        choices=[\"graphedits\"],\n",
    "        help=\"dataset name\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data_path\",\n",
    "        type=str,\n",
    "        help=\"dataset path\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data_type\",\n",
    "        type=str,\n",
    "        help=\"dataset type\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_edge_level\",\n",
    "        default=None,\n",
    "        type=int,\n",
    "        help=\"maximum edge level to use\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--device\", default=\"cuda\", help=\"device to use for training / testing\"\n",
    "    )\n",
    "    parser.add_argument(\"--seed\", default=0, type=int)\n",
    "    parser.add_argument(\"--num_workers\", default=60, type=int)\n",
    "    parser.add_argument(\n",
    "        \"--pin_mem\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.\",\n",
    "    )\n",
    "\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d3da4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dummy arg string to debug the parser\n",
    "call_string = \"\"\"--ae_pth /ibex/user/slimhy/Shape2VecSet/output/graph_edit/ae/ae_m512.pth \\\n",
    "    --ae kl_d512_m512_l8 \\\n",
    "    --ae-latent-dim 4096 \\\n",
    "    --text_model_name bert-base-uncased \\\n",
    "    --dataset graphedits \\\n",
    "    --data_path /ibex/user/slimhy/ShapeWalk_RND/ \\\n",
    "    --data_type release \\\n",
    "    --batch_size 32 \\\n",
    "    --num_workers 8 \\\n",
    "    --device cuda \\\n",
    "    --fetch_keys \\\n",
    "    --seed 0\"\"\"\n",
    "\n",
    "# Parse the arguments\n",
    "args = get_args_parser()\n",
    "args = args.parse_args(call_string.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "116cb4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slimhy/conda/envs/shape2vecset/lib/python3.10/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading autoencoder /ibex/user/slimhy/Shape2VecSet/output/graph_edit/ae/ae_m512.pth\n"
     ]
    }
   ],
   "source": [
    "# --------------------\n",
    "args.use_clip = \"clip\" in args.text_model_name\n",
    "device = torch.device(args.device)\n",
    "\n",
    "# Fix the seed for reproducibility\n",
    "seed = args.seed + misc.get_rank()\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "args.fetch_keys = True\n",
    "dataset_train = build_shape_surface_occupancy_dataset(\"train\", args=args)\n",
    "dataset_val = build_shape_surface_occupancy_dataset(\"val\", args=args)\n",
    "\n",
    "# Create data loaders\n",
    "data_loader_train = torch.utils.data.DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "# --------------------\n",
    "\n",
    "# Instantiate autoencoder\n",
    "ae = ae_mods.__dict__[args.ae]()\n",
    "ae.eval()\n",
    "print(\"Loading autoencoder %s\" % args.ae_pth)\n",
    "ae.load_state_dict(torch.load(args.ae_pth, map_location=\"cpu\")[\"model\"])\n",
    "ae.to(device)\n",
    "\n",
    "# Initialize text CLIP model\n",
    "if args.use_clip:\n",
    "    # Instantiate tokenizer + CLIP model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.text_model_name)\n",
    "    text_model = CLIPTextModel.from_pretrained(args.text_model_name).to(device)\n",
    "else:\n",
    "    # Instantiate BERT model and create linear projection layer\n",
    "    tokenizer = BertTokenizer.from_pretrained(args.text_model_name)\n",
    "    text_model = BertModel.from_pretrained(args.text_model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5023b58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_embeddings(args, data_loader):\n",
    "    text_latent_dim = 512 if args.use_clip else 768\n",
    "\n",
    "    # Create stacked numpy arrays to store embeddings\n",
    "    B = args.batch_size\n",
    "    n_batches = len(data_loader)\n",
    "    n_entries = n_batches * B\n",
    "    embeds_xa   = np.zeros((n_entries, args.ae_latent_dim))\n",
    "    embeds_xb   = np.zeros((n_entries, args.ae_latent_dim))\n",
    "    embeds_text = np.zeros((n_entries, text_latent_dim))\n",
    "    all_keys = [\"_\" for _ in range(n_entries)]\n",
    "\n",
    "    # Iterate over the dataset and extract text embeddings\n",
    "    for k, (edit_keys, nodes_a, nodes_b, prompts_ab) in enumerate(tqdm(data_loader)):\n",
    "        nodes_a = nodes_a.to(device, non_blocking=True)\n",
    "        nodes_b = nodes_b.to(device, non_blocking=True)\n",
    "        embeds_ab = get_text_embeddings(text_model=text_model,\n",
    "                                        tokenizer=tokenizer,\n",
    "                                        texts=prompts_ab,\n",
    "                                        device=device)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=False):\n",
    "            with torch.no_grad():\n",
    "                _, x_a = ae.encode(nodes_a)\n",
    "                _, x_b = ae.encode(nodes_b)\n",
    "\n",
    "        # Move batch to CPU and convert to numpy\n",
    "        embeds_ab = embeds_ab.cpu().numpy()\n",
    "        x_a = x_a.cpu().numpy()\n",
    "        x_b = x_b.cpu().numpy()\n",
    "\n",
    "        # Store to stacked arrays\n",
    "        embeds_xa[k*B:(k+1)*B] = x_a.reshape(B, -1)\n",
    "        embeds_xb[k*B:(k+1)*B] = x_b.reshape(B, -1)\n",
    "        embeds_text[k*B:(k+1)*B] = embeds_ab.reshape(B, -1)\n",
    "        all_keys[k*B:(k+1)*B] = edit_keys\n",
    "\n",
    "        #if k==2: break\n",
    "    \n",
    "    return all_keys, embeds_xa, embeds_xb, embeds_text\n",
    "\n",
    "\n",
    "def extract_embeddings(split, data_loader):\n",
    "    \"\"\"\n",
    "    Extract embeddings and remap to shape/edit keys.\n",
    "    \"\"\"\n",
    "    # Extract embeddings\n",
    "    all_keys, embeds_xa, embeds_xb, embeds_text = get_split_embeddings(args, data_loader)\n",
    "\n",
    "    edit_keys_sp = [k.split('_') for k in all_keys]\n",
    "    keys_node_a = [k[0] for k in edit_keys_sp]\n",
    "    keys_node_b = [k[1] for k in edit_keys_sp]\n",
    "\n",
    "    # Map node keys to indices\n",
    "    node_a_to_idx = {k: i for i, k in enumerate(keys_node_a)}\n",
    "    node_b_to_idx = {k: i for i, k in enumerate(keys_node_b)}\n",
    "    all_nodes = list(set(keys_node_a + keys_node_b))\n",
    "    print(\"all_nodes=\", len(all_nodes))\n",
    "\n",
    "    # Build a matrix with all the embeddings\n",
    "    # using the indices\n",
    "    shape_embeds = np.zeros((len(all_nodes), args.ae_latent_dim))\n",
    "    k = 0\n",
    "    key_to_shape_embeds = {}\n",
    "    for node_key in node_a_to_idx:\n",
    "        idx = node_a_to_idx[node_key]\n",
    "        shape_embeds[k] = embeds_xa[idx]\n",
    "        key_to_shape_embeds[node_key] = k\n",
    "        k += 1\n",
    "\n",
    "    # Remove all nodes already added from node_b\n",
    "    for node_key in (node_b_to_idx.keys() - node_a_to_idx.keys()):\n",
    "        idx = node_b_to_idx[node_key]\n",
    "        shape_embeds[k] = embeds_xb[idx]\n",
    "        key_to_shape_embeds[node_key] = k\n",
    "        k += 1\n",
    "\n",
    "    # Double check that everything is correct\n",
    "    # Iterate on edit_keys\n",
    "    print(\"Checking shape embeddings...\")\n",
    "    intersec_nodes = node_a_to_idx.keys() & node_b_to_idx.keys()\n",
    "    for node_a, node_b in edit_keys_sp:\n",
    "        assert node_a in key_to_shape_embeds\n",
    "        assert node_b in key_to_shape_embeds\n",
    "    \n",
    "        # Check that embeddings are correct\n",
    "        idx_a = key_to_shape_embeds[node_a]\n",
    "        idx_b = key_to_shape_embeds[node_b]\n",
    "       \n",
    "        if node_a not in intersec_nodes:\n",
    "            assert np.allclose(shape_embeds[idx_a], embeds_xa[node_a_to_idx[node_a]])\n",
    "        if node_b not in intersec_nodes:\n",
    "            assert np.allclose(shape_embeds[idx_b], embeds_xb[node_b_to_idx[node_b]])\n",
    "    print(\"Done!\")\n",
    "\n",
    "    key_pair_to_text_embeds = {key_pair : k for k, key_pair in enumerate(all_keys)}\n",
    "\n",
    "    # Double check that everything is correct\n",
    "    # Iterate on edit_keys\n",
    "    print(\"Checking text embeddings...\")\n",
    "    for key_pair in all_keys:\n",
    "        assert key_pair in key_pair_to_text_embeds\n",
    "\n",
    "    print(\"Done!\")\n",
    "\n",
    "    return shape_embeds, key_to_shape_embeds, embeds_text, key_pair_to_text_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a8cf139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hdf5(args, split, shape_embeds, key_to_shape_embeds, embeds_text, key_pair_to_text_embeds):\n",
    "    \"\"\"\n",
    "    Create HDF5 file with the embeddings\n",
    "    \"\"\"\n",
    "    # Create HDF5 file\n",
    "    hdf5_path = os.path.join(args.data_path, args.data_type, \"embeddings_%s.hdf5\" % split)\n",
    "    # If exists: delete\n",
    "    if os.path.exists(hdf5_path):\n",
    "        os.remove(hdf5_path)\n",
    "        print(\"Deleted existing HDF5 file %s\" % hdf5_path)\n",
    "    print(\"Creating HDF5 file %s\" % hdf5_path)\n",
    "    f = h5py.File(hdf5_path, \"w\")\n",
    "\n",
    "    # Create datasets\n",
    "    f.create_dataset(\"shape_embeds\", data=shape_embeds)\n",
    "    f.create_dataset(\"text_embeds\", data=embeds_text)\n",
    "    f.create_dataset(\"key_to_shape_embeds\",\n",
    "                     data=json.dumps(key_to_shape_embeds),\n",
    "                     shape=(1,),\n",
    "                     dtype=h5py.string_dtype(encoding=\"utf-8\"))\n",
    "    f.create_dataset(\"key_pair_to_text_embeds\",\n",
    "                     data=json.dumps(key_pair_to_text_embeds),\n",
    "                     shape=(1,),\n",
    "                     dtype=h5py.string_dtype(encoding=\"utf-8\"))\n",
    "    f.close()\n",
    "\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0ecfb9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a00a8551d3f34a92b10f05cd02ea9f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_latent_dim = 512 if args.use_clip else 768\n",
    "\n",
    "# Create stacked numpy arrays to store embeddings\n",
    "B = args.batch_size\n",
    "n_batches = len(data_loader_val)\n",
    "n_entries = n_batches * B\n",
    "embeds_xa   = np.zeros((n_entries, args.ae_latent_dim))\n",
    "embeds_xb   = np.zeros((n_entries, args.ae_latent_dim))\n",
    "embeds_text = np.zeros((n_entries, text_latent_dim))\n",
    "all_keys = [\"_\" for _ in range(n_entries)]\n",
    "\n",
    "# Iterate over the dataset and extract text embeddings\n",
    "for k, (edit_keys, nodes_a, nodes_b, prompts_ab) in enumerate(tqdm(data_loader_val)):\n",
    "    nodes_a = nodes_a.to(device, non_blocking=True)\n",
    "    nodes_b = nodes_b.to(device, non_blocking=True)\n",
    "\n",
    "    with torch.cuda.amp.autocast(enabled=False):\n",
    "        with torch.no_grad():\n",
    "            _, x_a = ae.encode(nodes_a)\n",
    "            _, x_b = ae.encode(nodes_b)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e30c756-203e-498e-a02a-85d067826a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def reorder_pointclouds(nodes_a, nodes_b):\n",
    "    \"\"\"\n",
    "    Reorder pointclouds to minimize the distance between\n",
    "    corresponding points.\n",
    "    \"\"\"\n",
    "    # Compute pairwise distances\n",
    "    dists = torch.cdist(nodes_a, nodes_b, p=2)\n",
    "\n",
    "    # Compute optimal assignment\n",
    "    row_ind, col_ind = linear_sum_assignment(dists.cpu().numpy())\n",
    "\n",
    "    #col_ind = np.random.permutation(len(nodes_a))\n",
    "\n",
    "    # Reorder nodes_b\n",
    "    nodes_b = nodes_b.clone()\n",
    "    nodes_b = nodes_b[col_ind]\n",
    "\n",
    "    return nodes_a, nodes_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5aab424-aa5b-4ead-87a5-d61ca881b39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pwise_dist(pc_a, pc_b):\n",
    "    dist_total = 0.0\n",
    "    for i in range(pc_a.shape[0]):\n",
    "        dist = (pc_a[i] - pc_b[i])**2\n",
    "        dist_total += dist.mean()\n",
    "    return dist_total/pc_a.shape[0]\n",
    "\n",
    "def embed_dist(pc_a, pc_b):\n",
    "    _, x_a = ae.encode(nodes_a)\n",
    "    _, x_b = ae.encode(nodes_b)\n",
    "    return (torch.abs(x_b - x_a)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4107cc0c-9bec-481c-aa37-bdcbec92fad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2125, device='cuda:0'),\n",
       " tensor(0.9245, device='cuda:0', grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute pairwise distances\n",
    "p_a, p_b = nodes_a[0].clone(), nodes_b[0].clone()\n",
    "pwise_dist(p_a, p_b), embed_dist(p_a, p_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85fad22d-dcc6-413a-ad72-4a818bfb589d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0013, device='cuda:0'),\n",
       " tensor(0.9240, device='cuda:0', grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute pairwise distances\n",
    "t_a, t_b = reorder_pointclouds(p_a, p_b)\n",
    "pwise_dist(t_a, t_b), embed_dist(t_a, t_b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shape2vecset",
   "language": "python",
   "name": "shape2vecset"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
