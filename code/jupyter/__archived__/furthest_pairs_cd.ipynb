{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8baaacac-613d-4166-8469-4e64a7330d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find furthest pairs of shapes in the dataset using CD.\n",
    "\"\"\"\n",
    "import argparse\n",
    "import datetime\n",
    "import h5py\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, CLIPTextModel, BertTokenizer, BertModel\n",
    "\n",
    "import util.misc as misc\n",
    "from engine_node2node import get_text_embeddings\n",
    "from util.datasets import build_shape_surface_occupancy_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d12d84d6-1863-44ad-924d-5e3dd3e4ec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser(\"Latent Diffusion\", add_help=False)\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        default=32,\n",
    "        type=int,\n",
    "        help=\"Batch size per GPU (effective batch size is batch_size * accum_iter * # gpus\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--text_model_name\",\n",
    "        type=str,\n",
    "        help=\"Text model name to use\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ae\",\n",
    "        type=str,\n",
    "        metavar=\"MODEL\",\n",
    "        help=\"Name of autoencoder\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ae-latent-dim\",\n",
    "        type=int,\n",
    "        default=512*8,\n",
    "        help=\"AE latent dimension\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ae_pth\",\n",
    "        required=True,\n",
    "        help=\"Autoencoder checkpoint\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--point_cloud_size\",\n",
    "        default=2048,\n",
    "        type=int,\n",
    "        help=\"input size\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--fetch_keys\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_embeds\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ft_bert\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--intensity_loss\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Contrastive edit intensity loss using ground-truth labels.\",\n",
    "    )\n",
    "\n",
    "    # Dataset parameters\n",
    "    parser.add_argument(\n",
    "        \"--dataset\",\n",
    "        type=str,\n",
    "        choices=[\"graphedits\"],\n",
    "        help=\"dataset name\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data_path\",\n",
    "        type=str,\n",
    "        help=\"dataset path\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data_type\",\n",
    "        type=str,\n",
    "        help=\"dataset type\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_edge_level\",\n",
    "        default=None,\n",
    "        type=int,\n",
    "        help=\"maximum edge level to use\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--device\", default=\"cuda\", help=\"device to use for training / testing\"\n",
    "    )\n",
    "    parser.add_argument(\"--seed\", default=0, type=int)\n",
    "    parser.add_argument(\"--num_workers\", default=60, type=int)\n",
    "    parser.add_argument(\n",
    "        \"--pin_mem\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.\",\n",
    "    )\n",
    "\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d3da4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dummy arg string to debug the parser\n",
    "call_string = \"\"\"--ae_pth /ibex/user/slimhy/Shape2VecSet/output/graph_edit/ae/ae_m512.pth \\\n",
    "    --ae kl_d512_m512_l8 \\\n",
    "    --ae-latent-dim 4096 \\\n",
    "    --text_model_name bert-base-uncased \\\n",
    "    --dataset graphedits \\\n",
    "    --data_path /ibex/user/slimhy/ShapeWalk/ \\\n",
    "    --data_type release \\\n",
    "    --batch_size 32 \\\n",
    "    --num_workers 8 \\\n",
    "    --device cuda \\\n",
    "    --fetch_keys \\\n",
    "    --seed 0\"\"\"\n",
    "\n",
    "# Parse the arguments\n",
    "args = get_args_parser()\n",
    "args = args.parse_args(call_string.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "116cb4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slimhy/conda/envs/shape2vecset/lib/python3.10/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "# --------------------\n",
    "args.use_clip = \"clip\" in args.text_model_name\n",
    "device = torch.device(args.device)\n",
    "\n",
    "# Fix the seed for reproducibility\n",
    "seed = args.seed + misc.get_rank()\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "args.fetch_keys = True\n",
    "dataset_train = build_shape_surface_occupancy_dataset(\"train\", args=args)\n",
    "dataset_val = build_shape_surface_occupancy_dataset(\"val\", args=args)\n",
    "\n",
    "# Create data loaders\n",
    "data_loader_train = torch.utils.data.DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "# --------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5023b58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch, chamfer3D.dist_chamfer_3D\n",
    "from chamfer3D.fscore import fscore\n",
    "\n",
    "closest_pairs = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Iterate over train set\n",
    "    for edit_key, nodes_a, nodes_b, embeds_ab in tqdm(data_loader_train):\n",
    "        # Find the closest pair in embeddings from a to b\n",
    "        nodes_a = nodes_a.to(device)\n",
    "        nodes_b = nodes_b.to(device)\n",
    "        embeds_ab = embeds_ab.to(device)\n",
    "    \n",
    "        # Compute pairwise chamfer distances between nodes a and nodes b\n",
    "        dist_mat = torch.norm(nodes_a - nodes_b, dim=1)\n",
    "        \n",
    "        # Iterate over the batch\n",
    "        # map edit_key to distance in closest_pairs dict\n",
    "        for i in range(len(edit_key)):\n",
    "            closest_pairs[edit_key[i]] = dist_mat[i].item()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bf406c-2d1f-46cc-8369-1b32a60399c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shape2vecset",
   "language": "python",
   "name": "shape2vecset"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
