{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Neural listener evaluation of the models.\n",
    "\"\"\"\n",
    "import argparse\n",
    "import datetime\n",
    "import h5py\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, CLIPTextModel, BertTokenizer, BertModel\n",
    "\n",
    "import util.misc as misc\n",
    "from engine_node2node import get_text_embeddings\n",
    "from util.datasets import build_shape_surface_occupancy_dataset\n",
    "\n",
    "\n",
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser(\"Performing Chained Eval\", add_help=False)\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        default=32,\n",
    "        type=int,\n",
    "        help=\"Batch size per GPU (effective batch size is batch_size * accum_iter * # gpus\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--text_model_name\",\n",
    "        type=str,\n",
    "        help=\"Text model name to use\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ae\",\n",
    "        type=str,\n",
    "        metavar=\"MODEL\",\n",
    "        help=\"Name of autoencoder\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ae-latent-dim\",\n",
    "        type=int,\n",
    "        default=512*8,\n",
    "        help=\"AE latent dimension\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ae_pth\",\n",
    "        required=True,\n",
    "        help=\"Autoencoder checkpoint\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--point_cloud_size\",\n",
    "        default=2048,\n",
    "        type=int,\n",
    "        help=\"input size\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--fetch_keys\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_clip\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_embeds\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--intensity_loss\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Contrastive edit intensity loss using ground-truth labels.\",\n",
    "    )\n",
    "\n",
    "    # Dataset parameters\n",
    "    parser.add_argument(\n",
    "        \"--dataset\",\n",
    "        type=str,\n",
    "        choices=[\"graphedits\", \"graphedits_chained\", \"graphedits_nrl\"],\n",
    "        help=\"dataset name\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data_path\",\n",
    "        type=str,\n",
    "        help=\"dataset path\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data_type\",\n",
    "        type=str,\n",
    "        help=\"dataset type\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_edge_level\",\n",
    "        default=None,\n",
    "        type=int,\n",
    "        help=\"maximum edge level to use\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--chain_length\",\n",
    "        default=None,\n",
    "        type=int,\n",
    "        help=\"length of chains to load\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--device\", default=\"cuda\", help=\"device to use for training / testing\"\n",
    "    )\n",
    "    parser.add_argument(\"--seed\", default=0, type=int)\n",
    "    parser.add_argument(\"--num_workers\", default=60, type=int)\n",
    "    parser.add_argument(\n",
    "        \"--pin_mem\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--alt_ae_embeds\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Alternative autoencoder embeddings to use\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ft_bert\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Also fine-tune the BERT model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model\",\n",
    "        type=str,\n",
    "        metavar=\"MODEL\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--resume\",\n",
    "        default=\"\",\n",
    "        help=\"Resume from checkpoint\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--resume_full_weights\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Resume the full model weights with the EDM wrapper\",\n",
    "    )\n",
    "\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint [/ibex/user/slimhy/Shape2VecSet/output/graph_edit/dm/nrl_listener_bert_256_chained/checkpoint-799.pth]...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import models.listeners as listener\n",
    "\n",
    "# Set dummy arg string to debug the parser\n",
    "call_string = \"\"\"--ae_pth /ibex/user/slimhy/Shape2VecSet/output/pc_ae/best_model.pt \\\n",
    "    --ae-latent-dim 256 \\\n",
    "    --num_workers 4 \\\n",
    "    --batch_size 64 \\\n",
    "    --text_model_name \"bert-base-uncased\" \\\n",
    "    --dataset graphedits_nrl \\\n",
    "    --data_path /ibex/user/slimhy/ShapeWalk_RND/ \\\n",
    "    --data_type release \\\n",
    "    --model nrl_listener_bert_256_pcae \\\n",
    "    --resume /ibex/user/slimhy/Shape2VecSet/output/graph_edit/dm/nrl_listener_bert_256_chained/checkpoint-799.pth \\\n",
    "    --resume_full_weights \\\n",
    "    --use_embeds \\\n",
    "    --alt_ae_embeds pc_ae \\\n",
    "    --seed 0\"\"\"\n",
    "\n",
    "# Parse the arguments\n",
    "args = get_args_parser()\n",
    "args = args.parse_args(call_string.split())\n",
    "args.use_clip = \"clip\" in args.text_model_name\n",
    "device = torch.device(args.device)\n",
    "\n",
    "model = listener.__dict__[args.model](use_linear_proj=not args.use_clip)\n",
    "model.to(device)\n",
    "\n",
    "# Load the checkpoint\n",
    "if args.resume:\n",
    "    print(\"Loading checkpoint [%s]...\" % args.resume)\n",
    "    checkpoint = torch.load(args.resume, map_location=\"cpu\")\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Fix the seed for reproducibility\n",
    "seed = args.seed + misc.get_rank()\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "args.fetch_keys = True\n",
    "args.fetch_intensity = True\n",
    "\n",
    "dataset_val = build_shape_surface_occupancy_dataset(\"val\", args=args)\n",
    "\n",
    "# Create data loaders\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "# --------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8845108695652174\n"
     ]
    }
   ],
   "source": [
    "from util.misc import AverageMeter, MetricLogger\n",
    "from losses.chamfer import chamfer_loss\n",
    "from losses.listener_loss import ListenerLoss\n",
    "\n",
    "model = model.eval()\n",
    "metric_meter = MetricLogger()\n",
    "criterion = ListenerLoss()\n",
    "\n",
    "def get_affected_param(edge_dict):\n",
    "    edge_dict = json.loads(edge_dict)\n",
    "    edge = list(edge_dict.keys())\n",
    "    if len(edge) > 2:\n",
    "        return \"err\"\n",
    "    else:\n",
    "        return edge[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_k, (edit_keys, x_a, x_b, embed_ab, edge_dict, labels) in enumerate(data_loader_val):\n",
    "        x_a = x_a.to(device)\n",
    "        x_b = x_b.to(device)\n",
    "        embed_ab = embed_ab.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        loss, pred = criterion(model, x_a, x_b, embed_ab, labels)\n",
    "        acc = (pred > 0.5).eq(labels).float().mean()\n",
    "\n",
    "        # Also log accuracy per parameter affected\n",
    "        # use get_affected_param(edge_dict) which returns a single affected parameter\n",
    "        unique_affected_params = [[get_affected_param(edge_dict[i]), i] for i in range(len(edge_dict))]\n",
    "        # Map the affected parameters to the corresponding edge_dict index\n",
    "        param_to_idx = {param: i for param, i in unique_affected_params}\n",
    "        for param, param_idx in param_to_idx.items():\n",
    "            param_acc = (pred[param_idx] > 0.5).eq(labels[param_idx]).float().mean()\n",
    "            metric_meter.update(**{param: param_acc.item()})\n",
    "        \n",
    "        metric_meter.update(global_acc=acc.item())\n",
    "\n",
    "print(metric_meter.global_acc.global_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_legs_shape_1: 1.0000 (0.9565)\tv_back_leg_mid_y_offset_pct: 1.0000 (0.9556)\tv_y: 1.0000 (0.9783)\tv_x: 1.0000 (0.9783)\tb_is_handles_cusion: 1.0000 (0.7500)\tv_cr_count: 1.0000 (0.6667)\tv_seat_shape: 1.0000 (0.9783)\tv_z: 1.0000 (1.0000)\tv_tr_scale_z: 1.0000 (0.9556)\tv_seat_pos: 1.0000 (1.0000)\terr: 1.0000 (0.7561)\tv_curvature: 1.0000 (1.0000)\tv_legs_shape_2: 1.0000 (0.5556)\tv_back_leg_bottom_y_offset_pct: 1.0000 (0.9333)\tv_legs_bevel: 1.0000 (0.5870)\tv_monoleg_tent_count: 1.0000 (0.6571)\tv_tr_shape_1: 1.0000 (0.9773)\tglobal_acc: 0.8906 (0.8845)\tv_tr_scale_y: 1.0000 (0.9512)\tv_vr_count: 1.0000 (0.6667)\n"
     ]
    }
   ],
   "source": [
    "print(str(metric_meter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"model_name:\", args.model)\n",
    "for chain_length, batch_size, drop_n in [[10, 8, 0], [15, 4, 3], [20, 3, 0]]:\n",
    "    #print(\"chain_length=\", chain_length, \"batch_size=\", batch_size)\n",
    "    data_loader_val = get_loader(args, batch_size=batch_size, chain_length=chain_length)\n",
    "    metric_meter = get_metrics(data_loader_val)\n",
    "\n",
    "    #print(\"|P| = %d\" % chain_length)\n",
    "    table_str = \"& & FCD & \\multicolumn{1}{c}{ACD} & \\multicolumn{1}{c}{FL2} & \\multicolumn{1}{c}{AL2} \"\n",
    "    table_str = table_str.replace(\"FCD\", \"%0.3f\" % round(metric_meter.final_cd_dist.global_avg*10**4, 3))\n",
    "    table_str = table_str.replace(\"ACD\", \"%0.3f\" % round(metric_meter.cd_dist.global_avg*10**4, 3))\n",
    "    \n",
    "    table_str = table_str.replace(\"FL2\", \"%0.3f\" % round(metric_meter.final_l2_dist.global_avg, 3))\n",
    "    table_str = table_str.replace(\"AL2\", \"%0.3f\" % round(metric_meter.avg_l2_dist.global_avg, 3))\n",
    "\n",
    "    print(table_str)\n",
    "print(\"\\\\\\\\\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shape2vecset",
   "language": "python",
   "name": "shape2vecset"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
