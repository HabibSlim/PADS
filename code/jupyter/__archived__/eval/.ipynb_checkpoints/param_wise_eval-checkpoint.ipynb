{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ibex/user/slimhy/Shape2VecSet/code\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slimhy/conda/envs/shape2vecset/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jitting Chamfer 3D\n",
      "Loaded JIT 3D CUDA chamfer distance\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Evaluation of the models.\n",
    "\"\"\"\n",
    "import argparse\n",
    "import os.path as osp\n",
    "import pprint\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import Sampler\n",
    "\n",
    "import util.misc as misc\n",
    "from util.misc import MetricLogger\n",
    "from util.datasets import build_shape_surface_occupancy_dataset\n",
    "\n",
    "import models.mlp_mapper as mlp_mapper\n",
    "from models.mlp import MLP\n",
    "from models.point_net import PointNet\n",
    "from models.pointcloud_autoencoder import PointcloudAutoencoder\n",
    "\n",
    "from losses.chamfer import chamfer_loss\n",
    "\n",
    "\n",
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser(\"Performing Chained Eval\", add_help=False)\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        default=32,\n",
    "        type=int,\n",
    "        help=\"Batch size per GPU (effective batch size is batch_size * accum_iter * # gpus\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--text_model_name\",\n",
    "        type=str,\n",
    "        help=\"Text model name to use\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ae\",\n",
    "        type=str,\n",
    "        metavar=\"MODEL\",\n",
    "        help=\"Name of autoencoder\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ae-latent-dim\",\n",
    "        type=int,\n",
    "        default=512*8,\n",
    "        help=\"AE latent dimension\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ae_pth\",\n",
    "        required=True,\n",
    "        help=\"Autoencoder checkpoint\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--point_cloud_size\",\n",
    "        default=2048,\n",
    "        type=int,\n",
    "        help=\"input size\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--fetch_keys\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_clip\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_embeds\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--intensity_loss\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Contrastive edit intensity loss using ground-truth labels.\",\n",
    "    )\n",
    "\n",
    "    # Dataset parameters\n",
    "    parser.add_argument(\n",
    "        \"--dataset\",\n",
    "        type=str,\n",
    "        choices=[\"graphedits\", \"graphedits_chained\"],\n",
    "        help=\"dataset name\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data_path\",\n",
    "        type=str,\n",
    "        help=\"dataset path\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data_type\",\n",
    "        type=str,\n",
    "        help=\"dataset type\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_edge_level\",\n",
    "        default=None,\n",
    "        type=int,\n",
    "        help=\"maximum edge level to use\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--chain_length\",\n",
    "        default=None,\n",
    "        type=int,\n",
    "        help=\"length of chains to load\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--device\", default=\"cuda\", help=\"device to use for training / testing\"\n",
    "    )\n",
    "    parser.add_argument(\"--seed\", default=0, type=int)\n",
    "    parser.add_argument(\"--num_workers\", default=60, type=int)\n",
    "    parser.add_argument(\n",
    "        \"--pin_mem\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--alt_ae_embeds\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Alternative autoencoder embeddings to use\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ft_bert\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Also fine-tune the BERT model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model\",\n",
    "        type=str,\n",
    "        metavar=\"MODEL\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--resume\",\n",
    "        default=\"\",\n",
    "        help=\"Resume from checkpoint\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--resume_full_weights\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Resume the full model weights with the EDM wrapper\",\n",
    "    )\n",
    "\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint [/ibex/user/slimhy/Shape2VecSet/output/graph_edit/dm/mlp_mapper_bert_bneck_1024_pcae__fine_chained/checkpoint-59.pth]...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "xx mlp_mapper_bert_l1__256\n",
    "/ibex/user/slimhy/Shape2VecSet/output/graph_edit/dm/mlp_mapper_bert_l1__256/checkpoint-50.pth\n",
    "\n",
    "xx mlp_mapper_bert_direct_latent_256\n",
    "/ibex/user/slimhy/Shape2VecSet/output/graph_edit/dm/mlp_mapper_bert_direct_latent_256/checkpoint-50.pth\n",
    "\n",
    "====================================\n",
    "\n",
    "xx mlp_mapper_bert_bneck_1024_pcae_cpl\n",
    "/ibex/user/slimhy/Shape2VecSet/output/graph_edit/dm/mlp_mapper_bert_bneck_1024_pcae__fine_chained_cpl/checkpoint-59.pth\n",
    "\n",
    "xx mlp_mapper_bert_bneck_512_pcae_cpl\n",
    "/ibex/user/slimhy/Shape2VecSet/output/graph_edit/dm/mlp_mapper_bert_bneck_512_pcae__fine_cpl__chained/checkpoint-59.pth\n",
    "\n",
    "xx mlp_mapper_bert_bneck_256_pcae_cpl\n",
    "/ibex/user/slimhy/Shape2VecSet/output/graph_edit/dm/mlp_mapper_bert_bneck_256_pcae__fine_cpl__chained/checkpoint-59.pth\n",
    "\n",
    "xx mlp_mapper_bert_l8_pcae_cpl\n",
    "/ibex/user/slimhy/Shape2VecSet/output/graph_edit/dm/mlp_mapper_bert_l8_pcae__fine_cpl__chained/checkpoint-59.pth\n",
    "\n",
    "xx mlp_mapper_bert_l4_pcae_cpl\n",
    "/ibex/user/slimhy/Shape2VecSet/output/graph_edit/dm/mlp_mapper_bert_l4_pcae__fine_cpl__chained/checkpoint-59.pth\n",
    "\n",
    "====================================\n",
    "xx mlp_mapper_bert_bneck_1024_pcae\n",
    "/ibex/user/slimhy/Shape2VecSet/output/graph_edit/dm/mlp_mapper_bert_bneck_1024_pcae__fine_chained/checkpoint-59.pth\n",
    "\n",
    "xx mlp_mapper_bert_bneck_512_pcae\n",
    "/ibex/user/slimhy/Shape2VecSet/output/graph_edit/dm/mlp_mapper_bert_bneck_512_pcae__fine_chained/checkpoint-59.pth\n",
    "\n",
    "xx mlp_mapper_bert_bneck_256_pcae\n",
    "/ibex/user/slimhy/Shape2VecSet/output/graph_edit/dm/mlp_mapper_bert_bneck_256_pcae__fine_chained/checkpoint-59.pth\n",
    "\n",
    "xx mlp_mapper_bert_l8_pcae\n",
    "/ibex/user/slimhy/Shape2VecSet/output/graph_edit/dm/mlp_mapper_bert_l8_pcae__fine_chained/checkpoint-59.pth\n",
    "\n",
    "xx mlp_mapper_bert_l4_pcae\n",
    "/ibex/user/slimhy/Shape2VecSet/output/graph_edit/dm/mlp_mapper_bert_l4_pcae__fine_chained/checkpoint-59.pth\n",
    "\"\"\"\n",
    "\n",
    "# Set dummy arg string to debug the parser\n",
    "call_string = \"\"\"--ae_pth /ibex/user/slimhy/Shape2VecSet/output/pc_ae/best_model.pt \\\n",
    "    --ae-latent-dim 256 \\\n",
    "    --text_model_name bert-base-uncased \\\n",
    "    --dataset graphedits \\\n",
    "    --data_path /ibex/user/slimhy/ShapeWalk_RND/ \\\n",
    "    --data_type release \\\n",
    "    --num_workers 8 \\\n",
    "    --model mlp_mapper_bert_bneck_1024_pcae \\\n",
    "    --resume /ibex/user/slimhy/Shape2VecSet/output/graph_edit/dm/mlp_mapper_bert_bneck_1024_pcae__fine_chained/checkpoint-59.pth \\\n",
    "    --resume_full_weights \\\n",
    "    --device cuda \\\n",
    "    --fetch_keys \\\n",
    "    --use_embeds \\\n",
    "    --alt_ae_embeds pc_ae \\\n",
    "    --seed 0\"\"\"\n",
    "\n",
    "# Parse the arguments\n",
    "args = get_args_parser()\n",
    "args = args.parse_args(call_string.split())\n",
    "args.use_clip = \"clip\" in args.text_model_name\n",
    "device = torch.device(args.device)\n",
    "\n",
    "model = mlp_mapper.__dict__[args.model](use_linear_proj=not args.use_clip)\n",
    "model.to(device)\n",
    "\n",
    "# Load the checkpoint\n",
    "if args.resume:\n",
    "    print(\"Loading checkpoint [%s]...\" % args.resume)\n",
    "    checkpoint = torch.load(args.resume, map_location=\"cpu\")\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slimhy/conda/envs/shape2vecset/lib/python3.10/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "# --------------------\n",
    "# Fix the seed for reproducibility\n",
    "seed = args.seed + misc.get_rank()\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "args.fetch_keys = True\n",
    "args.fetch_intensity = True\n",
    "\n",
    "dataset_train = build_shape_surface_occupancy_dataset(\"train\", args=args)\n",
    "\n",
    "# Create data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "# --------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading autoencoder [/ibex/user/slimhy/Shape2VecSet/output/pc_ae/best_model.pt]...\n",
      "Pretrained PC-AE is loaded at epoch 186.\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3853324/2478703457.py:64: UserWarning: The saved best_model.pt in the corresponding log_dir is not equal to the one requested.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def describe_pc_ae(args):\n",
    "    # Make an AE.\n",
    "    if args.encoder_net == \"pointnet\":\n",
    "        ae_encoder = PointNet(init_feat_dim=3, conv_dims=args.encoder_conv_layers)\n",
    "        encoder_latent_dim = args.encoder_conv_layers[-1]\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    if args.decoder_net == \"mlp\":\n",
    "        ae_decoder = MLP(\n",
    "            in_feat_dims=encoder_latent_dim,\n",
    "            out_channels=args.decoder_fc_neurons + [args.n_pc_points * 3],\n",
    "            b_norm=False,\n",
    "        )\n",
    "\n",
    "    model = PointcloudAutoencoder(ae_encoder, ae_decoder)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_state_dicts(checkpoint_file, map_location=None, **kwargs):\n",
    "    \"\"\" Load torch items from saved state_dictionaries\"\"\"\n",
    "    if map_location is None:\n",
    "        checkpoint = torch.load(checkpoint_file)\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint_file, map_location=map_location)\n",
    "\n",
    "    for key, value in kwargs.items():\n",
    "        value.load_state_dict(checkpoint[key])\n",
    "\n",
    "    epoch = checkpoint.get('epoch')\n",
    "    if epoch:\n",
    "        return epoch\n",
    "\n",
    "\n",
    "def read_saved_args(config_file, override_or_add_args=None, verbose=False):\n",
    "    \"\"\"\n",
    "    :param config_file: json file containing arguments\n",
    "    :param override_args: dict e.g., {'gpu': '0'} will set the resulting arg.gpu to be 0\n",
    "    :param verbose:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    args = parser.parse_args([])\n",
    "    with open(config_file, \"r\") as f_in:\n",
    "        args.__dict__ = json.load(f_in)\n",
    "\n",
    "    if override_or_add_args is not None:\n",
    "        for key, val in override_or_add_args.items():\n",
    "            args.__setattr__(key, val)\n",
    "\n",
    "    if verbose:\n",
    "        args_string = pprint.pformat(vars(args))\n",
    "        print(args_string)\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "def load_pretrained_pc_ae(model_file):\n",
    "    config_file = osp.join(osp.dirname(model_file), \"config.json.txt\")\n",
    "    pc_ae_args = read_saved_args(config_file)\n",
    "    pc_ae = describe_pc_ae(pc_ae_args)\n",
    "\n",
    "    if osp.join(pc_ae_args.log_dir, \"best_model.pt\") != osp.abspath(model_file):\n",
    "        warnings.warn(\n",
    "            \"The saved best_model.pt in the corresponding log_dir is not equal to the one requested.\"\n",
    "        )\n",
    "\n",
    "    best_epoch = load_state_dicts(model_file, model=pc_ae)\n",
    "    print(f\"Pretrained PC-AE is loaded at epoch {best_epoch}.\")\n",
    "    return pc_ae, pc_ae_args\n",
    "\n",
    "\n",
    "# Instantiate autoencoder\n",
    "print(\"Loading autoencoder [%s]...\" % args.ae_pth)\n",
    "pc_ae, pc_ae_args = load_pretrained_pc_ae(args.ae_pth)\n",
    "pc_ae = pc_ae.to(device)\n",
    "pc_ae = pc_ae.eval()\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_edit(net, x_a, embed_ab):\n",
    "    # Reshape from (B, D, K) to (B, M)\n",
    "    x_a = x_a.flatten(1)\n",
    "    embed_ab = embed_ab.flatten(1)\n",
    "\n",
    "    # Concatenate the latent vector with the embedding\n",
    "    edit_vec = net(x_a, embed_ab)\n",
    "\n",
    "    # Add the edit vector to the latent vector\n",
    "    #return edit_vec\n",
    "    return edit_vec + x_a\n",
    "\n",
    "def apply_edit__gt(net, x_a, x_b, embed_ab):\n",
    "    # Reshape from (B, D, K) to (B, M)\n",
    "    x_a = x_a.flatten(1)\n",
    "    embed_ab = embed_ab.flatten(1)\n",
    "\n",
    "    # Concatenate the latent vector with the embedding\n",
    "    edit_dir, magnitude = net.forward_decoupled(x_a, embed_ab)\n",
    "    gt_vec = (x_b-x_a)\n",
    "    opt_direction = torch.zeros_like(edit_dir)\n",
    "    for i in range(x_a.shape[0]):\n",
    "        opt_direction[i] = gt_vec[i] / (torch.norm(gt_vec[i]) + 1e-8)\n",
    "\n",
    "    # Add the edit vector to the latent vector\n",
    "    return opt_direction*magnitude + x_a\n",
    "\n",
    "def apply_edit__gt_mag(net, x_a, x_b, embed_ab):\n",
    "    # Reshape from (B, D, K) to (B, M)\n",
    "    x_a = x_a.flatten(1)\n",
    "    embed_ab = embed_ab.flatten(1)\n",
    "\n",
    "    # Concatenate the latent vector with the embedding\n",
    "    edit_dir, magnitude = net.forward_decoupled(x_a, embed_ab)\n",
    "    gt_vec = (x_b-x_a)\n",
    "    opt_mag = torch.zeros_like(magnitude)\n",
    "    for i in range(x_a.shape[0]):\n",
    "        opt_mag[i] = gt_vec[i] / (torch.norm(gt_vec[i]) + 1e-8)\n",
    "\n",
    "    # Add the edit vector to the latent vector\n",
    "    return opt_direction*magnitude + x_a\n",
    "\n",
    "def apply_iterated_edits(model, embeds_a, embeds_b, embeds_text, use_gt=False):\n",
    "    # Move all the garbage to CUDA\n",
    "    embeds_a = embeds_a.cuda()\n",
    "    embeds_b = embeds_b.cuda()\n",
    "    embeds_text = embeds_text.cuda()\n",
    "\n",
    "    if use_gt:\n",
    "        x_b_edited = apply_edit__gt(model, embeds_a, embeds_b, embeds_text)\n",
    "    else:\n",
    "        x_b_edited = apply_edit(model, embeds_a, embeds_text)\n",
    "    x_b = embeds_b\n",
    "    x_a = embeds_a\n",
    "\n",
    "    # Decode the batch\n",
    "    b_size = x_b.shape[0]\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        orig = pc_ae.decoder(x_a).reshape([b_size, 4096, 3])\n",
    "        rec = pc_ae.decoder(x_b_edited).reshape([b_size, 4096, 3])\n",
    "        rec_gt = pc_ae.decoder(x_b).reshape([b_size, 4096, 3])\n",
    "\n",
    "    return (orig, rec, rec_gt), (x_a, x_b_edited, x_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 8543/28500 [02:22<05:27, 60.89it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "model = model.eval()\n",
    "metric_meter = MetricLogger()\n",
    "\n",
    "def get_affected_param(edge_dict):\n",
    "    edge_dict = json.loads(edge_dict)\n",
    "    edge = list(edge_dict.keys())\n",
    "    if len(edge) > 2:\n",
    "        return \"err\"\n",
    "    else:\n",
    "        return edge[0]\n",
    "\n",
    "def get_metrics(data_loader):\n",
    "    with torch.no_grad():\n",
    "        for batch_k, (node_ids, node_a, node_b, text_embeds, edge_dict) in tqdm(enumerate(data_loader), total=len(data_loader)):          \n",
    "            # Apply the edits\n",
    "            (orig, rec, rec_gt), (x_a, x_b_edited, x_b) = apply_iterated_edits(model,\n",
    "                                                                               embeds_a=node_a,\n",
    "                                                                               embeds_b=node_b,\n",
    "                                                                               embeds_text=text_embeds,\n",
    "                                                                               use_gt=True)\n",
    "    \n",
    "            # Compute average pairwise L2 distance in feature space\n",
    "            l2_dist = torch.norm(x_b_edited - x_b, p=2)\n",
    "            l2_scale = torch.norm(x_b_edited - x_a, p=2)\n",
    "    \n",
    "            # Compute average pairwise CD\n",
    "            cd_dist = chamfer_loss(rec, rec_gt, reduction=\"mean\").mean()\n",
    "            cd_scale = chamfer_loss(rec, orig, reduction=\"mean\").mean()\n",
    "    \n",
    "            metric_meter.update(avg_l2_dist=l2_dist.item(),\n",
    "                                avg_l2_scale=l2_scale.item(),\n",
    "                                cd_dist=cd_dist.item(),\n",
    "                                cd_scale=cd_scale.item())\n",
    "       \n",
    "            # Also log accuracy per parameter affected\n",
    "            # use get_affected_param(edge_dict) which returns a single affected parameter\n",
    "            unique_affected_params = [[get_affected_param(edge_dict[i]), i] for i in range(len(edge_dict))]\n",
    "            # Map the affected parameters to the corresponding edge_dict index\n",
    "            param_to_idx = {param: i for param, i in unique_affected_params}\n",
    "            for param, param_idx in param_to_idx.items():\n",
    "                rec_param = rec[param_idx].unsqueeze(0)\n",
    "                rec_gt_param = rec_gt[param_idx].unsqueeze(0)\n",
    "                param_cd_dist = chamfer_loss(rec_param, rec_gt_param, reduction=\"mean\").mean()\n",
    "                param_l2_dist = torch.norm(x_b_edited[param_idx] - x_b[param_idx], p=2)\n",
    "                metric_meter.update(**{param + \"_cd_dist\": param_cd_dist.item(),\n",
    "                                       param + \"_l2_dist\": param_l2_dist.item()})\n",
    "\n",
    "    return metric_meter\n",
    "\n",
    "metric_meter = get_metrics(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "defaultdict(util.misc.SmoothedValue,\n",
    "            {'avg_l2_dist': <util.misc.SmoothedValue at 0x152c06037610>,\n",
    "             'avg_l2_scale': <util.misc.SmoothedValue at 0x152c06037640>,\n",
    "             'cd_dist': <util.misc.SmoothedValue at 0x152c06037700>,\n",
    "             'cd_scale': <util.misc.SmoothedValue at 0x152c06037760>,\n",
    "             'v_legs_bevel_cd_dist': <util.misc.SmoothedValue at 0x152c060377c0>,\n",
    "             'v_legs_bevel_l2_dist': <util.misc.SmoothedValue at 0x152c060377f0>,\n",
    "             'v_z_cd_dist': <util.misc.SmoothedValue at 0x152c06037850>,\n",
    "             'v_z_l2_dist': <util.misc.SmoothedValue at 0x152c060378b0>,\n",
    "             'v_seat_pos_cd_dist': <util.misc.SmoothedValue at 0x152c06037910>,\n",
    "             'v_seat_pos_l2_dist': <util.misc.SmoothedValue at 0x152c06037970>,\n",
    "             'v_tr_scale_z_cd_dist': <util.misc.SmoothedValue at 0x152c060379d0>,\n",
    "             'v_tr_scale_z_l2_dist': <util.misc.SmoothedValue at 0x152c06037a30>,\n",
    "             'v_legs_shape_1_cd_dist': <util.misc.SmoothedValue at 0x152c06037a90>,\n",
    "             'v_legs_shape_1_l2_dist': <util.misc.SmoothedValue at 0x152c06037af0>,\n",
    "             'v_back_leg_bottom_y_offset_pct_cd_dist': <util.misc.SmoothedValue at 0x152c06037b50>,\n",
    "             'v_back_leg_bottom_y_offset_pct_l2_dist': <util.misc.SmoothedValue at 0x152c06037bb0>,\n",
    "             'v_y_cd_dist': <util.misc.SmoothedValue at 0x152c06037c10>,\n",
    "             'v_y_l2_dist': <util.misc.SmoothedValue at 0x152c06037c70>,\n",
    "             'v_seat_shape_cd_dist': <util.misc.SmoothedValue at 0x152c06037cd0>,\n",
    "             'v_seat_shape_l2_dist': <util.misc.SmoothedValue at 0x152c06037d30>,\n",
    "             'v_tr_shape_1_cd_dist': <util.misc.SmoothedValue at 0x152c06037d90>,\n",
    "             'v_tr_shape_1_l2_dist': <util.misc.SmoothedValue at 0x152c06037df0>,\n",
    "             'v_curvature_cd_dist': <util.misc.SmoothedValue at 0x152c06037e50>,\n",
    "             'v_curvature_l2_dist': <util.misc.SmoothedValue at 0x152c06037eb0>,\n",
    "             'v_back_leg_mid_y_offset_pct_cd_dist': <util.misc.SmoothedValue at 0x152c06037f10>,\n",
    "             'v_back_leg_mid_y_offset_pct_l2_dist': <util.misc.SmoothedValue at 0x152c06037f70>,\n",
    "             'b_is_handles_cusion_cd_dist': <util.misc.SmoothedValue at 0x152c06037fd0>,\n",
    "             'b_is_handles_cusion_l2_dist': <util.misc.SmoothedValue at 0x152c075fc070>,\n",
    "             'v_x_cd_dist': <util.misc.SmoothedValue at 0x152c075fc0d0>,\n",
    "             'v_x_l2_dist': <util.misc.SmoothedValue at 0x152c075fc130>,\n",
    "             'err_cd_dist': <util.misc.SmoothedValue at 0x152c075fc190>,\n",
    "             'err_l2_dist': <util.misc.SmoothedValue at 0x152c075fc2b0>,\n",
    "             'v_tr_scale_y_cd_dist': <util.misc.SmoothedValue at 0x152c075fc400>,\n",
    "             'v_tr_scale_y_l2_dist': <util.misc.SmoothedValue at 0x152c075fc430>,\n",
    "             'v_legs_shape_2_cd_dist': <util.misc.SmoothedValue at 0x152c075fc460>,\n",
    "             'v_legs_shape_2_l2_dist': <util.misc.SmoothedValue at 0x152c075fc4c0>,\n",
    "             'v_monoleg_tent_count_cd_dist': <util.misc.SmoothedValue at 0x152c075fc760>,\n",
    "             'v_monoleg_tent_count_l2_dist': <util.misc.SmoothedValue at 0x152c075fc8b0>,\n",
    "             'v_cr_count_cd_dist': <util.misc.SmoothedValue at 0x152c075fc8e0>,\n",
    "             'v_cr_count_l2_dist': <util.misc.SmoothedValue at 0x152c075fca90>,\n",
    "             'v_vr_count_cd_dist': <util.misc.SmoothedValue at 0x152c075fd360>,\n",
    "             'v_vr_count_l2_dist': <util.misc.SmoothedValue at 0x152c075fd4b0>})\n",
    "\"\"\"\n",
    "param_groups = {\n",
    "    \"seat_height\": [\"v_seat_pos\"],\n",
    "    \"backrest curvature\": [\"v_curvature\"],\n",
    "    \"object width/height/depth\": [\"v_x\", \"v_x\", \"v_y\"],\n",
    "    \"seat roudness\": [\"v_seat_shape\"],\n",
    "    \"top bar thickness/height\": [\"v_tr_shape_1\", \"v_tr_shape_1\"],\n",
    "    \"legs thickness\": [\"v_legs_shape_1\", \"v_legs_shape_2\"],\n",
    "    \"adding/removing handle cushions\": [\"b_is_handles_cusion\"],\n",
    "    \"number of legs/backrest rails\": [\"v_monoleg_tent_count\", \"v_cr_count\", \"v_vr_count\"],\n",
    "    \"legs bending/curvature/roundness/indentation\": [\"v_legs_bevel\", \"v_legs_bevel\"],\n",
    "}\n",
    "\n",
    "# Group the metrics by parameter group in metric_logger.meters\n",
    "for param_group, param_list in param_groups.items():\n",
    "    group_avg_l2_dist = 0\n",
    "    group_avg_cd_dist = 0\n",
    "    for param in sorted(param_list):\n",
    "        group_avg_cd_dist += metric_meter.meters[param + \"_cd_dist\"].avg*(10**4)\n",
    "        group_avg_l2_dist += metric_meter.meters[param + \"_l2_dist\"].avg\n",
    "    group_avg_l2_dist /= len(param_list)\n",
    "    group_avg_cd_dist /= len(param_list)\n",
    "    print(f\"{param_group} & {group_avg_l2_dist:.4f} & {group_avg_cd_dist:.4f} \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shape2vecset",
   "language": "python",
   "name": "shape2vecset"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
