{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ibex/user/slimhy/Shape2VecSet/code\n"
     ]
    }
   ],
   "source": [
    "%cd /ibex/user/slimhy/Shape2VecSet/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jitting Chamfer 3D\n",
      "Loaded JIT 3D CUDA chamfer distance\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Chained evaluation of the models.\n",
    "\"\"\"\n",
    "import argparse\n",
    "import os.path as osp\n",
    "import pprint\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import Sampler\n",
    "\n",
    "import util.misc as misc\n",
    "from util.misc import MetricLogger\n",
    "from util.datasets import build_shape_surface_occupancy_dataset\n",
    "\n",
    "import models.mlp_mapper as mlp_mapper\n",
    "from models.mlp import MLP\n",
    "from models.point_net import PointNet\n",
    "from models.pointcloud_autoencoder import PointcloudAutoencoder\n",
    "\n",
    "from losses.chamfer import chamfer_loss\n",
    "\n",
    "\n",
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser(\"Performing Chained Eval\", add_help=False)\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        default=32,\n",
    "        type=int,\n",
    "        help=\"Batch size per GPU (effective batch size is batch_size * accum_iter * # gpus\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--text_model_name\",\n",
    "        type=str,\n",
    "        help=\"Text model name to use\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ae\",\n",
    "        type=str,\n",
    "        metavar=\"MODEL\",\n",
    "        help=\"Name of autoencoder\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ae-latent-dim\",\n",
    "        type=int,\n",
    "        default=512*8,\n",
    "        help=\"AE latent dimension\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ae_pth\",\n",
    "        required=True,\n",
    "        help=\"Autoencoder checkpoint\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--point_cloud_size\",\n",
    "        default=2048,\n",
    "        type=int,\n",
    "        help=\"input size\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--fetch_keys\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_clip\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_embeds\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--intensity_loss\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Contrastive edit intensity loss using ground-truth labels.\",\n",
    "    )\n",
    "\n",
    "    # Dataset parameters\n",
    "    parser.add_argument(\n",
    "        \"--dataset\",\n",
    "        type=str,\n",
    "        choices=[\"graphedits\", \"graphedits_chained\"],\n",
    "        help=\"dataset name\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data_path\",\n",
    "        type=str,\n",
    "        help=\"dataset path\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data_type\",\n",
    "        type=str,\n",
    "        help=\"dataset type\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_edge_level\",\n",
    "        default=None,\n",
    "        type=int,\n",
    "        help=\"maximum edge level to use\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--chain_length\",\n",
    "        default=None,\n",
    "        type=int,\n",
    "        help=\"length of chains to load\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--device\", default=\"cuda\", help=\"device to use for training / testing\"\n",
    "    )\n",
    "    parser.add_argument(\"--seed\", default=0, type=int)\n",
    "    parser.add_argument(\"--num_workers\", default=60, type=int)\n",
    "    parser.add_argument(\n",
    "        \"--pin_mem\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--alt_ae_embeds\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Alternative autoencoder embeddings to use\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ft_bert\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Also fine-tune the BERT model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model\",\n",
    "        type=str,\n",
    "        metavar=\"MODEL\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--resume\",\n",
    "        default=\"\",\n",
    "        help=\"Resume from checkpoint\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--resume_full_weights\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Resume the full model weights with the EDM wrapper\",\n",
    "    )\n",
    "\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint [/ibex/user/slimhy/Shape2VecSet/output/graph_edit/dm/mlp_mapper_bert_l8_pcae__fine_chained/checkpoint-59.pth]...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "xx mlp_mapper_bert_l1__256\n",
    "/ibex/user/slimhy/Shape2VecSet/output/graph_edit/dm/mlp_mapper_bert_l1__256/checkpoint-50.pth\n",
    "\n",
    "xx mlp_mapper_bert_direct_latent_256\n",
    "/ibex/user/slimhy/Shape2VecSet/output/graph_edit/dm/mlp_mapper_bert_direct_latent_256/checkpoint-50.pth\n",
    "\n",
    "====================================\n",
    "\n",
    "xx mlp_mapper_bert_bneck_1024_pcae_cpl\n",
    "/ibex/user/slimhy/Shape2VecSet/output/graph_edit/dm/mlp_mapper_bert_bneck_1024_pcae__fine_chained_cpl/checkpoint-59.pth\n",
    "\n",
    "xx mlp_mapper_bert_bneck_512_pcae_cpl\n",
    "/ibex/user/slimhy/Shape2VecSet/output/graph_edit/dm/mlp_mapper_bert_bneck_512_pcae__fine_cpl__chained/checkpoint-59.pth\n",
    "\n",
    "xx mlp_mapper_bert_bneck_256_pcae_cpl\n",
    "/ibex/user/slimhy/Shape2VecSet/output/graph_edit/dm/mlp_mapper_bert_bneck_256_pcae__fine_cpl__chained/checkpoint-59.pth\n",
    "\n",
    "xx mlp_mapper_bert_l8_pcae_cpl\n",
    "/ibex/user/slimhy/Shape2VecSet/output/graph_edit/dm/mlp_mapper_bert_l8_pcae__fine_cpl__chained/checkpoint-59.pth\n",
    "\n",
    "xx mlp_mapper_bert_l4_pcae_cpl\n",
    "/ibex/user/slimhy/Shape2VecSet/output/graph_edit/dm/mlp_mapper_bert_l4_pcae__fine_cpl__chained/checkpoint-59.pth\n",
    "\n",
    "====================================\n",
    "o mlp_mapper_bert_bneck_1024_pcae\n",
    "/ibex/user/slimhy/Shape2VecSet/output/graph_edit/dm/mlp_mapper_bert_bneck_1024_pcae__fine_chained/checkpoint-59.pth\n",
    "\n",
    "o mlp_mapper_bert_bneck_512_pcae\n",
    "/ibex/user/slimhy/Shape2VecSet/output/graph_edit/dm/mlp_mapper_bert_bneck_512_pcae__fine_chained/checkpoint-59.pth\n",
    "\n",
    "o mlp_mapper_bert_bneck_256_pcae\n",
    "/ibex/user/slimhy/Shape2VecSet/output/graph_edit/dm/mlp_mapper_bert_bneck_256_pcae__fine_chained/checkpoint-59.pth\n",
    "\n",
    "o mlp_mapper_bert_l8_pcae\n",
    "/ibex/user/slimhy/Shape2VecSet/output/graph_edit/dm/mlp_mapper_bert_l8_pcae__fine_chained/checkpoint-59.pth\n",
    "\n",
    "xx mlp_mapper_bert_l4_pcae\n",
    "/ibex/user/slimhy/Shape2VecSet/output/graph_edit/dm/mlp_mapper_bert_l4_pcae__fine_chained/checkpoint-59.pth\n",
    "\"\"\"\n",
    "\n",
    "# Set dummy arg string to debug the parser\n",
    "call_string = \"\"\"--ae_pth /ibex/user/slimhy/Shape2VecSet/output/pc_ae/best_model.pt \\\n",
    "    --ae-latent-dim 256 \\\n",
    "    --text_model_name bert-base-uncased \\\n",
    "    --dataset graphedits_chained \\\n",
    "    --data_path /ibex/user/slimhy/ShapeWalk/ \\\n",
    "    --data_type release_chained \\\n",
    "    --num_workers 8 \\\n",
    "    --model mlp_mapper_bert_l8_pcae \\\n",
    "    --resume /ibex/user/slimhy/Shape2VecSet/output/graph_edit/dm/mlp_mapper_bert_l8_pcae__fine_chained/checkpoint-59.pth \\\n",
    "    --resume_full_weights \\\n",
    "    --device cuda \\\n",
    "    --fetch_keys \\\n",
    "    --use_embeds \\\n",
    "    --alt_ae_embeds pc_ae \\\n",
    "    --seed 0\"\"\"\n",
    "# Parse the arguments\n",
    "args = get_args_parser()\n",
    "args = args.parse_args(call_string.split())\n",
    "args.use_clip = \"clip\" in args.text_model_name\n",
    "device = torch.device(args.device)\n",
    "\n",
    "model = mlp_mapper.__dict__[args.model](use_linear_proj=not args.use_clip)\n",
    "model.to(device)\n",
    "\n",
    "# Load the checkpoint\n",
    "if args.resume:\n",
    "    print(\"Loading checkpoint [%s]...\" % args.resume)\n",
    "    checkpoint = torch.load(args.resume, map_location=\"cpu\")\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ChainSampler(Sampler):\n",
    "    def __init__(self, dataset, batch_size, chain_length):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.chain_length = chain_length\n",
    "        self.num_samples = len(dataset)\n",
    "        self.n_batches = self.num_samples // (self.batch_size)\n",
    "        self.n_chains = self.num_samples // self.chain_length\n",
    "\n",
    "    def __iter__(self):\n",
    "        t = 0\n",
    "        for m in range(self.n_chains):\n",
    "            for k in range(self.chain_length):\n",
    "                for j in range(self.batch_size):\n",
    "                    yield m*(self.chain_length*self.batch_size)+k+j*self.chain_length\n",
    "                    t += 1\n",
    "                    if t == self.num_samples:\n",
    "                        return\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "def get_loader(args, batch_size, chain_length):\n",
    "    args.batch_size = batch_size\n",
    "    args.chain_length = chain_length\n",
    "\n",
    "    dataset_val = build_shape_surface_occupancy_dataset(\"val\", args=args)\n",
    "    chain_sampler = ChainSampler(dataset_val, batch_size=args.batch_size, chain_length=args.chain_length)\n",
    "    data_loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "        sampler=chain_sampler\n",
    "    )\n",
    "\n",
    "    return data_loader_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Fix the seed for reproducibility\n",
    "seed = args.seed + misc.get_rank()\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "args.fetch_keys = True\n",
    "# --------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading autoencoder [/ibex/user/slimhy/Shape2VecSet/ckpt/pc_ae/best_model.pt]...\n",
      "Pretrained PC-AE is loaded at epoch 186.\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1132892/2777894551.py:64: UserWarning: The saved best_model.pt in the corresponding log_dir is not equal to the one requested.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def describe_pc_ae(args):\n",
    "    # Make an AE.\n",
    "    if args.encoder_net == \"pointnet\":\n",
    "        ae_encoder = PointNet(init_feat_dim=3, conv_dims=args.encoder_conv_layers)\n",
    "        encoder_latent_dim = args.encoder_conv_layers[-1]\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    if args.decoder_net == \"mlp\":\n",
    "        ae_decoder = MLP(\n",
    "            in_feat_dims=encoder_latent_dim,\n",
    "            out_channels=args.decoder_fc_neurons + [args.n_pc_points * 3],\n",
    "            b_norm=False,\n",
    "        )\n",
    "\n",
    "    model = PointcloudAutoencoder(ae_encoder, ae_decoder)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_state_dicts(checkpoint_file, map_location=None, **kwargs):\n",
    "    \"\"\" Load torch items from saved state_dictionaries\"\"\"\n",
    "    if map_location is None:\n",
    "        checkpoint = torch.load(checkpoint_file)\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint_file, map_location=map_location)\n",
    "\n",
    "    for key, value in kwargs.items():\n",
    "        value.load_state_dict(checkpoint[key])\n",
    "\n",
    "    epoch = checkpoint.get('epoch')\n",
    "    if epoch:\n",
    "        return epoch\n",
    "\n",
    "\n",
    "def read_saved_args(config_file, override_or_add_args=None, verbose=False):\n",
    "    \"\"\"\n",
    "    :param config_file: json file containing arguments\n",
    "    :param override_args: dict e.g., {'gpu': '0'} will set the resulting arg.gpu to be 0\n",
    "    :param verbose:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    args = parser.parse_args([])\n",
    "    with open(config_file, \"r\") as f_in:\n",
    "        args.__dict__ = json.load(f_in)\n",
    "\n",
    "    if override_or_add_args is not None:\n",
    "        for key, val in override_or_add_args.items():\n",
    "            args.__setattr__(key, val)\n",
    "\n",
    "    if verbose:\n",
    "        args_string = pprint.pformat(vars(args))\n",
    "        print(args_string)\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "def load_pretrained_pc_ae(model_file):\n",
    "    config_file = osp.join(osp.dirname(model_file), \"config.json.txt\")\n",
    "    pc_ae_args = read_saved_args(config_file)\n",
    "    pc_ae = describe_pc_ae(pc_ae_args)\n",
    "\n",
    "    if osp.join(pc_ae_args.log_dir, \"best_model.pt\") != osp.abspath(model_file):\n",
    "        warnings.warn(\n",
    "            \"The saved best_model.pt in the corresponding log_dir is not equal to the one requested.\"\n",
    "        )\n",
    "\n",
    "    best_epoch = load_state_dicts(model_file, model=pc_ae)\n",
    "    print(f\"Pretrained PC-AE is loaded at epoch {best_epoch}.\")\n",
    "    return pc_ae, pc_ae_args\n",
    "\n",
    "AE_MODEl_NAME = \"pc_ae\"\n",
    "AE_MODEL_PATH = \"/ibex/user/slimhy/Shape2VecSet/ckpt/pc_ae/best_model.pt\"\n",
    "\n",
    "# Instantiate autoencoder\n",
    "print(\"Loading autoencoder [%s]...\" % AE_MODEL_PATH)\n",
    "pc_ae, pc_ae_args = load_pretrained_pc_ae(AE_MODEL_PATH)\n",
    "pc_ae = pc_ae.to(device)\n",
    "pc_ae = pc_ae.eval()\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def apply_edit(net, x_a, embed_ab):\n",
    "    # Reshape from (B, D, K) to (B, M)\n",
    "    x_a = x_a.flatten(1)\n",
    "    embed_ab = embed_ab.flatten(1)\n",
    "\n",
    "    # Concatenate the latent vector with the embedding\n",
    "    edit_vec = net(x_a, embed_ab)\n",
    "\n",
    "    # Add the edit vector to the latent vector\n",
    "    #return edit_vec\n",
    "    return edit_vec + x_a\n",
    "\n",
    "def apply_edit__oracle_mag(net, x_a, x_b, embed_ab):\n",
    "    # Reshape from (B, D, K) to (B, M)\n",
    "    x_a = x_a.flatten(1)\n",
    "    embed_ab = embed_ab.flatten(1)\n",
    "\n",
    "    # Concatenate the latent vector with the embedding\n",
    "    edit_dir, magnitude = net.forward_decoupled(x_a, embed_ab)\n",
    "    gt_vec = (x_b-x_a)\n",
    "    opt_magnitude = torch.zeros_like(magnitude)\n",
    "    for i in range(x_a.shape[0]):\n",
    "        opt_magnitude[i] = solve_eq(a=edit_dir[i], b=gt_vec[i])\n",
    "    edit_vec = edit_dir * opt_magnitude\n",
    "\n",
    "    # Add the edit vector to the latent vector\n",
    "    return edit_vec + x_a\n",
    "\n",
    "def apply_edit__oracle_dir(net, x_a, x_b, embed_ab):\n",
    "    # Reshape from (B, D, K) to (B, M)\n",
    "    x_a = x_a.flatten(1)\n",
    "    embed_ab = embed_ab.flatten(1)\n",
    "\n",
    "    # Concatenate the latent vector with the embedding\n",
    "    edit_dir, magnitude = net.forward_decoupled(x_a, embed_ab)\n",
    "    gt_vec = (x_b-x_a)\n",
    "    opt_direction = torch.zeros_like(edit_dir)\n",
    "    for i in range(x_a.shape[0]):\n",
    "        opt_direction[i] = gt_vec[i] / (torch.norm(gt_vec[i]) + 1e-8)\n",
    "\n",
    "    # Add the edit vector to the latent vector\n",
    "    return opt_direction*magnitude + x_a\n",
    "\n",
    "def solve_eq(a, b):\n",
    "    \"\"\"\n",
    "    Minimize ||x*a - b||^2, where x is a scalar, and a and b are fixed vectors.\n",
    "    \"\"\"\n",
    "    return (b * a).sum() / (a * a).sum()\n",
    "\n",
    "def apply_iterated_edits(model, embeds_a, embeds_b, embeds_text):\n",
    "    # Move all the garbage to CUDA\n",
    "    embeds_a = embeds_a.cuda()\n",
    "    embeds_b = embeds_b.cuda()\n",
    "    embeds_text = embeds_text.cuda()\n",
    "\n",
    "    x_b_edited = apply_edit__oracle_dir(model, embeds_a, embeds_b, embeds_text)\n",
    "    x_b = embeds_b\n",
    "    x_a = embeds_a\n",
    "\n",
    "    # Decode the batch\n",
    "    b_size = x_b.shape[0]\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        orig = pc_ae.decoder(x_a).reshape([b_size, 4096, 3])\n",
    "        rec = pc_ae.decoder(x_b_edited).reshape([b_size, 4096, 3])\n",
    "        rec_gt = pc_ae.decoder(x_b).reshape([b_size, 4096, 3])\n",
    "\n",
    "    return (orig, rec, rec_gt), (x_a, x_b_edited, x_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slimhy/conda/envs/shape2vecset/lib/python3.10/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp_mapper_bert_l8_pcae\n",
      "============================================================\n",
      "& & 34.114 & \\multicolumn{1}{c}{21.346} & \\multicolumn{1}{c}{7.912} & \\multicolumn{1}{c}{6.539} \n",
      "& & 27.319 & \\multicolumn{1}{c}{21.362} & \\multicolumn{1}{c}{6.447} & \\multicolumn{1}{c}{5.299} \n",
      "& & 24.498 & \\multicolumn{1}{c}{17.756} & \\multicolumn{1}{c}{5.798} & \\multicolumn{1}{c}{4.452} \n",
      "\\\\\n",
      "\n",
      "& \\multicolumn{1}{c}{\\icono} & & 28.644 & 20.155 & 6.719 & 5.430 \\\n"
     ]
    }
   ],
   "source": [
    "model = model.eval()\n",
    "metric_meter = MetricLogger()\n",
    "\n",
    "def get_metrics(data_loader):\n",
    "    with torch.no_grad():\n",
    "        chain_count = 0\n",
    "        for batch_k, (chain_ids, edit_keys, node_a, node_b, text_embeds) in enumerate(data_loader):\n",
    "            #print(chain_ids)\n",
    "            if batch_k == len(data_loader_val) - 1 - drop_n:\n",
    "                break\n",
    "            if chain_count == 0:\n",
    "                prev_node = node_a\n",
    "            \n",
    "            # Apply the edits\n",
    "            (orig, rec, rec_gt), (x_a, x_b_edited, x_b) = apply_iterated_edits(model,\n",
    "                                                                               embeds_a=prev_node,\n",
    "                                                                               embeds_b=node_b,\n",
    "                                                                               embeds_text=text_embeds)\n",
    "    \n",
    "            # Compute average pairwise L2 distance in feature space\n",
    "            l2_dist = torch.norm(x_b_edited - x_b, p=2)\n",
    "            l2_scale = torch.norm(x_b_edited - x_a, p=2)\n",
    "    \n",
    "            # Compute average pairwise CD\n",
    "            cd_dist = chamfer_loss(rec, rec_gt, reduction=\"mean\").mean()\n",
    "            cd_scale = chamfer_loss(rec, orig, reduction=\"mean\").mean()\n",
    "    \n",
    "            metric_meter.update(avg_l2_dist=l2_dist.item(),\n",
    "                                avg_l2_scale=l2_scale.item(),\n",
    "                                cd_dist=cd_dist.item(),\n",
    "                                cd_scale=cd_scale.item())\n",
    "    \n",
    "            prev_node = x_b_edited\n",
    "    \n",
    "            chain_count += 1\n",
    "            if chain_count == args.chain_length:\n",
    "                metric_meter.update(final_l2_dist=l2_dist.item(),\n",
    "                                    final_l2_scale=l2_scale.item(),\n",
    "                                    final_cd_dist=cd_dist.item(),\n",
    "                                    final_cd_scale=cd_scale.item())\n",
    "                chain_count = 0\n",
    "\n",
    "    return metric_meter\n",
    "\n",
    "avg_fcd = []\n",
    "avg_acd = []\n",
    "avg_fl2 = []\n",
    "avg_al2 = []\n",
    "for chain_length, batch_size, drop_n in [[10, 8, 0], [15, 4, 3], [20, 3, 0]]:\n",
    "    #print(\"chain_length=\", chain_length, \"batch_size=\", batch_size)\n",
    "    data_loader_val = get_loader(args, batch_size=batch_size, chain_length=chain_length)\n",
    "    metric_meter = get_metrics(data_loader_val)\n",
    "    \n",
    "    if chain_length==10:\n",
    "        print(args.model)\n",
    "        print(\"======\"*10)\n",
    "\n",
    "    # print(\"|P| = %d\" % chain_length)\n",
    "    table_str = \"& & FCD & \\multicolumn{1}{c}{ACD} & \\multicolumn{1}{c}{FL2} & \\multicolumn{1}{c}{AL2} \"\n",
    "    table_str = table_str.replace(\"FCD\", \"%0.3f\" % round(metric_meter.final_cd_dist.global_avg*10**4, 3))\n",
    "    table_str = table_str.replace(\"ACD\", \"%0.3f\" % round(metric_meter.cd_dist.global_avg*10**4, 3))\n",
    "    \n",
    "    table_str = table_str.replace(\"FL2\", \"%0.3f\" % round(metric_meter.final_l2_dist.global_avg, 3))\n",
    "    table_str = table_str.replace(\"AL2\", \"%0.3f\" % round(metric_meter.avg_l2_dist.global_avg, 3))\n",
    "    avg_fcd += [metric_meter.final_cd_dist.global_avg*10**4]\n",
    "    avg_acd += [metric_meter.cd_dist.global_avg*10**4]\n",
    "    avg_fl2 += [metric_meter.final_l2_dist.global_avg]\n",
    "    avg_al2 += [metric_meter.avg_l2_dist.global_avg]\n",
    "\n",
    "    print(table_str)\n",
    "print(\"\\\\\\\\\")\n",
    "print()\n",
    "print(\"& \\multicolumn{1}{c}{\\icono} & & %0.3f & %0.3f & %0.3f & %0.3f \\\\\" % (np.mean(avg_fcd), np.mean(avg_acd), np.mean(avg_fl2), np.mean(avg_al2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shape2vecset",
   "language": "python",
   "name": "shape2vecset"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
