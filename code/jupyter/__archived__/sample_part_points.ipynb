{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /ibex/user/slimhy/PADS/code\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "from util.misc import fps_subsample\n",
    "from datasets.sampling import sample_surface_tpp\n",
    "from datasets.metadata import COMPAT_TRANSFORMS, hex_to_class\n",
    "from util.mesh import CUDAMesh\n",
    "from util.misc import generate_colormap_colors\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "PART_INSTANCES = \"/ibex/project/c2273/3DCoMPaT/manifold_part_instances/\"\n",
    "OUT_DIR = \"/ibex/project/c2273/PADS/3DCoMPaT/part_points\"\n",
    "POINTS_PER_PART = 4096\n",
    "FPS_RATIO = 0.25\n",
    "N_PROCS = 12\n",
    "N_NODES = 4\n",
    "PROC_ID = 0\n",
    "\n",
    "\n",
    "def sample_part_points(pkl_file, return_stuff=False):\n",
    "    mesh_dict = pickle.load(open(os.path.join(PART_INSTANCES, pkl_file), \"rb\"))\n",
    "    model_name = pkl_file.split(\".\")[0]\n",
    "    \n",
    "    points_dict = {}\n",
    "    for mesh_k, mesh in mesh_dict.items():\n",
    "        cuda_mesh = CUDAMesh.from_trimesh(mesh).to(\"cpu\")\n",
    "        p_points = sample_surface_tpp(cuda_mesh, POINTS_PER_PART)\n",
    "        p_points = fps_subsample(p_points / FPS_RATIO, ratio=FPS_RATIO)\n",
    "        points_dict[mesh_k] = p_points.cpu().squeeze()\n",
    "\n",
    "    # Process the point clouds\n",
    "    cls_name = os.path.basename(pkl_file).split(\".\")[0].split(\"_\")[0]\n",
    "    stacked_points = process_point_clouds(points_dict, hex_to_class(cls_name))\n",
    "\n",
    "    # Save points to npy file\n",
    "    npy_file = f\"{OUT_DIR}/{model_name}.npy\"\n",
    "    np.save(npy_file, stacked_points)\n",
    "\n",
    "    if mesh_dict:\n",
    "        return npy_file.replace(\".pkl\", \".npy\"), mesh_dict\n",
    "    return pkl_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh\n",
    "\n",
    "\n",
    "def normalize_numpy_array(points):\n",
    "    \"\"\"\n",
    "    Normalize a numpy array of 3D points to fully fill a unit cube along all axes.\n",
    "    \n",
    "    :param points: numpy array of shape (N, 3) where N is the number of points\n",
    "    :return: normalized numpy array of shape (N, 3)\n",
    "    \"\"\"\n",
    "    if len(points.shape) != 2 or points.shape[1] != 3:\n",
    "        raise ValueError(\"Input must be a numpy array of shape (N, 3)\")\n",
    "\n",
    "    # Calculate the bounding box\n",
    "    min_coords = np.min(points, axis=0)\n",
    "    max_coords = np.max(points, axis=0)\n",
    "    \n",
    "    # Calculate the center of the bounding box\n",
    "    center = (min_coords + max_coords) / 2\n",
    "    \n",
    "    # Center the points\n",
    "    centered_points = points - center\n",
    "    \n",
    "    # Calculate the scale factors for each axis\n",
    "    scale_factors = 2 / (max_coords - min_coords)\n",
    "    \n",
    "    # Scale the points to fill the [-1, 1] cube\n",
    "    normalized_points = centered_points * scale_factors\n",
    "    \n",
    "    return normalized_points / 2.\n",
    "\n",
    "\n",
    "def process_point_clouds(point_cloud_dict, shape_cls):\n",
    "    # Concatenate all point clouds\n",
    "    all_points = np.concatenate([cloud for cloud in point_cloud_dict.values()], axis=0)\n",
    "\n",
    "    # Align the concatenated point cloud\n",
    "    if shape_cls in COMPAT_TRANSFORMS:\n",
    "        align_t = np.array(COMPAT_TRANSFORMS[shape_cls])\n",
    "        all_points = all_points @ align_t\n",
    "    \n",
    "    # Normalize the concatenated point cloud\n",
    "    all_points = normalize_numpy_array(all_points)\n",
    "    \n",
    "    # Assert that the points are within the [-0.5, 0.5] cube\n",
    "    assert np.all(all_points >= -0.5) and np.all(all_points <= 0.5)\n",
    "    \n",
    "    # Create a K, N, 3 tensor\n",
    "    n_part_points = list(point_cloud_dict.values())[0].shape[0]\n",
    "    stacked_points = np.zeros((len(point_cloud_dict), n_part_points, 3))\n",
    "    start_idx = 0\n",
    "    for k, (_, cloud) in enumerate(point_cloud_dict.items()):\n",
    "        num_points = cloud.shape[0]\n",
    "        end_idx = start_idx + num_points\n",
    "        \n",
    "        # Store in the new stacked array\n",
    "        stacked_points[k] = all_points[start_idx:end_idx]\n",
    "        start_idx = end_idx\n",
    "    \n",
    "    return stacked_points\n",
    "\n",
    "\n",
    "def visualize_pointcloud(\n",
    "    stacked_points,\n",
    "    point_radius=0.005,\n",
    "    colormap=\"viridis\",\n",
    "    alpha=1.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a scene combining the main mesh, its bounding boxes, and points for each part.\n",
    "    Bounding boxes and corresponding points share the same color.\n",
    "    \"\"\"\n",
    "    scene = trimesh.Scene()\n",
    "\n",
    "    # Generate colors for parts\n",
    "    n_parts = len(stacked_points)\n",
    "    colors = generate_colormap_colors(n_parts, colormap_name=colormap, alpha=alpha)\n",
    "\n",
    "    for i, color in enumerate(colors):\n",
    "        # Add points for this part using the same color\n",
    "        part_points = stacked_points[i]\n",
    "        \n",
    "        # Plot every point as a sphere\n",
    "        for point in part_points:\n",
    "            sphere = trimesh.creation.uv_sphere(radius=point_radius)\n",
    "            sphere.apply_translation(point)\n",
    "            sphere.visual.face_colors = np.array(color) * 255\n",
    "            scene.add_geometry(sphere)\n",
    "\n",
    "    return scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_files = [f for f in os.listdir(PART_INSTANCES) if f.endswith(\".pkl\")]\n",
    "pkl_files.sort()\n",
    "\n",
    "\n",
    "def chunk(l, n):\n",
    "    \"\"\"\n",
    "    Chunk a list into n equally sized sublists.\n",
    "    And NOT chunk it into sublists of size n.\n",
    "    \"\"\"\n",
    "    return [l[i*len(l)//n:(i+1)*len(l)//n] for i in range(n)]\n",
    "\n",
    "pkl_files = chunk(pkl_files, N_NODES)[PROC_ID]\n",
    "\n",
    "# Create a pool of workers\n",
    "with mp.Pool(processes=N_PROCS) as pool:\n",
    "    # Use imap to process files and update the progress bar\n",
    "    for _ in tqdm(pool.imap_unordered(sample_part_points, pkl_files), total=len(pkl_files), desc=\"Processing files\"):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
